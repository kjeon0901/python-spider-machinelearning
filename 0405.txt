웹크롤링 → 파이썬문법 → 데이터전처리 → 머신러닝
웹스크래핑 - url하나로 접근해서 한 페이지에서만 긁어옴
웹크롤링 - 페이지 by 페이지로 넘나들며 긁어옴
	- selenium(브라우저 통하기 때문에 느림) vs. 파이썬에서 바로 웹크롤링. 둘다 사용할 줄 알아야 함. 
	- <web scraping with python 한빛미디어> 책 good
크롤러 너무 많이 사용하면 엄청난 transaction으로 서버 마비될 수 있음. 그래서 '로봇이 아닙니다' 이런 거 생김. 

구글링 실력도 실력이다. 수업 듣고 이해만 하고 공부하지 말아라. 문제로 공부하고 모를때만 찾아보면 자동공부^~^
나중에 구글 "코렙"도 사용해볼것 => 구글드라이브랑 연동돼서 편함
 : 클라우드 기반 플랫폼 - GPU(머신러닝을 그래픽카드를 이용해 효율적으로 돌릴 수 있게)와 TPU(머신러닝 자체에 최적화된 칩셋 자체를 만듦) 사용가능
보통은 pycharm을 더 많이 씀, 우린 anaconda사용(패키지관리 프로그램, 이 안에 spider(==pycharm)있음)
파이썬에도 MATLAB(MathWorks회사에서 만듦)처럼 행렬 연산, 신호처리 등 수치 연산 도와주는 패키지 있다
	-> 학생땐 무료이용가능, 졸업하면 라이선스 비용만 한사람당 1억 넘게 주고 사야됨....ㅜ - 회사에서 불법 소프트웨어 많이 사용^^,, 개인은 불법 다운받아도 안 잡음ㅋ 그래서 회사에서 내맘대로 깔면 큰일남....!!!!!
데이터분석은 걍 다 파이썬으로함
보통 거의 64bit쓰고 우리도 64bit쓰지만, 수업은 쉽게 32bit로 예를 들어 진행할 예정.
코드 공유 https://replit.com
머신러닝 코드 https://github.com/wikibook/ml-definitive-guide
완전 복잡한 산식 계산 https://www.wolframalpha.com
드래그 + F9 : 부분 실행
cowork할 때 변수명, 함수명 이름만 들어도 알 수 있게 하기
함수는 "함수명에 해당하는 그 기능만!!" 수행하게 해야 나중에 문제 생겼을 때 좋음
딥러닝할 때 GPU가 있으면 속도가 엄청 좋아지는데, 우리는 GPU가 없다. 대신 google colab (구글에서 제공하는 클라우드 서버, 구글드라이브와 연동) 사용하면 GPU 쓸 수 있다. → GPU 갖고 있는 구글의 데이터센터에 내 코드만 보내서 돌려볼 수 있음. 무료로 하면 시간제한 있긴 함. 
%matplotlib inline => jupyter notebook 플랫폼에서 사용하는 매직 코드. 플랫폼마다 매직코드 다름. spider에서 가끔 오류날때도 있는데 그냥 주석처리하든 말든 경고만 뜨고 알아서 처리함. 

딥러닝 : 기계학습을 하는 layer가 여러 층일 때부터
머신러닝 패키지
  - 텐서플로우[Tensorflow]
  - 케라스[Keras]		(텐서플로우가 다루기 어려워서 api식으로 만든 것)
  - 사이킷런[Scikit-Learn]	(딥러닝 외의 머신러닝을 위함. 多사용. kaggle에서도 多)
행렬/선형대수/통계 패키지
  - 넘파이[NumPy]
  - 사이파이[SciPy]
데이터 전처리 패키지
  - 판다스[Pandas]		파이썬의 리스트, 컬렉션, 넘파이 등의 내부 데이터 뿐 아니라 CSV등의 파일을 쉽게 DataFrame으로 변경해서 데이터의 가공/분석을 쉽게 만들어준다
			판다스의 핵심 객체는 DataFrame
시각화
  - 맷플롯립[Matplotlib]
  - 시본[Seaborn]

실제 ML(Machine Learning) 모델을 생성하고 예측하는 데 있어서 ML알고리즘이 차지하는 비중은 10이고, 그보다 
데이터를 전처리하고 적절한 피처(Feature)를 가공/추출하는 부분이 훨씬 중요한 90을 차지한다. 
문제발생하면 1. 전처리 2. 파라미터설정

■■■■■■■■■■■■■0405 수업中 새로 알게된것■■■■■■■■■■■■■

32bit vs 64bit란?
네모난 메모리 공간 코드-데이터-힙-스택 영역은 수많은 이진수 묶음들로 차있다.
(하나의 작은 마이크로칩(메모리IC)을 확대하면 수많은 작은 트랜지스터들이 엄청 연결되어 있는 걸 볼 수 있다)
1bit에 이진수 1개 저장되는데(전기적 신호로 1이면 on, 0이면 off) 그 이진수 한 묶음의 폭이 64bit(==8byte)면 32bit(==4byte)보다 더 많은 명령을 내릴 수 있음
이진수(하드웨어)에 가장 가까운 언어:어셈블리어. ex) 	mov	1	0x8000
						lrd	0x8000	#32
						str	..	.......
어셈블리어의 STOP==0000, LD==0001, ST==0010, MOVAC==0011, MOV==0100, ADD==0101, ...이렇게 약속된 명령어들이 이진수로 나열된 것이다. 

데이터 타입이 다르다 == 메모리에 저장되는 형태와 방식이 다르다
	cf)파이썬은 변수 하나하나가 객체라서 실제 메모리 상에 c언어처럼 명확하게 저장x, 근데 그딴거 알필요 없어!
16이라는 십진수는 이진수 10000로 변환하여 한 줄이 32bit인 마이크로칩에 000000000...10000 라고 저장
"python"이라는 문자열은 문자 그대로 저장할 수 없기 때문에(전기적신호로 변환한 이진수만 트렌지스터에 저장 가능) => 아스키 코드값(최대사이즈가 127(<2^8)이므로 1byte==8bit)으로 => 또 이진수로 바꿔 저장
12433.45라는 실수는 32bit의 영역을 둘로 나눠서, 앞영역에는 1.243345를 저장하고 뒷영역에는 10^4를 곱해야 하므로 소수점 위치를 뜻하는 4(지수)를 저장
c언어에서는 문자와 문자열이 다른 데이터 타입이지만, 파이썬은 차이 없음

진수란? 한 자리수를 몇 가지 숫자로 표현하는가
16진수 : 0 1 2 3 4 5 6 7 8 9  A  B  C   D  E   F  10 11 12 .. =>헷갈리니까 앞에 0x
10진수 : 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ..
16진수 사용 이유?
1. 엄청 큰 2진수는 두 수가 같은지 비교하는 것도 어려움. 바꿔야 함. 
2. 10진수로 바꾸는 건 어렵지만, 16진수로 바꾸는 건 쉽다. 
1101 0001 1000 1011 0011 (2진수 4개씩 끊어 보기 <= 16진수는 한 자리수를 표현하는 데 16가지 숫자가 필요하고, 2진수의 4비트로 나타낼 수 있는 경우의 수는 2^4=16이므로)
D     1      8      B      3	=> 0xD18B3

문자열 표현 키워드는 "~"와 '~' 둘 다 가능. 문자열 안에 "(큰따옴표) 자체가 있을 때엔 '~'로 묶고, 반대도 마찬가지
문자열 안의 \n은 문자열이 아니라 특정한 기능을 하는 키워드라고 인터프리터와 약속함(print()함수 안에 그런 코드가 이미 다 적혀 있음)

■■■■■■■■■■■■■0406 수업中 새로 알게된것■■■■■■■■■■■■■

하나의 변수에 여러 줄의 문자열을 넣고 싶을 때 전체를 A='''~''', B="""~""" 처럼 묶어준다
print() 함수에 마지막에 디폴트값으로 자동으로 개행문자라는 파라미터가 들어간다. print()로 개행 하기 싫다면 end=""로 마지막 파라미터 다시 설정해줌


★
문자열 + → 연결		//a="a""b", b="a"+"b" 둘 다 "ab"로 연결됨
문자열 * → 곱한 만큼 반복
문자열 요소는 바꿀 수 X !!! (tuple이라는 데이터타입도 요소 추가는 가능하지만 삭제는 불가능)
요소 하나를 바꾸려면 문자열 슬라이싱[:]을 통해 새로운 문자열 만들어야 함
문자열 값 대입 식으로 수정 불가능
문자열 슬라이싱 가능
len(문자열) → 인풋값으로 들어온 문자열의 요소(element)의 개수
문자열.count('문자열a') → 문자열 중 문자열a의 개수 리턴
문자열.find('문자열a') → 문자열 중 문자열a가 처음 나온 인덱스 리턴	// 없으면 -1 리턴 (코드 안 멈추고 계속 진행)
문자열.index('문자열a') → 문자열 중 문자열a가 처음 나온 인덱스 리턴	// 없으면 error (코드 멈춤 => 예외처리필요)
'새문자열'.join(문자열or리스트or튜플) → 문자열or리스트or튜플의 각 요소 사이사이에 새문자열 삽입
	A='abcd'
	C=', '.join(A)	____  A='abcd', C='a, b, c, d'
	A=['a', 'b', 'c', 'd']	   //리스트 요소가 모두 문자열일 때만 가능!!
	C=', '.join(A)	____  A=['a', 'b', 'c', 'd'], C='a, b, c, d'     //string내장함수니까 결과값 C도 string
문자열.upper() / 문자열.lower() → 문자열을 대문자로 / 소문자로
문자열.lstrip() / 문자열.rstrip() / 문자열.strip() → 왼쪽 / 오른쪽 / 양쪽 공백 지우기
문자열.replace('a', 'b') → 문자열 안의 a를 b로 바꾸기
문자열.split('문자열a') → 문자열을 문자열a를 기준으로 나눔. 문자열a 안 넣어주면 공백 기준
★


%d, %c, %s, %f, %o(8진수), %x(16진수), %%(%) 등 형식지정자도 \n처럼 문자 그대로가 아니라 파이썬 인터프리터와 약속된 키워드의 역할 함
%s는 예외적으로 어떤 자료형 값을 넣든 알아서 해줌

___format 1. %사용___
>>> print("I eat %d apples so I was sick for %s days"%(3, "five"))
>>> A=[1, 2, 3, 4, 5]
      for idx, a in enumerate(A):
      print("I eat %d apples. "%A[idx])
-------------------------------------------------------
>>> I eat 3 apples so I was sick for five days
>>> I eat 1 apples. 
      I eat 2 apples. 
      I eat 3 apples. 
      I eat 4 apples. 
      I eat 5 apples. 

___format 2. format함수 사용___
>>> number=10
      day="three"
      print("I ate {0} apples so I was sick for {1} days. ".format(number, day))
>>> print("I ate {number} apples so I was sick for {day} days. ".format(number=10, day="three"))
-------------------------------------------------------
>>> I ate 10 apples so I was sick for three days. 
>>> I ate 10 apples so I was sick for three days. 

{0:<10}, {1:<10}, ... 10칸 확보 후 왼쪽 정렬
{0:>10}, {1:>10}, ... 10칸 확보 후 오른쪽 정렬
{0:^10}, {1:^10}, ... 10칸 확보 후 가운데 정렬
{0:=<10}, {1:=>10}, {2:=^10}... 10칸 확보 후 각각 정렬, 공백 =로 채우기
{0:!<10}, {1:!>10}, {2:!^10}... 10칸 확보 후 각각 정렬, 공백 !로 채우기

___format 3. f문자열 포매팅 사용 : 쉬움!___
>>> name='홍길동'
      age=30
      print( f"나의 이름은 {name}입니다. 나이는 {age}입니다. ")
>>> d = {'name'='홍길동', 'age':30}	#딕셔너리 (key,value) 이렇게 사용할수있음
      print( f"나의 이름은 {d["name"]}입니다. 나이는 {d["age"]}입니다. ")
----------------------------------------------------------------
>>> 나의 이름은 홍길동입니다. 나이는 30입니다.
>>> 나의 이름은 홍길동입니다. 나이는 30입니다.


★
리스트 + → 연결		//대신 둘 다 타입 같아야 함. 타입 캐스팅 필요
			3+"hi"	   (x)에러
			str(3)+"hi"   (o)
리스트 * → 곱한 만큼 반복
len(리스트) → 인풋값으로 들어온 리스트의 요소(element)의 개수 리턴
	↓↓↓얘네 대부분은 원본 데이터 자체를 건드림↓↓↓
리스트 값 대입 식으로 수정 가능
리스트 슬라이싱 가능
del(리스트[인덱스]) → 해당 인덱스 요소만 삭제
리스트.append(요소) → 맨 뒤에 요소 추가
	a.append()를 인터프리터가 읽었을 때 "a의 데이터 타입에서 지원하는 내장 함수 중에서 append()를 실행하겠다"라고 이해하는데
	앞에 a=[]가 정의되지 않는다면 애초에 a를 모르기 때문에 에러
리스트.sort() → 오름차순 정렬
리스트.reverse() → 현재 리스트를 역순으로 나열
리스트.index(인덱스) → 리스트의 해당 인덱스의 요소 리턴
리스트.count(x) → 리스트 중 x의 개수 리턴
리스트.insert(a, b) → 리스트의 a번째 위치에 b 삽입
리스트.remove(x) → 리스트에서 처음 나오는 x 삭제
리스트.pop() → 스택(후입선출) 개념. 맨 마지막 요소 리턴하고 그 요소는 리스트에서 삭제
리스트.pop(x) → x번째 요소 리턴하고 그 요소는 리스트에서 삭제
리스트.extend(리스트a) → 리스트+=리스트a 와 같음
sum(리스트) : 모든 요소의 합 리턴
★


■■■■■■■■■■■■■0407 수업中 새로 알게된것■■■■■■■■■■■■■


★
튜플 + → 연결		//대신 둘의 타입 달라도 됨
튜플 * → 곱한 만큼 반복
리스트 a=[1,2,3]에서 수정(a[1]=3→a=[1,3,3]), 삭제, 추가 모두 가능
튜플   a=(1,2,3)에서는 수정, 삭제 불가능, 추가만 가능
	//문자열과 동일
튜플 값 대입 식으로 수정 불가능
튜플 슬라이싱 가능
len(튜플) → 인풋값으로 들어온 튜플의 요소(element)의 개수 리턴
★


문자열, 리스트, 튜플은 모두 순차적으로(sequential=인덱스부여되는 자료형이네~) 인덱스를 부여하여 인덱스로 요소에 접근 가능
딕셔너리 != 순차적. key값을 통해서 원하는 것만 바로 찾는다. 인덱스를 key값이 대신.

딕셔너리 주의사항	→ 1. 딕셔너리의 key값은 단일요소여야 함. 
			문자열, 튜플, key값 고정된 딕셔너리 (o)	리스트, key값 변하는 딕셔너리 (x)
		→ 2. key값은 모두 달라야 함. 만약 같으면 마지막만 제외하고 나머지 모두 무시


★
딕셔너리 a={1:'a'}에서
추가 : a[2]='b' → a={1:'a', 2:'b'}
삭제 : del a[1] → a={2:'b'}
딕셔너리.keys() → 딕셔너리의 모든 key값을 담은 (리스트와 호환 가능하도록 정의된) dict_keys 클래스 객체를 리턴 → 리스트로 형변환하거나 for문으로 요소 하나씩 빼냄
			//보통 모든 건 형변환과 for문을 통해 내가 쉽게 사용할 수 있게 만들어져있음
딕셔너리.values() → 딕셔너리의 모든 value값을 담은 (리스트와 호환 가능하도록 정의된) dict_values 클래스 객체를 리턴 → 리스트로 형변환하거나 for문으로 요소 하나씩 빼냄
딕셔너리.items() → 딕셔너리 모든 쌍을 튜플로 묶은 값을 담은 (리스트와 호환 가능하도록 정의된) dict_items 클래스 객체를 리턴 → 리스트로 형변환하거나 for문으로 요소 하나씩 빼냄
딕셔너리.clear() → 딕셔너리 쌍 모두 삭제
딕셔너리.get(key값a) → a에 해당하는 value값 리턴
딕셔너리.pop(key값a) → a에 해당하는 value값 리턴하고 삭제	//딕셔너리는 sequential하지 않으므로 스택 개념이 x
key값a in 딕셔너리 → a가 딕셔너리 안에 있는지 T/F
	A='spring'	'sp' in A → True 리턴
	A={'name':'pey'}	'name' in A → True 리턴
sum(튜플) : 모든 요소의 합 리턴
★


집합 자료형 set
1. 중복x   unique한값만 의미있음
2. 순서x

s1=set([1,2,3,4,5,6]), s2=set([4,5,6,7,8,9])일 때
s1 & s2 : 교집합 → {4,5,6}
s1 | s2 : 합집합 → {1,2,3,4,5,6,7,8,9}
s1 - s2 : 차집합 → {1,2,3}

//가장많이쓰는 방법 : 하나의 row 또는 하나의 column을 리스트로 가져옴 → 집합자료형으로 캐스팅해서 하나씩 걸러냄 → 다시 리스트로 캐스팅
A=[1,4,2,3,1,2]
SA = set(A) 	//{1,4,2,3}
NEWA = list(SA) 	//[1,4,2,3]

자료형의 참과 거짓♣
문자열	"python"	       T     |     리스트	[1,2,3]	  T
	""	       F     |		[]	  F
숫자형	0이아닌숫자    T     |     튜플	(1,2,3)	  T
	0	       F     |		()	  F
None		       F     |     딕셔너리	{1:'dic'}	  T
		             | 		{}	  F

a=[1,2,3]
b=a		//a is b==True, id(a)==id(b). 완전히 동일한 주소값을 참조
b=a[:], b=copy(a)	//element만 동일할 뿐 다른 주소. element만 복사됨

■■■■■■■■■■■■■0407~0408 스파이더 코드(gihu.csv데이터 사용)■■■■■■■■■■■■■

import csv                  #csv라는 패키지(csv 파일_:통계데이터 많이 담음_을 처리할 수 있게 해줌) 가져옴

f=open('C:/jeon/gihu.csv') #이 경로의 파일을 열고 불러와 객체로 f에 담기 (더블클릭 따닥-!)
main_data = csv.reader(f)   #내장함수/패키지에구현된함수/모듈안의함수 모두 .을 통해 불러옴
                            #f를 육안으로 확인하기 위해 csv.reader()함수의 인수로 넣어 리턴값을 main_data에 넣음
print(main_data)            #<_csv.reader object at 0x000002CD479F6A00> 출력됨(매번바뀜)
                            #위의 16자리 16진수 자리의 주소(16==2^4이므로 16진수 1자리==2진수 4자리. 즉 16진수 16자리==2진수 64자리==64bit : 메모리 한줄에 64bit 크기의 데이터를 넣음)에 저장되어 있다는 뜻

#필요한 데이터만 main_pt에 모으기
temp=[]                     #main_data가 reader타입이기 때문에, temp라는 리스트에 리스트 타입으로 바꿔 담아줄거임 
for row in main_data:       #순차적인 요소가 있는 main_data에서 한줄씩 row에 불러와서
    temp.append(row)        #data라는 리스트에 row를 순차적으로 추가
f.close()                   #파일 닫기

main_pt=temp[8:]           #temp의 핵심 데이터(12번째 row부터 끝 row까지)만 담기

'''
참고 1. 
리스트 슬라이싱은 자동으로 복사를 해와서 가져오는 것이기 때문에 main_pt=temp[12:]한다고 해서 a=b에서 아예 동일한 주소를 참조하게 되는 것과는 다름!! main_pt[0] = 1 해보면 main_pt에서만 바뀐다 

참고 2.
temp=[]
for row in main_data[12:]: #참고로 얘는 이렇게 슬라이싱이 안 된다. 오류남... 
    temp.append(row)
'''

#4번째 index만 비교해서 최고기온의 최댓값과 날짜 구하기
max=float(main_pt[0][4])
for idx, row in enumerate(main_pt):
    try:
        if row:         #오류 원인 2 해결 ========> row의 타입은 list. list가 True(==요소가 하나라도 있다)인 경우만 거르기 
            if row[4]:  #오류 원인 1 해결 ========> row[4]의 타입은 string. string이 True(==''이 아니다)인 경우만 거르기
                if float(row[4])>max:
                    max=float(row[4])
                    max_index=idx   #최댓값을 max에 업데이트하는 바로 그 순간의 for loop의 index. 
    except Exception as e:
        print(idx, e)   #어디서 오류 발생했는가 => 이 부분 출력해보면 오류 원인 알 수 있음 ↓↓↓
#print(main_pt[39758])  ========> 39758 could not convert string to float: ''    오류 원인 1 => ['2017-10-12', '108', '11.4', '8.8', '']. 4번째인덱스 비어있구나~
#print(main_pt[41033])  ========> 41033 list index out of range                  오류 원인 2 => []. 아예 리스트가 비어 있었구나~
print(main_pt[max_index][0], max)

'''
내가 했던 방법 1.
max=float(main_pt[0][4])
for x in range(1, len(main_pt)):
    if len(main_pt[x])<4:   #오류처리 인덱스 4까지 없을 때
        continue
    if main_pt[x][4]=='':   #오류처리 인덱스 4에 아무 값도 안 들어있을 때
        continue
    if float(main_pt[x][4])>max:
        max=float(main_pt[x][4])
print(max)

내가 했던 방법 2.
max=float(main_pt[0][4])
for row in main_pt:
    if len(row)<4 or row[4]=='':    #오류처리 인덱스가 4까지 없거나 빈 string일 때
        continue
    if float(row[4])>max:
        max=float(row[4])
print(max)

참고 1.
enumerate : for문이 돌면서 loop 횟수에 인덱스를 0, 1, 2, ... 부여해 idx에 넣는다. row에는 원래 for문처럼 temp의 요소 하나씩 가져온다. 
enumerate는 많이 쓰이니까 그냥 습관적으로 for문에 붙여주면 됨.

참고 2.
for idx, row in enumerate(temp):
    try:                    #try-except: try문에 묶인 코드 상에서 만약 에러가 발생했다면, 일단 실행은 stop하지 말고 try문 스킵한 다음에 except문을 대신 실행해줘!!!
        if row[4]:
            float(row[4])
    except Exception as e:  #에러가 발생하지 않는다면 실행되지 않는다 
        print(idx, e)
        print(row)
'''

■■■■■■■■■■■■■0409 수업中 새로 알게된것■■■■■■■■■■■■■

in vs not in
리스트		1 in [1,2,3] == True		1 not in [1,2,3] == False
튜플		'a' in ('a','b','c') == True		'a' not in ('a','b','c') == False
문자열♣		'py' in 'python' == True		'py' not in 'python' == False
딕셔너리		'name' in {'name':'pey'} == True	'name' not in {'name':'pey'} == False

조건문에 아무 것도 넣고 싶지 않다면 pass 쓰거나 아무거나 프린트라도 해야 함

input()에는 임시 데이터 저장 공간인 버퍼가 있음. 이 버퍼 안에 \n(개행, 즉 enter)가 들어가는 순간 처음부터 \n 앞까지의 모든 입력값을 하나의 문자열로 묶어서 리턴해준다. 

무한루프 중단법 : 정지 버튼 누르거나 Ctrl+C

range(부터, 이전까지, 만큼씩증가)

a=[1, 2, 3, 4]
result = [num*3  for num in a  if num%2==0]
           a------  b------------  c--------------
b→c→a 순서로
b. a에서 num을 하나씩 가져오는데
c. 조건문을 만족한 짝수만
a. 3배를 하여 result에 담는다

■■■■■■■■■■■■■0412 수업中 새로 알게된것■■■■■■■■■■■■■

import csv 

f=open('C:/jeon/gihu.csv')
main_data = csv.reader(f) 
print(main_data) 

#필요한 데이터만 main_pt에 모으기
temp=[]                   
for row in main_data:   
    temp.append(row)  
f.close()      

#temp의 핵심 데이터(12번째 row부터 끝 row까지)만 담기
main_pt=temp[8:]           

#내 생일 당일의 최고기온 구하기
for idx, row in enumerate(main_pt):
    if row and row[0] and row[4]:
        if row[0]=='2000-09-01':    #'2000-09-01' in row[0]도 여기서는 답이 맞긴 함. 
            print(row[4])

======================================================================

import csv
import matplotlib.pyplot as plt 	#matplotlib패키지 안의 pyplot모듈(그래프 그리기 위해서 필요)을 불러와 여기서 plt라 부르겠다 

f=open("C:/jeon/ingu.csv")
main_data=csv.reader(f)
temp=[]
for row in main_data:
    temp.append(row)
f.close()
main_pt=temp[1:]


#청담동의 총 인구 수를 int로 출력
for idx, row in enumerate(main_pt):
    if '청담' in row[0]:
        print("청담동 총 인구 수 :", int(row[1].replace(",", '')))


#총 인구 수가 가장 많은 동을 찾아서 출력
maxofdong=int(main_pt[2][1].replace(",", ''))
maxidx=2
for idx, row in enumerate(main_pt):
    #(나오기 직전까지 잘라서 마지막에 공백 있으면 continue, 아닐 때만 생각
    if row[0][:row[0].index('(')][-1]==" ":
        continue
    newrow1 =int(row[1].replace(",", ''))
    if newrow1>maxofdong:
        maxofdong=newrow1
        maxidx=idx
print("총 인구 수가 가장 많은 동 :", main_pt[maxidx][0], maxofdong, "명")


#그래프 그리기 : spider의 Plots 탭에서 확인 가능 
A=[1,5,8,8,4,3,2]
plt.plot(A) 		#plot()의 인수로는 수치적으로 판단 가능한 타입만 


#신중동에서 0~100세이상 까지의 그래프 그리기
shinjoong_bothsex =[]   	#슬라이싱 바로 하니까 int형으로 각각 넣을 수가 없어서 이렇게 바꿈
for x in range(3, 104):
    shinjoong_bothsex.append(int(main_pt[maxidx][x].replace(",", '')))
plt.plot(shinjoong_bothsex)


#xx동 입력받고 있으면 인덱스 구하기
dong=input("동 이름을 입력하세요:")         
for idx, row in enumerate(main_pt):
    if row and row[0]:
        if ' '+dong+'(' in row[0]:
            break
if idx==3845 and '예래동'!= dong:
    print("해당하는 동이 없습니다.")
else:       #xx동에서 0~100세이상 까지의 그래프 그리기
    dong_bothsex=[]
    for x in range(3, 104):
        dong_bothsex.append(int(main_pt[idx][x].replace(',', '')))
    plt.plot(dong_bothsex)
    
    
#신중동과 가장 유사한 그래프 모양을 가진 동 찾기
'''
ex
           0세      1세      2세      3세
신중동     100      200      150      180
청담동      80      150      170      100
효자동    1000     2000     1500     1800

차이는 청담동이 덜 나지만, 비율로 따져야 함. 효자동이 정답!
abs(신중동 0세/총인구 - xx동 0세/총인구) + abs(신중동 1세/총인구 - xx동 1세/총인구) + ...
'''
ratio_min_different=[main_pt[0][0], 100] #비율차이 가장 적은 동 이름, 그 동과의 비율차이 담을 변수 초기화
shinjoong_ratio=list(map(lambda x:x/sum(shinjoong_bothsex), shinjoong_bothsex))
for idx, row in enumerate(main_pt):
    if row[0][:row[0].index('(')][-1]==" ":
        continue
    if row[0]=='경기도 부천시 신중동(4119074200)':
        continue
    
    each_bothsex=[]
    for x in range(3, 104):
        each_bothsex.append(int(row[x].replace(",",'')))
    if row and row[0] and sum(each_bothsex)>0:
        each_ratio=list(map(lambda x:x/sum(each_bothsex), each_bothsex))
    
    sum_all_abs=0
    for n in range(len(shinjoong_ratio)):
        sum_all_abs += abs(shinjoong_ratio[n]-each_ratio[n])
    if sum_all_abs < ratio_min_different[1]:
        ratio_min_different[0]=row[0]
        ratio_min_different[1]=sum_all_abs
print(ratio_min_different)

■■■■■■■■■■■■■0413 수업中 새로 알게된것■■■■■■■■■■■■■

분기문 ===> 어떤 메모리 주소에서 어떤 메모리 주소로 jump하게 하는 것
-break : 반복문 빠져나간다
-return : 함수를 빠져나가면서 최종값을 던져준다
-goto(c언어) code_a : code_a로 돌아간다(도돌이표) 	//가독성 더러워서 회사에서 절대 안씀

함수의 관점에서 output이란 오직 리턴값만 의미. print()로 뭔가 출력했다고 해서 output이 있다는 건 아니다
output만이 함수가 호출된 곳에서 리턴되어 담길 수 있음. 리턴값 없으면 NoneType의 None 담김		//오류x
cf. 데이터 다루다 보면 빈 공간이  NA, None 등으로 자동적으로 채워진 것을 볼 수 있음. 

매개변수 지정해서 넘겨주기, 초기값 설정
def add(b, a, bool=True):	//매개변수 초기값 미리 설정
    if bool:
        return a+b
print(add(a=3, b=7))	//인수랑 매개변수 이름이 같아야 함. 순서 다르지만 b=7, a=3으로 들어감

여러 개의 input값 받는 파라미터 *args
def add_many(a, *args):
    result=0
    print(a)
    for i in args:		//args == (1, 2, 3, 4, 5), type(args) == 튜플, 요소에 하나씩 접근하려고 for문 사용
        result+=i
    return result
print(add_many("Add", 1, 2, 3, 4, 5))

딕셔너리 쌍을 input으로 받는 키워드 파라미터 **kwargs
def print_kwargs(**kwargs):
    print(kwargs)	
print_kwargs(a=1)		//딕셔너리 쌍 'a' : 1을 인수로 넣어줌
----------------------------------------
>>>{'a' : 1}

리턴값 2개 이상이면 무조건 하나로 묶인 (튜플)로 리턴됨♣	    //오류x

함수 안의 변수 vs. 함수 밖의 변수♣
a=1
def vartest(a):	//이름은 같지만 함수 밖의 a와 완전히 다른 변수(주소는 다르고 안에 담긴 value값만 같음. )
    a=a+1	
vartest(a)		//a의 주소 자체가 아니라, a가 가리키는 주소에 담긴 value값만을 넘겨준 것
print(a)
---------------------------------------
>>>1		//2가 아니라 1이 출력
함수가 실행되면 '데이터 메모리 공간'(함수 밖)에서 '스택 메모리 공간'(함수 안)으로 넘어감. 
함수가 끝나자마자 다시 '데이터 메모리 공간'으로 들어오고, 사용된 '스택 메모리 공간'은 사라짐.
return을 통해 '데이터 메모리 공간'까지 전달해주어서 함수 안에서 벌어진 무의미한 일이 유의미해짐

global 최대한 사용x. 
함수 내부와 외부의 연결은 매개변수와 리턴값 둘 뿐인 것으로 통일하면 좋다. 
디버깅할 때는 global 종종 쓰임. 함수 내부 변수는 함수 끝나면 사라지니까 스파이더의 Variable explorer 탭에서 값 확인 불가능해서 global로 정의해서 확인할 수 있으므로

lambda
함수를 한 줄로 간결히 생성할 때 사용
add = lambda a, b : a+b	//add라는 함수는 a와 b를 매개변수로 받아 a+b를 리턴한다
print(add(3, 4))
--------------------------
>>>7

■■■■■■■■■■■■■0414 수업中 새로 알게된것■■■■■■■■■■■■■

input() 실제로 쓰면 x !!
  1. 컴퓨터가 막 돌아가는데, 돌 때 마다 어떤 레지스터(□□□□□... 이렇게 생긴 메모리)를 확인한다. 
  2. 레지스터의 비트들은 모두 0인데 interrupt가 발생한 순간 특정 비트가 1로 변하는 flag가 발생한다. 
  3. '주인님이 지금 내가 돌고 있는 것보다 우선순위가 높은 명령을 내렸구나!' 
  4. 컴퓨터는 인터럽트 테이블을 보고 ex)'마우스가 움직여서 생긴 인터럽트구나!' 확인하고 마우스 제어에 관한 함수로 점프해서 해결하고 돌아온다
기본 순서가 이런데, input()은 인터럽트를 무시하고 모든 일을 멈춰서 잠시 죽은 상황이 된다. 마우스를 움직이는 것도 처리가 안 됨. 
그래서 실제 어플에서는 input() 절대 쓰면 안 되고 인터럽트에 해당하는 '통신'으로 대체함. 

파이썬은 회사에서 마라톤 테스트(어플이 언제 죽는지 가혹한 환경에서 계속 실험) 할 때, 통신연결(프로토콜 구현한 패키지가 있어서 메시지 주고받기 편함)할 때 많이 쓰임
이 때 몇날 몇시 몇분 몇초에 어떤 문제가 생겨서 어떤 부분이 죽었는지 로그 남길 때 f.write 많이 쓰임. 
얘네가 알아서 계속 해주니까 업무 자동화. 

w  write     쓰기 모드
r   read      읽기 모드
a   append  추가 모드
f.readline()   	#수행될 때마다 맨 위부터 한 줄씩 내려가면서 문자열로 묶어 담아줌
f.readlines()     	#파일의 모든 줄을 읽어서 각각의 줄을 str 요소로 갖는 리스트로 담아줌
f.read()          	#파일 전체를 문자열로 묶어 담아줌

f=open('C:/jeon/python_code/test.txt', 'w')     #f : 파일을 담은 객체
for i in range(1, 11):
    data="%d번째 줄입니다.\n"%i
    f.write(data)
f.close()
f=open('C:/jeon/python_code/test.txt', 'r')
while True:
    line=f.readline()
    if not line: break  #line : str. if line:에 역을 취한 것이 if not line:
    print(line)         #"%d번째 줄입니다.\n"의 개행문자에다가 print()에 자동으로 붙어 있는 개행문자가 겹쳐져서 두 번 개행됨
f.close()

with문 : f.close() 빠뜨리는 실수 안하게. with문 벗어나는 순간 f.close() 자동 처리
with open("test.txt", "w") as f:
    f.write("Life is too short, you need python")

■■■■■■■■■■■■■0415 수업中 새로 알게된것■■■■■■■■■■■■■

a = Cookie() 	//a는 객체, a는 클래스 Cookie()의 인스턴스 (걍 유연하게 생각해)
		//a는 클래스 Cookie() 안에 정의된 모든 메서드를 사용할 수 있게 됨
a.add()와 'python'.replace('p', 'a')에서의 . 사용은 일맥상통하다. 클래스 str 안에 replace라는 메서드가 정의되어 있는 것
파이썬의 각각의 변수들은 사실 클래스 객체임	type('hi'):<class 'str'>, type(1):<class 'int'>, type(1.1):<class 'float'>
클래스는 파이썬의 근간이 되는 개념

Cookie():
    def setdata(self, first, second):	//파이썬은 꼭 self 써야 함♣
        self.fitst=first
        self.second=second
a=Cookie()
a.setdata(4, 2)			//a→self, 4→first, 2→second

생성자 __init__()
인터프리터와 약속된 키워드. 생성자 이름은 __init__으로 정해져있음
객체가 생성되는 시점에 자동으로 호출
사실 상속받은 자식클래스에게 필요한 생성자가 부모클래스 생성자와 같다면 오버라이딩 필요 없지만
초기값 설정 이외에도 쓰이므로 습관적으로 써주자!

클래스 상속
: class 클래스명A(상속할 기존의 클래스명B)
어떤 라이브러리는 이미 컴파일이 끝난 binary, 즉 이진수 형태로 제공되어서 알아볼 수도, 수정할 수도 없게 제공된다
패키지를 만들어서 돈을 버는 회사는 당연히 그대로 보여주지 않는다^^ 이 때 사용됨

메소드 오버라이딩[덮어쓰기]
: A, B에 같은 이름의 메소드가 있다면 B는 무시하고 새로 업데이트된 A의 메소드로 사용
ex_ 나눗셈에서 0으로 나눌 때 원래는 오류뜨는데 새롭게 오류 처리된 메소드 정의하고 싶을 때

객체 변수(self.가 붙음)
생성, 사용 언제나 self. 붙여 써야 함. 각각의 객체에 종속되어 있어서 개별적		//id() 서로 다름

클래스 변수(self.가 붙지 않음)
해당 클래스를 공유하는 모든 객체들이 같이 쓰고 있어서 하나를 바꾸면 나머지도 바뀜	//id() 찍어보면 주소값도 똑같음. 완전히 same

■■■■■■■■■■■■■0416 수업中 새로 알게된것■■■■■■■■■■■■■

#판다스 맛보기 - kaggle - titanic
import csv
import pandas as pd

df_titanic = pd.read_csv("C:/jeon/titanic_train.csv")
'''
df-titanic의 타입: DataFrame, 사이즈:(891, 12) 즉 행렬!, column명과 index가 하나로 묶여 있는 데이터
DataFrame이라는 데이터 타입 안에 Pandas라는 패키지가 있다
Pandas : DataFrame 타입의 데이터를 굉장히 잘 다룰 수 있게 해줌
'''

print(df_titanic.info())
'''
↓출력
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype     #모델링한다 = 가중치를 찾아 낸다. 거기서 null값은 사용할 수 없으니까 채워 넣으라고 Non-Null Count 보여줌. 
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
None
'''

print(df_titanic.describe())
'''
↓출력 (표준편차: 분포 보여줌)
       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare
count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000
mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208
std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429
min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000
25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400
50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200
75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000
max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200
'''
============================================================

가장 많은 오류
-No such file or directory: '없는파일이름'
-division by zero
-list index out of range
cf. None 넣어주는 건 오류 x!!
	def add()의 리턴값이 없을 때 c=add()는 오류 x

예외처리 1. try, except문
try: except:			//모든 예외처리
try: except Exception as e:		//모든 예외처리 - 모든 예외의 에러 메시지를 출력할 때는 Exception을 사용
try: except FileNotFoundError as e: 	//No such file or directory만 예외처리. 나머지는 오류남
try: except ZeroDivisionErrow as e: 	//division by zero만 예외처리. 나머지는 오류남
try: except IndesErrow as e: 	//list index out of range만 예외처리. 나머지는 오류남
        cf. 	except문 여러개 쓰면 여러개 예외처리 할 수 있지만, 
	앞에서 오류가 발생했으면 이미 멈춰서(except문으로 이미 빠져버린다) 
	다음 코드는 실행되지도 않고 오류도 발생하지 않는다.

예외처리 2. try, finally문
형태는 똑같지만 try문 수행 도중 예외가 발생했든 아니든(try문 수행하든 except문 수행하든) finally문 항상 수행

raise 키워드로 오류 일부러 발생시키기
상속해주는 부모 클래스의 특정 메소드가 문제가 있어서 상속받은 자식 클래스에서 무조건 메소드 오버라이딩을 해줬으면 하는 상황에 쓰임
raise 에러 이름 → 해당 에러 발생
	ex_ raise NotImplementedError → 꼭 작성해야 하는 부분이 구현되지 않았을 때 발생
				    → 부모 클래스 메소드에 써둠. 자식 클래스에서 해당 메소드 구현하지 않을 경우 호출

■■■■■■■■■■■■■0419 수업中 새로 알게된것■■■■■■■■■■■■■

★
____________________________[파이썬 내장함수]____________________________
abs(x) → x의 절댓값
pow(x, y) → x^y 
round(x, y) → x를 소숫점 y째 자리까지 반올림 (y 없어도 됨) 
sum(s) → s의 모든 요소의 합
all(x)♣ → x의 모든 '요소'가 참일 때 True, 하나라도 거짓일 때 False
any(x)♣ → x의 하나의 '요소'라도 참일 때 True, 모두 거짓일 때 False
chr(아스키코드값), ord(문자) → 아스키코드에 해당하는 문자 리턴, 문자에 해당하는 아스키코드값 리턴
int(x), hex(x), oct(x) → 정수 x를 10진수, 16진수, 8진수로 변환해서 리턴
	cf___int('0xea', 16)  #지금 '0xea'는 16진수로 표기되어 있는데, 얘를 10진수로 바꿔주세요!
id(x) → 객체 x의 주소값
len(s) → s의 길이, 즉 s 안의 요소의 전체 개수
list(s) → s를 리스트로 만들어 리턴
dir(x) → x의 자료형이 사용할 수 있는 모든 내장함수 리턴
enumerate → for loop 돌린 횟수에 인덱스를 부여. 습관적으로 사용하자!
filter(함수명a, x)♣ → x의 요소 중 함수a 돌렸을 때 리턴값이 참인 것만 묶어서(걸러내서) 리턴
		→ x는 iterate(여러개의 요소 가짐)여야 함!
	>>>def positive(x):
	          return x>0	//양수면 True, 그게 아니면 False
	      print( list( filter(positive, [1,-3,2,0,-5,6])))
	>>>print( filter( lambda x : x>0, [1,-3,2,0,-5,6]))	    //lambda a, b : a+b → input값 a, b, output값 a+b
						    //이러면 함수a 말하면서 x도 써주니까 lambda만 넣어주면 됨
	------------------------------------------------------
	>>> [1, 2, 6]
	>>> [1, 2, 6]
map(함수명a, x)♣ → x의 모든 요소에 a효과를 넣어줌
		→ x는 iterate(여러개의 요소 가짐)여야 함!
	>>>def two_times(x):
	          return x*2
	      print( list( map(two_times, [1,2,3,4]))
	>>>print( map( lambda a : a*2, [1,2,3,4]))
            -------------------------------------------------------
	>>>[2, 4, 6, 8]
	>>>[2, 4, 6, 8]
sorted(s) → s의 요소들을 정렬한 뒤 리스트로 리턴	//리스트.sort()는 리턴해주지는 않는다
zip(동일한 len의 여러 자료)♣ → 같은 인덱스의 데이터끼리 묶어줌
★

모듈 : 파이썬 파일(.py)
여러 모듈(import해오는 것들)이 합쳐지면 : 패키지
여러 패키지가 합쳐지면 : 라이브러리

대화형 인터프리터 : Anaconda prompt 켜서 python 입력하면 >>>형태로 실행됨
if __name__ == "__main__" 이거 여기서 사용하는 것

__init__.py
해당 디렉터리가 패키지의 일부임을 알려준다. 
python3.3버전부터는 굳이 없어도 알아서 "이 디렉터리가 패키지를 위한 것이구나~" 인식해서 오류 x
그래도 python3.3 이하의 하위 버전에서 도는 애플리케이션까지 호환되면 좋으니까 그냥 __init__.py 쓰자!

A디렉터리의 a.py모듈이 B디렉터리의 b.py모듈을 사용하고 싶다면?
a.py 모듈 안에서 from B import b.py 해주고 아래에서 b.py모듈 안의 함수를 사용하면 됨

■■■■■■■■■■■■■0420 수업中 새로 알게된것■■■■■■■■■■■■■

import csv

f=open('C:/jeon/jihacheol.csv')
main_data=csv.reader(f)
temp=[]
for row in main_data:
    temp.append(row)
main_pt=temp[2:]
f.close()


#출근시간 7~9시에 가장 많은 하차 인원(11idx, 13idx만 확인)이 카운팅되는 역 찾기
maxgetoff_7to9=int(main_pt[0][11].replace(",", ''))+int(main_pt[0][13].replace(",", ''))
maxidx_7to9=0
for idx, row in enumerate(main_pt):
    if row and row[3] and row[11] and row[13]:
        newgetoff=int(row[11].replace(",", ''))+int(row[13].replace(",", ''))
        if newgetoff>maxgetoff_7to9:
            maxgetoff_7to9=newgetoff
            maxidx_7to9=idx
        
print(main_pt[maxidx_7to9][3], maxgetoff_7to9)
print("7~8시 :",main_pt[maxidx_7to9][11])
print("8~9시 :",main_pt[maxidx_7to9][13])


#모든 역에 대해 [역이름, 승차총합(모든시간대), 하차총합]을 요소르 담는 리스트 만들기
def getsum(n):			#함수 쓰면 리스트 다시 안 비워줘도 되니까 better
    sum_all=0
    for idx2, column in enumerate(row[4:52]):
        if n:
            if not idx2 %2:
                sum_all+=int(column.replace(',',''))
        else:
            if idx2 %2:
                sum_all+=int(column.replace(',',''))
    return sum_all
sum_all_onoff=[]
for idx, row in enumerate(main_pt):
    each_all_onoff=[]
    each_all_onoff.append(row[3])    	#역이름 
    each_all_onoff.append(getsum(1)) 	#승차총합
    each_all_onoff.append(getsum(0)) 	#하차총합
    sum_all_onoff.append(each_all_onoff)


#시간대별 가장 많은 승차인원, 가장 많은 하차인원 리스트로 만들기
#[시간, 역이름, 승차인원, 역이름, 하차인원]
def get_max(n): #(n인덱스가 최대인 역이름, 그 역의 n인덱스값) 튜플로 리턴
    max_idx=0
    max_men=int(main_pt[0][n].replace(",",''))
    for idx, row in enumerate(main_pt):
            if int(row[n].replace(",",''))>max_men:
                max_idx=idx
                max_men=int(row[n].replace(",",''))
    return main_pt[max_idx][3], max_men
def get_time(n): #시간 구하기
    time=n+4
    if time>=24:
        time-=24
    return str(time)+"시"
final=[]
for x in range(24):
    temp=[]    
    max_on=get_max(4+2*x)
    max_off=get_max(5+2*x)
    temp.append(get_time(x))
    temp.append(max_on[0])
    temp.append(max_on[1])
    temp.append(max_off[0])
    temp.append(max_off[1])
    final.append(temp)


#다른방법_1
def get_time(n): #시간 구하기
    time=n+4
    if time>=24:
        time-=24
    return str(time)+"시"
pol=[]
for _ in range(48):
    pol.append([0,0])
for idx, row in enumerate(main_pt):
    for idx2, row2 in enumerate(row[4:-1]):
        if int(row2.replace(",",''))>pol[idx2][1]:
            pol[idx2][0]=row[3]
            pol[idx2][1]=int(row2.replace(",",''))
final1=[]
for x in range(24):
    temp1=[]
    temp1.append(get_time(x))
    temp1.append(pol[x*2][0])
    temp1.append(pol[x*2][1])
    temp1.append(pol[x*2+1][0])
    temp1.append(pol[x*2+1][1])
    final1.append(temp1)


#다른방법_2
MaxMan_counted_list = [0]*48
print(len(MaxMan_counted_list))
station_name_list = [0]*48
temp_list = []
total_list = []

for idx, row in enumerate(main_pt):
    for idx2, row1 in enumerate(row[4:52]):
        if MaxMan_counted_list[idx2] < int(row1.replace(",","")):
            MaxMan_counted_list[idx2] = int(row1.replace(",",""))
            station_name_list[idx2] = str(main_pt[idx][3]) + '_' + str(idx+2)  #그냥 역ID도 같이 출력해봄

for idx, row in enumerate(range(24)):
    temp_list.append(temp[0][idx*2 + 4])
    temp_list.append(station_name_list[idx*2])
    temp_list.append(MaxMan_counted_list[idx*2])
    temp_list.append(station_name_list[idx*2 + 1])
    temp_list.append(MaxMan_counted_list[idx*2 + 1])
    total_list.append(temp_list)
    temp_list = []

■■■■■■■■■■■■■0421 수업中 새로 알게된것■■■■■■■■■■■■■

import csv
import matplotlib.pyplot as plt 	#matplotlib패키지 안의 pyplot모듈(그래프 그리기 위해서 필요)을 불러와 여기서 plt라 부르겠다 

f=open("C:/jeon/ingu.csv")
main_data=csv.reader(f)
temp=[]
for row in main_data:
    temp.append(row)
f.close()
main_pt=temp[1:]


#청담동의 총 인구 수를 int로 출력
for idx, row in enumerate(main_pt):
    if '청담' in row[0]:
        print("청담동 총 인구 수 :", int(row[1].replace(",", '')))


#총 인구 수가 가장 많은 동을 찾아서 출력
maxofdong=int(main_pt[2][1].replace(",", ''))
maxidx=2
for idx, row in enumerate(main_pt):
    #(나오기 직전까지 잘라서 마지막에 공백 있으면 continue, 아닐 때만 생각
    if row[0][:row[0].index('(')][-1]==" ":
        continue
    newrow1 =int(row[1].replace(",", ''))
    if newrow1>maxofdong:
        maxofdong=newrow1
        maxidx=idx
print("총 인구 수가 가장 많은 동 :", main_pt[maxidx][0], maxofdong, "명")


#그래프 그리기 : spider의 Plots 탭에서 확인 가능 
A=[1,5,8,8,4,3,2]
plt.plot(A) 		#plot()의 인수로는 수치적으로 판단 가능한 타입만 


#신중동에서 0~100세이상 까지의 그래프 그리기
shinjoong_bothsex =[]   	#슬라이싱 바로 하니까 int형으로 각각 넣을 수가 없어서 이렇게 바꿈
for x in range(3, 104):
    shinjoong_bothsex.append(int(main_pt[maxidx][x].replace(",", '')))
plt.plot(shinjoong_bothsex)


#xx동 입력받고 있으면 인덱스 구하기
dong=input("동 이름을 입력하세요:")         
for idx, row in enumerate(main_pt):
    if row and row[0]:
        if ' '+dong+'(' in row[0]:
            break
if idx==3845 and '예래동'!= dong:
    print("해당하는 동이 없습니다.")
else:       #xx동에서 0~100세이상 까지의 그래프 그리기
    dong_bothsex=[]
    for x in range(3, 104):
        dong_bothsex.append(int(main_pt[idx][x].replace(',', '')))
    plt.plot(dong_bothsex)
    
    
#신중동과 가장 유사한 그래프 모양을 가진 동 찾기
'''
ex
           0세      1세      2세      3세
신중동     100      200      150      180
청담동      80      150      170      100
효자동    1000     2000     1500     1800

차이는 청담동이 덜 나지만, 비율로 따져야 함. 효자동이 정답!
abs(신중동 0세/총인구 - xx동 0세/총인구) + abs(신중동 1세/총인구 - xx동 1세/총인구) + ...
'''
ratio_min_different=[main_pt[0][0], 999999] #비율차이 가장 적은 동 이름, 그 동과의 비율차이 담을 변수 초기화
shinjoong_ratio=list(map(lambda x:x/sum(shinjoong_bothsex), shinjoong_bothsex))
final_ratio=[]
for idx, row in enumerate(main_pt):
    if row[0][:row[0].index('(')][-1]==" ":
        continue
    if row[0]=='경기도 부천시 신중동(4119074200)':
        continue
    
    each_bothsex=[]
    for x in range(3, 104):
        each_bothsex.append(int(row[x].replace(",",'')))
    
    each_ratio=list(map(lambda x:x/sum(each_bothsex) if row and row[0] and sum(each_bothsex)>0 else False, each_bothsex))
    '''
    if row and row[0] and sum(each_bothsex)>0:
        each_ratio=list(map(lambda x:x/sum(each_bothsex), each_bothsex))
    이것을 줄여서 lambda문 안에 if문을 넣어줄 수 있다. 대신 else문 꼭 써줘야 함! elif는 불가능!
    '''
    sum_all_abs=0
    for n in range(len(shinjoong_ratio)):
        sum_all_abs += abs(shinjoong_ratio[n]-each_ratio[n])
    if sum_all_abs < ratio_min_different[1]:
        ratio_min_different[0]=row[0]
        ratio_min_different[1]=sum_all_abs
        final_ratio=each_ratio
print(ratio_min_different)
plt.plot(shinjoong_ratio)
plt.plot(final_ratio)

#다른방법
shinjoong_ratio2 = []
for row in temp:
    if "신중동" in row[0]:
        for row2 in row[3:104]:
            shinjoong_ratio2.append(int(row2.replace(",","")) / int(row[2].replace(",","")) ) #lambda를 안 쓰면 이렇게 길어진다
min = 999999
for idx0, row in enumerate(main_pt):
    if row[0]!='경기도 부천시 신중동(4119074200)':
        if row[0][:row[0].index('(')][-1]!=" ":
            each_ratio_substract = []
            try:
                for idx1 , row2 in enumerate(row[3:104]):
                    each_ratio_substract.append(abs(shinjoong_ratio2[idx1] - int(row2.replace(",","")) / int(row[2].replace(",","")) ))
            except:
                print(idx0)
                
            if(sum(each_ratio_substract) != 0 and sum(each_ratio_substract) < min ):
                min = sum(each_ratio_substract)
                final_data = each_ratio_substract
                final_idx = idx0           
print(main_pt[final_idx][0])
dong_reslut = main_pt[final_idx][0][-11:-1]
final_ratio2 = []
for row in temp:
    if dong_reslut in row[0]:
        for row2 in row[3:104]:
            final_ratio2.append(int(row2.replace(",","")) / int(row[2].replace(",","")) ) 
plt.plot(shinjoong_ratio2)
plt.plot(final_ratio2)

■■■■■■■■■■■■■0422 수업中 새로 알게된것■■■■■■■■■■■■■

파이썬에서 '배열의 형태로 만들어라~' == '연산 쉽게 ndarray로 만들어라~'
ndarray타입으로 바꾸면 계산도 빨라지고 행렬 연산도 쉽다
type과 shape(==차원, 형태) 중요!

ndarray 배열의 shape변수는 데이터의 크기를 튜플 형태 ex_(행, 열)로 가짐
array1 = np.array([1,2,3])			array1.shape → 1차원데이터 (series) : (3,)      // , 붙이기
array2 = np.array([[1,2,3], [2,3,4]])		array2.shape → 2x3 shape를 가진 2차원데이터 : (2, 3)
array3 = np.array([[1,2,3]])			array3.shape → 1x3 shape를 가진 2차원데이터 : (1, 3)

A=1에서 A와 같이 파이썬에서 모든 변수는 엄밀히 말하면 하나의 객체이다
type(A)는 <class 'int'>, 즉 int라는 객체. 4byte의 공간에 저장되지만, 그것을 위해 사실 더 많은 공간이 필요함
array1 = np.array([1,2,3])에서 type(array1)은 int32 (32bit 공간 안에만 존재하는 숫자 하나로 바뀐 것) 크기도 작아지고 심플해짐

[1 2 3]처럼 공백으로 구분
ndarray명.dtype → 해당 ndarray의 요소의 데이터 타입 리턴
ndarray는 무조건 동일한 데이터타입 가져야 함.      cf. 리스트 안에는 모든 타입 가능
	[int, int, str]을 ndarray로 만들면 [str str str]로 타입캐스팅 		[1, 2, 'test'] → ['1' '2' 'test']
	[int, int, float]을 ndarray로 만들면 [float float float]로 타입캐스팅 	[1, 2, 3.1] → [1. 2. 3.1] 
		#float형.astype(int형) → 소수점 날라감

np.array()♣
인자를 넣어주면 ndarray 데이터타입으로 변환

np.arange()♣
range()와 똑같은데 ndarray 데이터타입으로 만들어질 뿐

ndarray명.reshape()♣
값은 그대로고 shape만 바꿔줌
사용tip 1. 보통 행, 열 둘중에 하나만 정해주고 나머지 -1 넣어줌
	array1 = np.arange(10)
	array2 = array1.reshape(-1,5)
사용tip 2. 2차원 데이터를 1차원으로 reshape해준 뒤 데이터연산 함수에 input → output값을 다시 2차원으로 reshape
	이미지(디스플레이)는 여러 층의 rgb가 겹쳐져있는 개별 픽셀을 모아놓은 데이터임. 영상은 그 이미지가 프레임 단위로 막~~~ 넘어가는 것. 
	이런 이미지 처리도 shape 막~~ 바꿔가면서 함

tolist()♣
리스트로 변환

불린 인덱싱♣
array1d = np.arange(start=1, stop=10)
array3 = array1d[array1d > 5]	#[ ] 안에 array1d > 5 Boolean indexing을 적용
	> [F, F, F, F, F, T, T, T, T]
	> False값은 무시하고 True값에 해당하는 index만 저장 [5, 6, 7, 8]
	> 저장된 index값으로 데이터 조회 array1d[[5, 6, 7, 8]]=[6 7 8 9]
		#ndarray, DataFrame은 이렇게 접근 가능
★
np.sort(ndarray명)		→ 원본 데이터는 냅두고 결괏값을 리턴
ndarray명.sort()		→ 리턴값 없고 원본 데이터 자체를 수정

보통은 inplace라는 파라미터 설정을 통해 리턴해줄지 말지 결정
inplace = False 로 설정 	→ 원본 데이터는 냅두고 결괏값을 리턴 (default설정)
inplace = True 로 설정  	→ 리턴값 없고 원본 데이터 자체를 수정

[::-1] 			→ 내림차순정렬
np.sort(ndarray명, axis=0)	→ 로우 방향, 즉 로우가 증가하는 방향으로 정렬, axis=0
np.sort(ndarray명, axis=1)	→ 컬럼 방향, 즉 컬럼이 증가하는 방향으로 정렬, axis=1
// 보통 row는 axis=0인데, apply lambda에서는 axis=1
np.argsort()♣		→ 원본 행렬의 인덱스가 정렬되면 어디에 있는지

np.dot(A, B) 		→ A와 B 행렬 내적 (==행렬 곱)
np.transpose(A)		→ A행렬의 전치행렬
cf. 행렬 곱 계산
| 1 2 3 |     | 7  8  |  
| 4 5 6 |  x  | 9 10 |   
               |11 12 |
    ↓            ↓
  2 x 3        3 x 2	     //AxB의 크기 = A의 행 x B의 열
★

Series	   // 컬럼이 1개. 1차원데이터
DataFrame  // 컬럼이 2개 이상. 2차원데이터
DataFrame이라는 형태에서 컬럼이 1개인 것과 Series 인 것은 다르다! (컬럼이 1개라고 무조건 Series는 아님)

인덱싱/슬라이싱	위치기반(iloc)
		명칭기반(loc)
		불린

sort_values() 정렬 	1. by=['컬럼명', '컬럼명', ..]	해당 컬럼을 기준으로 정렬
		2. ascending=True		True:오름차순, False:내림차순
		3. inplace=False		False:원본데이터 안건드림, True:원본데이터 건드림
agg() 			aggregation함수
groupby(by='컬럼명') 	'컬럼명'의 유니크한 값을 기준으로 묶어서 dataframe 만듦. dataframe groupby는 agg()를 써야 처리하기 쉽다.

isna() 		결손 데이터 확인
fillna()		결손 데이터 대체
A.apply(lambda x:...)	똑같이 A는 iterate해야 함. A의 요소에 x가 들어감.

■■■■■■■■■■■■■0423 수업中 새로 알게된것■■■■■■■■■■■■■

import numpy as np
import pandas as pd

array1 = np.array([1,2,3])
print('array1 type:',type(array1))
print('array1 array 형태:',array1.shape) #1차원데이터 (series)
array2 = np.array([[1,2,3],
                  [2,3,4]])
print('array2 type:',type(array2))
print('array2 array 형태:',array2.shape) #2x3 shape를 가진 2차원데이터
array3 = np.array([[1,2,3]])
print('array3 type:',type(array3))
print('array3 array 형태:',array3.shape) #1x3 shape를 가진 2차원데이터 
print('array1: {0}차원, array2: {1}차원, array3: {2}차원'.format(array1.ndim, array2.ndim, array3.ndim))

col_name1=['col1']			# 1개의 컬럼명이 필요함
list1 = [1, 2, 3]
array1 = np.array(list1)
print('array1 shape:', array1.shape )
df_list1 = pd.DataFrame(list1, columns=col_name1)   	#컬럼네임을 col_name1로 지어줌
print('1차원 리스트로 만든 DataFrame:\n', df_list1)
df_array1 = pd.DataFrame(array1, columns=col_name1) 	#컬럼네임을 col_name1로 지어줌
print('1차원 ndarray로 만든 DataFrame:\n', df_array1)

col_name2=['col1', 'col2', 'col3']	# 3개의 컬럼명이 필요함
list2 = [[1, 2, 3],			# 2행x3열 형태의 리스트와 ndarray 생성 한 뒤 이를 DataFrame으로 변환. 
         [11, 12, 13]]
array2 = np.array(list2)
print('array2 shape:', array2.shape )
df_list2 = pd.DataFrame(list2, columns=col_name2)
print('2차원 리스트로 만든 DataFrame:\n', df_list2)
df_array2 = pd.DataFrame(array2, columns=col_name2)
print('2차원 ndarray로 만든 DataFrame:\n', df_array2)

#ndarray는 무조건 동일한 데이터타입 가져야 함.      cf. 리스트 안에는 모든 타입 가능
list1 = [1,2,3]
print(type(list1))
array1 = np.array(list1)
print(type(array1))
print(array1, array1.dtype)

list2 = [1, 2, 'test']
array2 = np.array(list2)
print(array2, array2.dtype) #[int, int, str]을 ndarray로 만들면 [str str str]로 타입캐스팅 → ['1' '2' 'test']
                            
list3 = [1, 2, 3.1]
array3 = np.array(list3)
print(array3, array3.dtype) #[int, int, float]을 ndarray로 만들면 [float float float]로 타입캐스팅 → [1. 2. 3.1] 


#0과 1로 초기화, dtype 설정해주지 않으면 default로 float로 들어감
zero_array = np.zeros((3,2),dtype='int32')
print(zero_array)
print(zero_array.dtype, zero_array.shape)

one_array = np.ones((3,2))
print(one_array)
print(one_array.dtype, one_array.shape)


#reshape()
array1 = np.arange(10)
print('array1:\n', array1)

array2 = array1.reshape(2,5)
print('array2:\n',array2)

array3 = array1.reshape(5,2)
print('array3:\n',array3)

array4 = array1.reshape(-1,5) #뒷자리가 5로 고정되면 앞자리는 무조건 2만 가능 => 알아서 해주세요~
print('array4 shape:',array4.shape)

array5 = array1.reshape(5,-1) #앞자리가 5로 고정되면 뒷자리는 무조건 2만 가능 => 알아서 해주세요~
print('array5 shape:',array5.shape)


#tolist()
array1 = np.arange(8)
array3d = array1.reshape((2,2,2))
print('array3d:\n',array3d.tolist())

array5 = array3d.reshape(-1,1)
print('array5:\n',array5.tolist())
print('array5 shape:',array5.shape)

array6 = array1.reshape(-1,1)
print('array6:\n',array6.tolist())
print('array6 shape:',array6.shape)
#불린인덱싱
array1d = np.arange(start=1, stop=10)
# [ ] 안에 array1d > 5 Boolean indexing을 적용 
array3 = array1d[array1d > 5]
print('array1d > 5 불린 인덱싱 결과 값 :', array3)


#정렬
org_array = np.array([ 3, 1, 9, 5]) 
print('원본 행렬:', org_array)

sort_array1 = np.sort(org_array)        #sorting된 데이터를 리턴 - 원본 데이터 그대로 냅둠   
print ('np.sort( ) 호출 후 반환된 정렬 행렬:', sort_array1) 
print('np.sort( ) 호출 후 원본 행렬:', org_array)

sort_array2 = org_array.sort()          #원본 데이터 자체를 수정 - 아무것도 리턴 안됨
print('org_array.sort( ) 호출 후 반환된 행렬:', sort_array2)
print('org_array.sort( ) 호출 후 원본 행렬:', org_array)

sort_array1_desc = np.sort(org_array)[::-1]     #:: 는 '거꾸로'
print ('내림차순으로 정렬:', sort_array1_desc) 

array2d = np.array([[8, 12], 
                   [7, 1 ]])
sort_array2d_axis0 = np.sort(array2d, axis=0)
print('로우 방향으로 정렬:\n', sort_array2d_axis0)      #8과 7을 비교, 12와 1을 비교
sort_array2d_axis1 = np.sort(array2d, axis=1)
print('컬럼 방향으로 정렬:\n', sort_array2d_axis1)      #8과 12를 비교, 7과 1을 비교

org_array = np.array([ 3, 1, 9, 5]) 
sort_indices = np.argsort(org_array)
print(type(sort_indices))
print('행렬 정렬 시 원본 행렬의 인덱스:', sort_indices)

name_array = np.array(['John', 'Mike', 'Sarah', 'Kate', 'Samuel'])
score_array= np.array([78, 95, 84, 98, 88])

sort_indices_asc = np.argsort(score_array)
print('성적 오름차순 정렬 시 score_array의 인덱스:', sort_indices_asc)
print('성적 오름차순으로 name_array의 이름 출력:', name_array[sort_indices_asc])

#행렬 내적(곱)
A = np.array([[1, 2, 3],
              [4, 5, 6]])
B = np.array([[7, 8],
              [9, 10],
              [11, 12]])

dot_product = np.dot(A, B)
print('행렬 내적 결과:\n', dot_product)



#DataFrame

col_name1=['col1']			# 1개의 컬럼명이 필요함
list1 = [1, 2, 3]
array1 = np.array(list1)
print('array1 shape:', array1.shape )
df_list1 = pd.DataFrame(list1, columns=col_name1)   	#컬럼네임을 col_name1로 지어줌
print('1차원 리스트로 만든 DataFrame:\n', df_list1)
df_array1 = pd.DataFrame(array1, columns=col_name1) 	#컬럼네임을 col_name1로 지어줌
print('1차원 ndarray로 만든 DataFrame:\n', df_array1)

col_name2=['col1', 'col2', 'col3']	# 3개의 컬럼명이 필요함
list2 = [[1, 2, 3],			# 2행x3열 형태의 리스트와 ndarray 생성 한 뒤 이를 DataFrame으로 변환. 
         [11, 12, 13]]
array2 = np.array(list2)
print('array2 shape:', array2.shape )
df_list2 = pd.DataFrame(list2, columns=col_name2)
print('2차원 리스트로 만든 DataFrame:\n', df_list2)
df_array2 = pd.DataFrame(array2, columns=col_name2)
print('2차원 ndarray로 만든 DataFrame:\n', df_array2)



# Key는 컬럼명으로 매핑, Value는 리스트 형(또는 ndarray)
dict = {'col1':[1, 11], 'col2':[2, 22], 'col3':[3, 33]}
df_dict = pd.DataFrame(dict)
print('딕셔너리로 만든 DataFrame:\n', df_dict)

# DataFrame을 리스트로 변환
print(df_dict.values)
list3 = df_dict.values.tolist()     #df_dict.values => ndarray로 바뀜. .tolist()로 최종적으로 리스트로 바뀜
                                    #바로 df_dict.tolist() 할 수 없다
print('df_dict.values.tolist() 타입:', type(list3))
print(list3)

# DataFrame을 딕셔너리로 변환
dict3 = df_dict.to_dict('list')
print('\n df_dict.to_dict() 타입:', type(dict3))
print(dict3)

##################### titanic 살짝 건드려보기 #############################

titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')

#1. 결측치(비어있는 값), 데이터타입(하나의 컬럼은 하나의 데이터 타입으로 이루어져야 함) 분석
print(titanic_df.info())
test=titanic_df.describe()
'''
count   non-null인 데이터 개수
mean    평균
std     표준편차
min     최솟값
25%     25% 위치의 값
50%     50% 위치의 값
75%     75% 위치의 값
max     최댓값
'''
value_counts = titanic_df['Age'].value_counts()  #Age에 대한 Series(1개의 컬럼)의 요소 별 그에 해당하는 인원

titanic_df['Age_0']=0    #새로운 컬럼 추가하고 0으로 모두 채우기
titanic_df.head(3)


#2. DataFrame과 리스트, 딕셔너리, 넘파이 ndarray의 상호 변환 - 사용하기 쉽게~

#3. DataFrame의 컬럼 데이터 세트 생성, 수정
titanic_df['Age_by_10'] = titanic_df['Age']*10    #컬럼의 연산의 결과는 각각의 개별 요소끼리의 연산 결과를 담은 컬럼. 하나의 Series.
titanic_df['Family_No'] = titanic_df['SibSp']+titanic_df['Parch']+1
titanic_df.head(3)
titanic_df['Age_by_10'] = titanic_df['Age']+100
titanic_df.head(3) #바뀜

#4. DataFrame 데이터 삭제 - axis=0인지 axis=1인지 잘 써줘야 함!!!
titanic_drop_df = titanic_df.drop('Age_0', axis=1 )     #y축에서 'Age_0' 컬럼 삭제
titanic_drop_df = titanic_df.drop(8, axis=0 )           #x축에서 8인덱스 로우 삭제 
titanic_drop_df.head(3)
drop_result = titanic_df.drop(['Age_0', 'Age_by_10', 'Family_No'], axis=1, inplace=False)   #inplace=False : 리턴하고 원본데이터는 그대로
print(' inplace=True 로 drop 후 반환된 값:',drop_result)
titanic_df.head(3) #그대로


#5. Index 객체 추출
indexes = titanic_df.index
print(indexes)

print('Index 객체 array값:\n',indexes.values) # .values를 통해 Index 객체를 실제 값 ndarray 데이터타입으로 변환 
indexes_value = indexes.values
'''
cf. 한번 만들어진 DataFrame 및 Series의 Index객체는 개별 row를 구분하는 "유니크한 값"(중복X)이기 때문에 수정 불가능
'''
print(type(indexes.values))
print(indexes.values.shape)
print(indexes[:5].values)
print(indexes.values[:5])
print(indexes[6])

series_fair = titanic_df['Fare']   #DataFrame의 컬럼 하나가 Series 타입으로 넘어감
print('Fair Series max 값:', series_fair.max())
print('Fair Series sum 값:', series_fair.sum())
print('sum() Fair Series:', sum(series_fair))
print('Fair Series + 3:\n',(series_fair + 3).head(3) )  #컬럼의 연산 : 각 개별 요소의 연산~~

titanic_reset_df = titanic_drop_df.reset_index(inplace=False)   #.reset_index()로 새 인덱스 생성, 기존의 인덱스는 새로운 컬럼으로 들어감                 
value_counts = titanic_df['Pclass'].value_counts()
print(value_counts)
print('value_counts 객체 변수 타입:',type(value_counts))
new_value_counts = value_counts.reset_index(inplace=False)      #titanic_df['Age'].value_counts()는 나이대로 정렬되는 게 아니라 ,
print(new_value_counts)                                         #유니크한 나이에 해당하는 요소의 개수(해당 나이의 인원은 몇명인가)에 따라 내림차순 정렬됨. 
print('new_value_counts 객체 변수 타입:',type(new_value_counts))


#6. 데이터 셀렉팅 및 슬라이싱 - 위치기반 iloc, 명칭기반 loc
data = {'Name': ['Chulmin', 'Eunkyung','Jinwoong','Soobeom'],
        'Year': [2011, 2016, 2015, 2015],
        'Gender': ['Male', 'Female', 'Male', 'Male']}
data_df = pd.DataFrame(data, index=['one','two','three','four'])
data_df

print(data_df.iloc[0, 0])           #iloc select
print(data_df.loc['one', 'Name'])   #loc select
print('위치기반 iloc slicing\n', data_df.iloc[0:1, 0])              #iloc slice
''' one    Chulmin'''  #one 은 index 출력된 것. 인덱스는 위치에 포함 안하니까~~!! 사실상 Chulmin만 출력됨.
print('명칭기반 loc slicing\n', data_df.loc['one':'two', 'Name'])   #loc slice
''' one     Chulmin
    two    Eunkyung''' #loc는 명칭기반이므로 특별하게 슬라이싱에서 [a:b]이면 b까지 포함!!

titanic_boolean = titanic_df[titanic_df['Age'] > 60] #boolean slice
print(type(titanic_boolean))
titanic_df[titanic_df['Age'] > 60][['Name','Age']].head(3) #'Name' 단일컬럼만 보려면 'Name'만 []에 넣어주면 되지만,
                                                            #'Name', 'Age'처럼 여러 컬럼 보려면 그걸 묶은 "리스트"를 넣어줘야 한다! ['Name', 'Age']리스트를 []에 넣어줌
print(type((titanic_df['Age'] > 60) & (titanic_df['Pclass']==1) & (titanic_df['Sex']=='female')))
print(titanic_df[ (titanic_df['Age'] > 60) & (titanic_df['Pclass']==1) & (titanic_df['Sex']=='female')])
'''cond1 = titanic_df['Age'] > 60
cond2 = titanic_df['Pclass']==1
cond3 = titanic_df['Sex']=='female'
titanic_df[ cond1 & cond2 & cond3]    얘를 줄여 쓴 것'''


#7. 정렬, Aggregation(집합) 함수-agg()♣, GroupBy 적용-groupby()♣
titanic_sorted = titanic_df.sort_values(by=['Name'])    #'Name' 컬럼을 기준으로 정렬해주세요~
titanic_sorted.head(3)
titanic_sorted = titanic_df.sort_values(by=['Pclass', 'Name'], ascending=[False, True]) #처음으로 들어온 'Pclass'로 정렬한 뒤, 'Pclass'는 건들지 않고, 같은 'Pclass' 안에서 'Name'으로 정리
titanic_sorted.head(3)                                                                  #'Pclass'는 내림차순, 'Name'은 오름차순으로 정렬

'''앞으로 aggregation함수를 쓸 때 무조건 agg('agg함수명')로 묶어서 ~!!'''
print(titanic_df.agg('count')) #각 컬럼에서 null값이 아닌(데이터가 애초에 없으니까 자동 제외) value만 count
print(titanic_df[['Age', 'Fare']].agg('mean')) #'Age', 'Fare'에서 null값이 아닌(데이터가 애초에 없으니까 자동 제외) value들의 평균

titanic_groupby = titanic_df.groupby('Pclass')
print(type(titanic_groupby))
print(titanic_groupby) #그냥 객체가 나와버렸다... 얘를 Aggregation함수와 함게 써야 의미가 있다!
titanic_groupby = titanic_df.groupby('Pclass').agg('count') #'Pclass'의 유니크한 값 1, 2, 3을 이용해 각 칼럼별로 null값이 아닌 value만 count
print(type(titanic_groupby))
print(titanic_groupby)  #Cabin이 처음에 titanic_df.info()로 봤을 때 null값이 엄청 많았는데, titanic_groupby로 확인하니까 Pclass가 1인 경우에는 그렇게 많지 않았다..!
                        #그와중에 Cabin이 2, 3인 경우는 많았다. 즉, 1등급이 아닌 2, 3등급 선실에서 묵은 사람들의 명부는 엄청 많이 누락되어 있구나~!
titanic_groupby = titanic_df.groupby('Pclass')[['PassengerId', 'Survived']].agg('count') #'Pclass'의 유니크한 값 1, 2, 3으로 카테고리 만들고, 12개의 컬럼 중에서 요 두개만 뽑아서 나타낼게요~

titanic_df.groupby('Pclass')['Age'].agg([max, min]) #'Pclass'의 유니크한 값 1, 2, 3을 카테고리로 하는 'Age'컬럼만을 보는데, 각각의 'Age'의 max, min 구한다                                                  
agg_format={'Age':'max', 'SibSp':'sum', 'Fare':'mean'} #딕셔너리 - key:컬럼명, value:적용시킬 aggregation 함수
titanic_df.groupby('Pclass').agg(agg_format) 


#8. 결손 데이터 처리 - 1. 해당 row 날려버리기 2. 해당 column 날려버리기 3. 평균값으로 채우기 4. 0으로 채우기
    #.isna() : 결손 데이터 확인
print(titanic_df.isna()) #titanic_df의 전체 데이터가 각각 null인지 모두 확인
print(titanic_df.isna().sum())  #titanic_df.isna()의 결과인 DataFrame에서 각 컬럼의 sum값을 출력. 
                                #titanic_df.isna()의 value는 모두 boolean값(null이면 True==1, 아니면 False==0)이므로 True값, 즉 null의 개수를 세준다. 
    #.fillna(A) : A로 결손 데이터 대체
titanic_df['Cabin'] = titanic_df['Cabin'].fillna('C000') #titanic_df['Cabin']에서 null을 'C000'로 대체
print(titanic_df['Cabin'].isna().sum())
titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].mean()) #titanic_df['Age']에서 null을 평균으로 대체
titanic_df['Embarked'] = titanic_df['Embarked'].fillna('S') #titanic_df['Embarked']에서 null을 'S'으로 대체
print(titanic_df.isna().sum())


#9. A.apply(lambda x:...) 식으로 데이터 가공    → A는 iterate, A의 요소가 x에 들어감.
lambda_square = lambda x : x ** 2    # a**b == a^b
print('3의 제곱은:',lambda_square(3))
titanic_df['Name_len']= titanic_df['Name'].apply(lambda x : len(x)) #'Name_len' 컬럼 추가해서 'Name'컬럼의 단일요소(str)의 길이를 넣어줌
titanic_df['Child_Adult'] = titanic_df['Age'].apply(lambda x : 'Child' if x <=15 else 'Adult') #'Child_Adult' 컬럼 추가해서 'Age'컬럼의 단일요소들이 15 이하면 'Child'를, 아니면 'Adult'를 단일요소로 넣어줌
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : 'Child' if x<=15 else ('Adult' if x <= 60 else 'Elderly')) #'Age_cat' 컬럼 추가해서 'Child_Adult' 컬럼보다 더 세분화해서 'Elderly'까지 단일요소로 넣어줌
titanic_df['Age_cat'].value_counts()                                                                                  #lambda x: A if a else(B if b else C) 형태 : a면 A, a가 아닌데 b면 B, 그것도 아니면 C                                                                                                                      
def get_category(age):  # 나이에 따라 세분화된 분류를 수행하는 함수 생성
    cat = ''
    if age <= 5: cat = 'Baby'
    elif age <= 12: cat = 'Child'
    elif age <= 18: cat = 'Teenager'
    elif age <= 25: cat = 'Student'
    elif age <= 35: cat = 'Young Adult'
    elif age <= 60: cat = 'Adult'
    else : cat = 'Elderly'
    return cat
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x)) #get_category(X)는 입력값으로 ‘Age’ 컬럼 값을 받아서 해당하는 cat 반환
titanic_df['Age_cat'].value_counts()                                        #lambda에서 if else문 쓰는 것보다 이게 더 나음

■■■■■■■■■■■■■0426 수업中 새로 알게된것■■■■■■■■■■■■■

'''
문제 1. 

1. 타이타닉 데이터를 데이터 프레임 형태로 업로드
2. null값 얼마나 있는지 확인
3. column별 데이터 분포 확인
4. Pclass의 value_count 확인
5. Age의 null값 채우되 Sibsp가 같은 나이의 평균으로 채운다 ★
    -Sibsp : 0~7까지 있다면, 동일한 Sibsp를 가진 아이들끼리의 묶음의 Age 평균값으로 채우는 것!

cf. 
.apply(lambda x:func(x)) DataFrame에서 람다함수 쓸 때 그냥 이 포맷으로 써라!
if문이 단 하나라도 들어간다거나 하면 func(x)써주는 게 나음~~
	컬럼.apply(lambda x:func(x)) => x:컬럼 안의 요소 하나하나
	x['Age']의 타입은 알 수 없다 (단일요소의 타입:int일 수도, float일 수도, Object(==str)일 수도...)
	DF.apply(lambda x:func(x), axis=1) => x:DataFrame 안의 로우 하나하나


'''
import numpy as np
import pandas as pd

titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
print(titanic_df.info())
print(titanic_df.isna().sum())
print(titanic_df.describe())
print(titanic_df['Pclass'].value_counts())
def get_category(x):
    if np.isnan(x['Age']): 	#isna()는 pandas, 즉 dataframe, series 둘에서만 사용 가능. 여기서 each_row['Age']는 float기 때문에 사용 못함
        if x['SibSp']==0:
            return titanic_df[titanic_df['SibSp']==0]['Age'].mean()
        elif x['SibSp']==1:
            return titanic_df[titanic_df['SibSp']==1]['Age'].mean()
        elif x['SibSp']==2:
            return titanic_df[titanic_df['SibSp']==2]['Age'].mean()
        elif x['SibSp']==3:
            return titanic_df[titanic_df['SibSp']==3]['Age'].mean()
        elif x['SibSp']==4:
            return titanic_df[titanic_df['SibSp']==4]['Age'].mean()
        elif x['SibSp']==5:
            return titanic_df[titanic_df['SibSp']==5]['Age'].mean()
        elif x['SibSp']==8:
            return 8
    else:
        return x['Age']

titanic_df['Age'] = titanic_df.apply(lambda x : get_category(x), axis=1) 
                #axis=1 => titanic_df의 row 한 줄을 전부 get_category()함수로 던져준다
                #axis=0(default)라면 => 컬럼 하나가 여기로 넘어감

'''
문제 1_다른방법. 
import pandas as pd
import numpy as np

titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
print('titanic 변수 type:',type(titanic_df))
titanic_df_my = titanic_df

print(titanic_df_my.info())
print(titanic_df_my.describe())

value_counts = titanic_df_my['Pclass'].value_counts()
print(value_counts)

value_counts = titanic_df_my['SibSp'].value_counts()
print(value_counts)

def get_category(row):
    cat = ''
    if np.isnan(row['Age']):
        if row['SibSp'] == 0:
            #cat = 0
            cat = titanic_df_my[titanic_df_my['SibSp'] == 0]['Age'].mean()
        elif row['SibSp'] == 1:
            cat = titanic_df_my[titanic_df_my['SibSp'] == 1]['Age'].mean()
        elif row['SibSp'] == 2:
            cat = titanic_df_my[titanic_df_my['SibSp'] == 2]['Age'].mean()
        elif row['SibSp'] == 3:
            cat = titanic_df_my[titanic_df_my['SibSp'] == 3]['Age'].mean()
        elif row['SibSp'] == 4:
            cat = titanic_df_my[titanic_df_my['SibSp'] == 4]['Age'].mean()
        elif row['SibSp'] == 5:
            cat = titanic_df_my[titanic_df_my['SibSp'] == 5]['Age'].mean()
        elif row['SibSp'] == 8:
            cat = 8   
    else:
        cat = row['Age']

    return int(cat)

titanic_df_my['Age'] = titanic_df_my.apply(lambda x : get_category(x), axis=1)
print(titanic_df_my['Age'].notnull().any())     #'Age'컬럼이 전부 null이 아니어야 True
        #isna(), isnull()   : null(O) - True, null(X) - False
        #notnull()          : null(O) - False, null(X) - True
        #all() : 모두 참이어야 참, 하나라도 거짓이면 거짓
        #any() : 하나라도 참이면 참, 모두 거짓이어야 거짓
'''

'''
문제 2. 
Pcall를 groupby하여서 Age_max, Age_min, Age_mean, Parch_max, Parch_min, Sibsp_max, Sibsp_min을 구하여라
'''

import pandas as pd
import numpy as np

titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
titanic_df_my = titanic_df

agg_format={'Age':['max', 'min', 'mean'], 'Parch':['max','min'], 'SibSp':['max','min']}
titanic_df_my = titanic_df_my.groupby(by='Pclass').agg(agg_format)

■■■■■■■■■■■■■0427 수업中 새로 알게된것■■■■■■■■■■■■■

지도학습 vs. 비지도학습
레이블 == 정답값 == 결정값 == 타겟값
지도학습은 레이블이 있는 datasets을 가지고 머신러닝 돌리는 것, 비지도학습은 레이블이 없는 datasets 가지고 머신러닝 돌리는 것

sklearn.datasets 내의 모듈 : 사이킷런에서 자체적으로 제공하는 연습 데이터 세트를 생성하는 모듈 모임
sklearn.tree 내의 모듈 : 트리 기반 ML 알고리즘을 구현한 클래스 모임	    cf. 트리 기반의 알고리즘, 경사 기반의 알고리즘이 있다
sklearn.model_selection 내의 모듈 : 학습데이터/검증데이터/예측데이터로 데이터 분리(train_test_split())하거나, 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈 모임

train과 test로 분리하는 이유
데이터를 만들고 나면 인공지능한테 학습하라고 던져줄 것. 보통 트리 외의 기반의 알고리즘은 '가중치 학습'(cf. 트리기반 알고리즘은 '트리 학습')이 많다. 
우리는 기계학습 중, 가중치 학습을 위한 데이터로 train데이터만 사용. test데이터는 오직 테스트를 위해서만 사용. 
얼마나 정확한 모델인지 검증할 때 test데이터까지 학습한 뒤, 그 안에서 test데이터 뽑아서 검증한다면? 정답률 100% 나옴 ㅋㅋ

layer와 가중치란?
트리가 아닌 회귀 기반의 알고리즘을 학습할 때 '가중치 학습'을 하는데, 가중치에는 여러 층(layer)이 있다. 
	X→Y 다음 층(layer)으로 넘어갈 때에는 각 개별 요소 x1, x2, x3, x4, ...에 가중치 w1_1, w1_2, w1_3, w2_1, ...를 곱한 값을 다 더한 값이 넘어감. 
	y1 = x1w1_1+x2w2_1+x3w3_1, y2 = x1w2_1+x2w2_2+x3w2_3, .....
	Y→Z는 더 늘어나겠지~~
딥 러닝 : 층이 늘어날 수록 가중치가 기하급수적으로 늘어남. 시간이 오래 걸림(회귀 기법)
회귀 기법 : 오차가 최소가 되는 그래프와 bias를 찾는 것. 

교차 검증 --------- 분할정복이구나~~~~~~~~^0^
train_test_split()해도 test데이터에 과적합(Overfitting모델이 학습 데이터에만 과도하게 최적화)되는 문제가 있다. 
계속 train, predict해서 90%이상 정확도가 나오게 되더라도 사실상 그건 test데이터에 Overfitting되어있을 가능성이 있으므로.
회귀 기반의 overfitting(과적합) : 사실상 1차함수에 가까운 분포였는데, 만약 좌표평면 위의 점들이 딱 들어맞는 4차함수 그래프를 찾아내서 이것을 토대로 예측한다면? 정확도 떨어짐. overfitting. 
	K폴드 교차 검증, Stratified K폴드 교차 검증, cross_val_score
트리 기반의 overfitting(과적합) : 너~~~무 깊이 트리가 만들어질 때 → 예방하려면 파라미터 설정값의 범위를 정해줘야 한다. 안 정해주면 끝까지 파고들어감. 그러면 대부분 overfitting.
	GridSearchCV

K폴드 교차 검증♣
K=5	[               train               ] [   test   ]
	-------------------------------------------
	[_test_][train][train][train][train] [   test   ]
	[train][_test_][train][train][train] [   test   ]
	[train][train][_test_][train][train] [   test   ]
	[train][train][train][_test_][train] [   test   ]
	[train][train][train][train][_test_] [   test   ]
	-------------------------------------------
	[               평균               ] [___test___]
	> 다섯 번의 교차검증의 평균으로 최종 test 검증 (처음부터 끝까지 test인 그 test)

Stratified K폴드 교차 검증♣
특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우칠 때 사용. 
원본 데이터의 유니크한 레이블 값의 분포가 예를 들어 0:1:2 = 10%:30%:60%라고 한다면, 처음 나눈 test데이터를 제외한 전체 train데이터를 K폴드 할 때 그 비율을 맞춰서 K폴드 해줘야 함
또한 예를 들어 0:1:2:3 = 25%:25%:25%:25%인데 섞이지 않고 0,0,0,,,,,0,1,1,1,.,,,,2,2,2,,,,,2,3,,3,,,,3, 이런 경우도 포함
그러면 최종 test데이터가 뭐가 오든 편중된 학습을 하지 않았으니까 좋을 듯~

cross_val_score(estimator, X, y=None, scoring=None, cv=None)♣
Stratified K폴드 교차 검증을 보다 간편하게 해줌
estimator : 사이킷런의 분류 알고리즘 클래스 Classifier의 객체 or 회귀 알고리즘 클래스 Regressor의 객체
	ex_	dt_clf = DecisionTreeClassifier(random_state=156)에서 dt_clf
X : 피처 데이터 세트
y : 레이블 데이터 세트
scoring : 자신이 원하는 판단 기준
	지금까지는 accuracy_score(a, b)로 비교하여 예측 정확도를 판단했지만, 판단 기준에는 accuracy_score만 있는 게 아니다. 
	ex_ 	신용카드를 긁었을 때 1:사기당해서 긁힌 경우, 0:내가 긁은 경우
		사기당했는지 판단하는 프로그램이 멍청해서 무조건 0만 출력한다고 가정. 근데 보통은 사기 아닌 경우(0인 경우)가 99.9999% 
		accuracy_score → 음! 아주 좋은 프로그램이군!
	이럴 땐 다른 판단 기준을 사용해야 한다. 
cv : K값. 몇 개로 split할 것인가. 
n_jobs : 사용 가능한 cpu의 개수

GridSearchCV♣♣♣
cf. 하이퍼 파라미너		-교차검증 수행하며 estimator(객체) 안에서 자동으로 설정되는 변수
			-하이퍼 파라미터 튜닝하면 알고리즘 예측 성능 개선할 수 있다. 
결정 트리 알고리즘을 구현한 DecisionTreeClassifier의 하이퍼파라미터를 max_depth=[1,2,3], min_samples_split=[1,2]로 내가 직접 설정한다. 
조합 (1, 2), (1, 3), (2, 2), (2, 3), (3, 2), (3, 3) 6가지가 나온다. 각 설정에 의한 교차 검증(cv=3이라 가정) 하면 각 조합마다 3회에 걸쳐 학습/평가하고 성능을 평균 내서 정확도 최고값 구한다.
이렇게 3x2x3 = 6x3 = 18번의 학습(fit)/검증으로 과적합 없이 최적의 결과 내는 파라미터 찾을 수 있다. 

=================================================================

from sklearn.datasets import load_iris #붓꽃 데이터 세트 예제를 sklearn.datasets에서 아예 제공해줌 
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score #정확도 구하는 함수 제공해줌

import pandas as pd

'''
꽃의 길이와 폭, 꽃받침의 길이와 폭 데이터를 가지고 지도학습을 한다. 이것으로 총 3개의 붓꽃 품종 중에서 어떤 종인지 맞춰본다. 
'''
# 붓꽃 데이터 세트를 로딩합니다. iris는 객체가 됨
iris = load_iris()

# iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터를 numpy로 가지고 있습니다. 
iris_data = iris.data #feature값 : 4개의 컬럼네임 Sepal length, Sepal width, Petal length, Petal width

# iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 가지고 있습니다. 
iris_label = iris.target #target값 : iris_data라는 feature값을 통해 알아낸 품종은 3개 중 무엇인가 (정답값이 있으므로 지도학습!!)
print('iris target값:', iris_label)
print('iris target명:', iris.target_names)

# 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다. 
#test=iris_data #얘는 ndarray
#test_=iris.feature_names #얘는 리스트. 리스트든, 딕셔너리든 DataFrame으로 감싸면 DataFrame됨
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) #data:value값에 해당하는 파라미터, columns:컬럼명에 해당하는 파라미터
iris_df['label'] = iris.target
iris_df.head(3)

# 1. 학습/테스트 데이터 세트 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)
'''
X_train     : train 데이터의 feature데이터
y_train     : train 데이터의 label데이터(뒤에 붙은 정답값)
X_test      : test 데이터의 feature데이터
y_test      : test 데이터의 label데이터(뒤에 붙은 정답값)

test_size       - 총 데이터 150개, 그 중 0.2만큼 test로 사용 => 30개 test, 120개 train
random_state    - test데이터도 랜덤하게 뽑아야 한다. 그 때 쓰이는 시드값 : random_state. 
                즉, random_state 설정해주면 매번 뽑히는 X_train, X_test, y_train, y_test값이 동일할 것!!
cf. 랜덤값도 사실 규칙(시드값)이 있다. 
    import random
    random.seed(100)    #100이라는 규칙을 가지고 랜덤 실행하세요~
    A = random.random() #A값 계속 동일. random.seed(100) 주석처리하면 A값 계속 달라짐.

'''

# 2. DecisionTreeClassifier 객체 생성     //DecisionTreeClassifier : 트리 기반 알고리즘(시각화가능), dt_clf는 그 알고리즘의 객체이고 생성자로 시드값 넣어줌
dt_clf = DecisionTreeClassifier(random_state=11)

# 3. 학습 수행 
dt_clf.fit(X_train, y_train)    #이제 dt_clf는 학습 완료된 모델

# 4. 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행. 
pred = dt_clf.predict(X_test)   #X_test : test데이터의 feature값. y_test에 이미 정답값(실제 레이블값)이 들어 있다. 

# 5. 예측 정확도 확인
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred))) #비교할 두 개 넣어주면 됨. 예측된 레이블값 pred와 비교해서 정확도 (맞은개수/전체개수) 비교 

■■■■■■■■■■■■■0428 수업中 새로 알게된것■■■■■■■■■■■■■

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:',features.shape[0])

n_iter = 0
train_index_chk = []
test_index_chk = []

# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  
for train_index, test_index  in kfold.split(features):  #train_index, test_index (kfold.split(features)로 리턴)로 학습용/검증용 데이터 추출해야 하는구나~
                                                        #원래는 kfold.split(A)에서 A 안에 train_test_split()한 뒤 train데이터만 넣어야 하는데, 그냥 전체 데이터 features를 train데이터라 생각하고 넣어보았다~
    train_index_chk.append(train_index) #variable explorer 보면 ndarray 하나씩 5번 들어감. for문이 5번 돌았구나~
    test_index_chk.append(test_index) #variable explorer 보면 ndarray 하나씩 5번 들어감. for문이 5번 돌았구나~
    
    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1
    
    # 반복 시 마다 정확도 측정 
    accuracy = np.round(accuracy_score(y_test,pred), 4)     #np.round(a, b) : a를 소수점 4째자리까지 반올림
    train_size = X_train.shape[0]   #120    ← X_train.shape : (120, 4)
    test_size = X_test.shape[0]     #30     ← X_test.shape  : (30, 4)
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)    #리스트로 담음
    
# 개별 iteration별 정확도를 합하여 평균 정확도 계산 
print('\n## 평균 검증 정확도:', np.mean(cv_accuracy)) 

===============================================================

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

import numpy as np
import pandas as pd

iris = load_iris()
dt_clf = DecisionTreeClassifier()
train_data = iris.data
train_label = iris.target
dt_clf.fit(train_data, train_label)

# 학습 데이터 셋으로 예측 수행
pred = dt_clf.predict(train_data)
print('예측 정확도:',accuracy_score(train_label,pred))



dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, 
                                                    test_size=0.3, random_state=121)

dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))




iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:',features.shape[0])




iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label']=iris.target
print(iris_df['label'].value_counts())

kfold = KFold(n_splits=3)
# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. 
n_iter =0
for train_index, test_index  in kfold.split(iris_df):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())

'''
[그냥 K폴드 교차 검증 결과]
학습(train) 데이터가 0 0 0 0 ... 1 1 1 1 ... 2 2 2 2 ... 이렇게 있는데,
                    ---50개---  ---50개---  ---50개---  
얘를 50개씩 끊어서 앞의 50개 test, 뒤의 100개 train 이런 식으로 교차 검증 해버리면 문제 있음

## 교차 검증: 1
학습 레이블 데이터 분포:
 2    50
1    50
Name: label, dtype: int64
검증 레이블 데이터 분포:
 0    50
Name: label, dtype: int64
'''


from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
n_iter=0

for train_index, test_index in skf.split(iris_df, iris_df['label']): #레이블값을 같이 넣어줌!!!★ 이거 하나 다름
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())

'''
[Stratified K폴드 교차 검증 결과]
아까와는 다르게 분포 비율이 유지가 되었다^-^

## 교차 검증: 1
학습 레이블 데이터 분포:
 2    34
1    33
0    33
Name: label, dtype: int64
검증 레이블 데이터 분포:
 1    17
0    17
2    16
Name: label, dtype: int64
'''




#정확도 계산 - 그냥
dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  
for train_index, test_index  in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정 
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)
    
# 교차 검증별 정확도 및 평균 정확도 계산 
print('\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))
print('## 평균 검증 정확도:', np.mean(cv_accuracy)) 


#정확도 계산 - cross_val_score 사용
iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개 
scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)
print('교차 검증별 정확도:',np.round(scores, 4))
print('평균 검증 정확도:', np.round(np.mean(scores), 4))

==================================================================

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

import pandas as pd


# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리
iris = load_iris()
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121)
dtree = DecisionTreeClassifier()

### parameter 들을 dictionary 형태로 설정
parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}


# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  
### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 교차검증 학습/평가 .
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과 추출하여 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]

print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))

# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
estimator = grid_dtree.best_estimator_

# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))

■■■■■■■■■■■■■0429 수업中 새로 알게된것■■■■■■■■■■■■■

데이터 전처리
-NaN Null값은 허용되지 않는다
-사이킷런의 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않는다

데이터 인코딩 - 1. 레이블 인코딩
    -카테고리 피처를 숫자값으로 변환. 
    -LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행한다. 
    -숫자값은 크고 작음의 특성도, 가중치로도 쓰이지 않고 인덱스처럼 오직 데이터 로우를 식별하는 용도로 사용되어야 한다. 
    	트리 기반의 ML 알고리즘 - 사용 가능
    	회귀 기반의 ML 알고리즘 - 사용 불가능(단점). 회귀 기반의 가중치 학습을 할 때 피처에 들어있는 값에 이 숫자값이 들어가게 되면 → 아무 의미 없게 쓰여야 하는 숫자값에 의미가 생겨버린다!!
데이터 인코딩 - 2. 원-핫 인코딩	//레이블 인코딩이 회귀 기반의 ML 알고리즘에서 쓰이지 못하는 문제 해결
    -새로운 피처를 추가해 그에 해당하는 컬럼에만 1, 나머지에는 0을 표시.
    -단일 컬럼일 때보다 컬럼의 개수가 늘어나니까 시간이 조금 더 걸린다. 
    -OneHotEncoder로 변환하기 전 모든 문자열 값이 LabelEncoder로 숫자값으로 변환되어야 하며, 입력값이 1차원이라면 reshape(-1,1)을 통해 2차원으로 변환해야 한다.

피처 스케일링 - 일정한 기준으로 비교하기 위함 (연봉 10,000,000, 형제 1의 값을 가진다면 회귀 기법 알고리즘에서 가중치 학습할 때 연봉의 영향력이 너무 커짐. 스케일링 해준 값으로 바꿔야 함!!!)
	1. 표준화
	2. 정규화
벡터 정규화 - 선형대수 개념의 정규화

모평균(m)
모분산(σ^2) = (X-m)²+(Y-m)²+(Z-m)² / 3 = ∑(편차)² / n
모표준편차(σ) = √모분산

표본평균(¯X)
표본분산(s²) = ∑(편차)² / n-1   
표본표준편차(s) = √표본분산	

cf. 표본분산은 n-1로 나눠줘야 하는 이유
우리가 구해야 하는 것은 모표준편차와 최대한 유사하게 구해야 함. 
근데 표본을 뽑아서 평균을 구하면 모집단의 평균과 살짝 다를 것이다. 그래서 표본분산 식에 모집단을 넣어 계산하면 분산이 크게 나온다. (편차가 조금만 달라져도 분산 커짐) 
그 차이를 맞추기 위해 n-1 해줌. 

표준화 = (X-m) / 표준편차		→ 평균이 0이고 분산이 1인 정규 분포를 가진 값
정규화 = (X-min) / (max-min) 	→ 무조건 0과 1 사이의 값 (최솟값 0, 최댓값 1)
벡터 정규화 = X / √(X^2+Y^2+Z^2)	→ 무조건 0과 1 사이의 값 (최솟값 0, 최댓값 1)

StandardScaler : 표준화 지원. 개별 피처를 평균 0 분산 1인 값으로 변환. 
MinMaxScaler : 정규화 지원. 데이터값을 0과 1 사이(음수값 있다면 -1과 1 사이)의 범위 값으로 변환. → 최솟값, 최댓값 무조건 0, 1 (or -1, 1)
	train [1 3 7 11]→[0. 0.2 0.6 1.] 로 한번 스케일링하면 test [2 4 10 12]→[0. 0.2 0.8 1.](x) 새롭게(x)
						  		  [0.1 0.3 0.9 1.](o) 그 스케일링 그대로(o)  

==========================================================

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import numpy as np

items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']


'''1. 레이블 인코딩'''
# LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. 
encoder = LabelEncoder()
encoder.fit(items) #각각의 유니크한 값에다가 숫자값 부여
labels = encoder.transform(items) #인코딩 변환 
print('인코딩 변환값:',labels)
print('인코딩 클래스:',encoder.classes_) #번호 부여된 유니크한 값들을 0번 값부터 차례대로 출력
print('디코딩 원본 값:',encoder.([4, 5, 2, 0, 1, 1, 3, 3])) #번호에 해당하는 유니크한 값들을 역으로 출력


'''2. 원-핫 인코딩'''
# 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환합니다. 
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
# 2차원 데이터로 변환합니다. 
labels = labels.reshape(-1,1)

# 원-핫 인코딩을 적용합니다. 
oh_encoder = OneHotEncoder()
oh_encoder.fit(labels)
oh_labels = oh_encoder.transform(labels)
print('원-핫 인코딩 데이터')
print(oh_labels.toarray())
print('원-핫 인코딩 데이터 차원')
print(oh_labels.shape)

==========================================================

from sklearn.datasets import load_iris
import pandas as pd
# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환합니다. 
iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print('feature 들의 평균 값')
print(iris_df.mean())
print('feature 들의 분산 값')
print(iris_df.var())



from sklearn.preprocessing import StandardScaler

# StandardScaler객체 생성
scaler = StandardScaler()
# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df) #평균 0 분산 1

#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('\nfeature 들의 평균 값')
print(iris_df_scaled.mean())
print('feature 들의 분산 값')
print(iris_df_scaled.var())
'''
feature 들의 평균 값
sepal length (cm)   -1.690315e-15       → 얘네가 0임 ^^    1.69 x 1/10^15는 거의 0에 가까움~
sepal width (cm)    -1.842970e-15
petal length (cm)   -1.698641e-15
petal width (cm)    -1.409243e-15
dtype: float64
feature 들의 분산 값
sepal length (cm)    1.006711           → 얘네가 1임 ^^    1.006711은 거의 1에 가까움~
sepal width (cm)     1.006711
petal length (cm)    1.006711
petal width (cm)     1.006711
dtype: float64
'''



from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler객체 생성
scaler = MinMaxScaler()
# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
print('\nfeature들의 최소 값')
print(iris_df_scaled.min())
print('feature들의 최대 값')
print(iris_df_scaled.max())




from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 학습 데이터는 0 부터 10까지, 테스트 데이터는 0 부터 5까지 값을 가지는 데이터 세트로 생성
# Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경
train_array = np.arange(0, 11).reshape(-1, 1)
test_array =  np.arange(0, 6).reshape(-1, 1)



# 최소값 0, 최대값 1로 변환하는 MinMaxScaler객체 생성
scaler = MinMaxScaler()
# fit()하게 되면 train_array 데이터의 최소값이 0, 최대값이 10으로 설정.  
scaler.fit(train_array)
# 1/10 scale로 train_array 데이터 변환함. 원본 10-> 1로 변환됨.
train_scaled = scaler.transform(train_array)
print('\n원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

# (이렇게 하면 안 됨!) 앞에서 생성한 MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최소값이 0, 최대값이 5으로 설정됨 
scaler.fit(test_array)
# 1/5 scale로 test_array 데이터 변환함. 원본 5->1로 변환.  
test_scaled = scaler.transform(test_array)
# train_array 변환 출력
print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))



# 재시도(이렇게 해야 됨!) train_array로 fit() → 헷갈리니까 가능하다면 전처리 전에 스케일부터 해놓고 해라!
'''
경우 1. 가능하면 전처리 train_test_split() 전에 전체 데이터를 스케일부터 해놓고 시작하기 (나중에 스케일 생각 안 해도 됨)
경우 2. 불가능하면 (나중에 train값 수정해야 한다거나..) train_array로 fit(), 이후에 이미 fit된 scaler 객체 이용해서 transform() 만으로 변환해야 함.
'''
scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)
print('\n원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform() 만으로 변환해야 함. 
test_scaled = scaler.transform(test_array)
print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))

■■■■■■■■■■■■■0430 수업中 새로 알게된것■■■■■■■■■■■■■

##################### titanic 살짝 건드려보기 2 #############################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline


titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')


print(titanic_df.info())
test=titanic_df.describe()
'''
count   non-null인 데이터 개수
mean    평균
std     표준편차
min     최솟값
25%     25% 위치의 값
50%     50% 위치의 값
75%     75% 위치의 값
max     최댓값
'''
titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True)     #Age null값은 Age 평균으로 채우기
titanic_df['Cabin'].fillna('N',inplace=True)                        #Cabin null값은 N으로 채우기
titanic_df['Embarked'].fillna('N',inplace=True)                     #Embarked null값은 N으로 채우기
print('데이터 세트 Null 값 갯수 ',titanic_df.isnull().sum().sum())
                #titanic_df.isnull() : boolean으로 채워진 DataFrame
                #titanic_df.isnull().sum() : 각 컬럼에 해당하는 True값의 갯수를 더해준 Series
                #titanic_df.isnull().sum().sum() : Series 각각의 요소(int)를 더해준 int64(걍 int라고 생각)


print(' Sex 값 분포 :\n',titanic_df['Sex'].value_counts())         #Sex 컬럼에서 유니크한 값 별로 몇개가 있는지
print('\n Cabin 값 분포 :\n',titanic_df['Cabin'].value_counts())   #Cabin ~
print('\n Embarked 값 분포 :\n',titanic_df['Embarked'].value_counts()) #Embarked ~
'''
Cabin 출력값에서 이상한 점을 발견했다!
1. N만 너무 크다
2. C23, C25, C27처럼 여러 Cabin이 한꺼번에 출력됐다
3. 첫번째 알파벳이 선실 등급을 나타내니까 중요한 듯하다
'''

titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]   #우선 Cabin은 첫번째 인덱스만 남겨서 재구성하자!
print(titanic_df['Cabin'].head(3))                  #어쨌든 str값이니까 나중에 인코딩하겠네~~

titanic_df.groupby(['Sex','Survived'])['Survived'].count()  #Sex, Survived의 유니크한 값 기준을로 묶어서 DataFrame 만들고, 그 안에서 non-null인 count 개수 보여줌
titanic_df.groupby(['Pclass','Sex','Survived'])['Survived'].count()  #Sex, Survived의 유니크한 값 기준을로 묶어서 DataFrame 만들고, 그 안에서 non-null인 count 개수 보여줌
    # serial data : histogram, 나머지 : barplot
sns.barplot(x='Sex', y = 'Survived', data=titanic_df)   #Sex의 유니크한 값 별로 Survived의 총 count중에서 1이 차지하는 비율. 즉, titanic_df.groupby(['Sex','Survived'])['Survived'].count()의 비율
sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df) #얘도 위의 titanic_df.groupby(['Pclass','Sex','Survived'])['Survived'].count()의 비율


# 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame의 apply lambda식에 사용. 
def get_category(age):
    cat = ''
    if age <= -1: cat = 'Unknown'
    elif age <= 5: cat = 'Baby'
    elif age <= 12: cat = 'Child'
    elif age <= 18: cat = 'Teenager'
    elif age <= 25: cat = 'Student'
    elif age <= 35: cat = 'Young Adult'
    elif age <= 60: cat = 'Adult'
    else : cat = 'Elderly'
    return cat

# 막대그래프의 크기 figure를 더 크게 설정 
plt.figure(figsize=(10,6))

#X축의 값을 순차적으로 표시하기 위한 설정 - 얘네는 value값이 str이라서 이렇게 안 해주면 이 순서가 아니라 titanic_df['Age_cat']에 처음 저장된 순서대로 출력
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']

# lambda 식에 위에서 생성한 get_category( ) 함수를 반환값으로 지정. 
# get_category(X)는 입력값으로 'Age' 컬럼값을 받아서 해당하는 cat 반환
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
sns.barplot(x='Age_cat', y = 'Survived', hue='Sex', data=titanic_df, order=group_names)
titanic_df.drop('Age_cat', axis=1, inplace=True)



'''
지금까지 분석한 결과, 중요하게 생존을 좌우하는 피처 : Sex, Age, Pclass
얘네를 인코딩해보자! (문자열 -> 숫자형)
'''
from sklearn import preprocessing

def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = preprocessing.LabelEncoder()
        le = le.fit(dataDF[feature]) #인코딩 모델 만들어짐. 
        dataDF[feature] = le.transform(dataDF[feature]) #인코딩해서 다시 원본데이터에 넣음
    return dataDF

titanic_df = encode_features(titanic_df)
titanic_df.head()

=============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn.preprocessing import LabelEncoder

'''
앞서 계속 해왔던 데이터 전처리 과정을 함수로 깔끔하게 만들어본다면?
'''

# Null 처리 함수
def fillna(df):
    df['Age'].fillna(df['Age'].mean(),inplace=True)
    df['Cabin'].fillna('N',inplace=True)
    df['Embarked'].fillna('N',inplace=True)
    df['Fare'].fillna(0,inplace=True)
    return df

# 머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)
    return df

# 레이블 인코딩 수행. 
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin','Sex','Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 앞에서 설정한 Data Preprocessing 함수 호출
def transform_features(df): #위에 있는 함수들을 수행하기 위한 함수. 그냥 얘 수행하면 전체 함수 수행. (모든 전처리 완료)
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df



# 원본 데이터를 재로딩 하고, feature데이터 셋과 Label 데이터 셋 추출. 
titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived',axis=1)

X_titanic_df = transform_features(X_titanic_df) #한번에 전처리 해버렸다!

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11) # 이제 estimator에 넣을 준비 다 되었다!
'''
X_train     : train 데이터의 feature데이터
y_train     : train 데이터의 label데이터(뒤에 붙은 정답값)
X_test      : test 데이터의 feature데이터
y_test      : test 데이터의 label데이터(뒤에 붙은 정답값)
'''



from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성 ---> estimator
dt_clf = DecisionTreeClassifier(random_state=11) #트리 기반(분류 기반) 알고리즘
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression(max_iter=50)    #회귀 기반(좌표평면 경사 기반) 알고리즘  - max_iter 파라미터 있음. 설정 안해주면 오히려 느려짐...
                                            #                                      - 안에서 돌다가 해당 max_iter를 만나면 그만 돈다. (시간때문에 사용)

# DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train , y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

# RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train , y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))

# LogisticRegression 학습/예측/평가
lr_clf.fit(X_train , y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))



#KFold
from sklearn.model_selection import KFold

def exec_kfold(clf, folds=5):
    # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한  리스트 객체 생성.
    kfold = KFold(n_splits=folds)
    scores = []
    
    # KFold 교차 검증 수행. 
    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성
        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]
        
        # Classifier 학습, 예측, 정확도 계산 
        clf.fit(X_train, y_train) #clf는 DecisionTreeClassifier 객체였었음.
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        scores.append(accuracy)
        print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))     
    
    # 5개 fold에서의 평균 정확도 계산. 
    mean_score = np.mean(scores)
    print("평균 정확도: {0:.4f}".format(mean_score)) 
# exec_kfold 호출
exec_kfold(dt_clf , folds=5)



#cross_val_score (Stratified KFold 사용, 위의 KFold와 값이 다름)
from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf, X_titanic_df , y_titanic_df , cv=5)
for iter_count,accuracy in enumerate(scores):
    print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))

print("평균 정확도: {0:.4f}".format(np.mean(scores)))



from sklearn.model_selection import GridSearchCV

parameters = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]} #36개의 경우의 수

grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5) #train/test 알고리즘 수행은 총 36x5 = 180번 돈다.
grid_dclf.fit(X_train , y_train)

scores_df = pd.DataFrame(grid_dclf.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score']]
print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))
best_dclf = grid_dclf.best_estimator_ #fit를 통해 최적의 하이퍼 파라미터로 학습시킨 estimator를 best_dclf에 담았다. 

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_dclf.predict(X_test) #X_test(test데이터의 feature)로 예측하여 예상 y_test(test데이터의 label)을 dpredictions에 담았다. 
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy))
'''
GridSearchCV를 통해 지금까지 최고의 정확도를 획득했습니다!
'''

■■■■■■■■■■■■■0503 수업中 새로 알게된것■■■■■■■■■■■■■

머신러닝 : 데이터 가공/변환 → 모델 학습/예측 → 평가
회귀의 예측 성능 평가 지표
분류의 예측 성능 평가 지표	1. 정확도
			2. 오차행렬
			3. 정밀도
			4. 재현율
			5. F1 스코어
			6. ROC AUC

오차행렬 confusion_matrix()♣
- TN, FP, FN, TP 사분면부터 그리고 시작해라
	     예측 	N	P
	실제	
	   N	TN	FP  | 정밀도
	   P	FN	TP  |
		ㅡㅡㅡㅡㅡㅡ
		재현율
- 다중 분류 Multiple Classification → 각 label마다 matrix 만든다. label 3개면 각 label을 P로 둔 3개의 matrix
	     예측 	P	N	N
	실제	
	   P	TP________|_FN______FN__| 
	   N	FP|	| TN	TN  |			--- 사분면 만들기
	   N	FP|	| TN	TN  |	
	> Micro Average(3개합 TPtotal,FNtotal,FPtotal,TNtotal) 	--- 최종 사분면 만들기
	> Macro Average(세 개의 재현율,정밀도 구해서 평균)
	> Weighted Averate(Macro Average에 가중치)

- 예측 N, P에서 중점적으로 찾아야 하는 매우 적은 수의 결괏값에 Positive, 나머지에 Negative 주자
- 정밀도와 재현율은 trade-off관계(하나가 높아지면 하나는 낮아짐). 그래서 케이스별로 둘 중에 잘 선택해야 함

- 정확도(accuracy)♣	= (TN + TP) / (TN + FP + FN + TP)
	FP+FN 낮춰야 함
- 정밀도(precision_score)♣ 	= TP / (FP + TP)
	실제 N데이터를 P으로 잘못 판단하면 치명적인 경우 ex_스팸메일 판단모델
	FP 낮춰야 함
- 재현율(recall_score)♣ 	= TP / (FN + TP)   (TPR, True Positive Rate, 민감도)
	실제 P데이터를 N으로 잘못 판단하면 치명적인 경우 ex_암 판단모델, 금융사기 적발모델
	FN 낮춰야 함
	cf. 민감도 TPR vs. 특이성 TNR
		TPR = TP / (FN + TP), 질별이 있는 사람은 질병이 있는 것으로 양성 판정, 클수록 Good!
		TNR = TN / (TN + FP), 질병이 없는 사람은 질병이 없는 것으로 음성 판정, 클수록 Good!
- F1스코어(score)♣	= (2*precision*recall) / (precision+recall)   (조화평균)
	정밀도와 재현율을 결합한 지표
	precision과 recall이 0 에 가까울수록 F1 score도 동일하게 낮은 값을 갖도록 하기 위함
	분류 클래스 간 데이터가 심각한 불균형을 이루는 경우 사용
- ROC곡선(Receiver Operation Characteristic Curve)과 AUC♣
	FPR = 1 - TNR = 1 - 특이성
	ROC곡선 → x축은 FPR (작을수록 Good!), y축은 TPR(작을수록 Good!)
	AUC      → ROC곡선의 면적 : 1(최댓값, TPR과 FPR은 각각 100% 넘을순 없으니까)에 가까울수록 Good!

   test┐
fit ----------> predict ----> 예측값 ex_타이타닉이면 생존, 사망 (1, 0)
        ↓
    predict_proba() ; 사실 예측 확률을 반환하는 이 과정이 숨겨져 있었음.
       0    1
  > (0.4  0.6)	=	1   →예측값 : 0.5보다 큰 확률을 가지는 것으로 정해짐. 여기서는 0.5가 임계값(threshold)
  > (0.9  0.1)	=	0
  > (0.1  0.9)	=	1
     ...			...

threshold를 변화시키며 정밀도, 재현율을 높이고 낮출 수 있다. 
	threshold 증가 - 재현율 감소, 정밀도 증가
	threshold 감소 - 재현율 증가, 정밀도 감소

■■■■■■■■■■■■■0504 수업中 새로 알게된것■■■■■■■■■■■■■

# 이진분류에서 accuracy만 따지면 생기는 문제 

import numpy as np
from sklearn.base import BaseEstimator

class MyDummyClassifier(BaseEstimator): #BaseEstimator클래스 상속 - 우리가 나름대로 최적의 estimator 커스터마이징하기 위함
    '''
    원래는 
    mydummy_estimator = MyDummyClassifier()
    mydummy_estimator.fit(X_train, y_train)
    mydummy_estimator.predict(X_test)
    '''
    
    # fit( ) 메소드는 아무것도 학습하지 않음. 
    def fit(self , X , y=None):
            pass
    
    # predict( ) 메소드는 단순히 Sex feature가 1 이면 0 , 그렇지 않으면 1 로 예측함. 
    def predict(self, X): #X = X_test들어옴. test의 feature데이터. 
        pred = np.zeros( ( X.shape[0] , 1)) #titanic_df.shape는 (891, 12), (X.shape[0], 1)은 (test의 feature데이터의 행 크기, 1). 얘를 0으로 꽉 채워줌
        for i in range (X.shape[0]) :
            if X['Sex'].iloc[i] == 1: #남자냐? - 죽었습니다
                pred[i] = 0
            else :                    #여자냐? - 살았습니다
                pred[i] = 1
        return pred         #이렇게 이진 분류로 해버리고 accuracy를 측정해보자~ (분명히 정확도 높게 나올 것. 이러면 안됨!)
    
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Null 처리 함수
def fillna(df):
    df['Age'].fillna(df['Age'].mean(),inplace=True)
    df['Cabin'].fillna('N',inplace=True)
    df['Embarked'].fillna('N',inplace=True)
    df['Fare'].fillna(0,inplace=True)
    return df

# 머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)
    return df

# 레이블 인코딩 수행. 
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin','Sex','Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 앞에서 설정한 Data Preprocessing 함수 호출
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 원본 데이터를 재로딩, 데이터 가공, 학습데이터/테스트 데이터 분할. 
titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived', axis=1)
X_titanic_df = transform_features(X_titanic_df)
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, \
                                                  test_size=0.2, random_state=0)

# 위에서 생성한 Dummy Classifier를 이용하여 학습/예측/평가 수행. 
myclf = MyDummyClassifier()
myclf.fit(X_train, y_train)

mypredictions = myclf.predict(X_test)
print('Dummy Classifier의 정확도는: {0:.4f}'.format(accuracy_score(y_test , mypredictions)))

■■■■■■■■■■■■■0505 수업中 새로 알게된것■■■■■■■■■■■■■

from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

class MyFakeClassifier(BaseEstimator):
    def fit(self,X,y):
        pass
    
    # 입력값으로 들어오는 X 데이터 셋의 크기만큼 모두 0값으로 만들어서 반환
    def predict(self,X):
        return np.zeros( (len(X), 1) , dtype=bool)  #len((행, 열)) : 행의 크기!, 모두 0으로 채우고 그 값을 boolean으로 바꿔라 => 전부 False로 바뀜

# 사이킷런의 내장 데이터 셋인 load_digits( )를 이용하여 MNIST 데이터 로딩
digits = load_digits()

print(digits.data)
print("### digits.data.shape:", digits.data.shape)
print(digits.target)
print("### digits.target.shape:", digits.target.shape)

# digits번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 False이고 0으로 변환. 
#y = digits.target                      #label값 받아옴
#y = (digits.target == 7)               #digits.target가 7일 때만 True, 나머지 False
y = (digits.target == 7).astype(int)    #True면 1, False면 0으로 type casting
X_train, X_test, y_train, y_test = train_test_split( digits.data, y, random_state=11)
test = X_train[0].reshape(8, 8) #X_train의 크기는 (1347, 64). 8x8의 1347개의 데이터가 있을텐데 8x8을 쫙 펴서 하나의 row안에 담음. 
                            #X_train의의 row 한 줄을 8x8형태로 바꿔서 test에 담음(원래대로 돌아옴)
                            #0을 제외하고 숫자인 것만 이어보면 2의 형태와 비슷. 

# 불균형한 레이블 데이터 분포도 확인. 
print('레이블 테스트 세트 크기 :', y_test.shape)
print('테스트 세트 레이블 0 과 1의 분포도')
print(pd.Series(y_test).value_counts())

# Dummy Classifier로 학습/예측/정확도 평가
fakeclf = MyFakeClassifier()
fakeclf.fit(X_train , y_train)
fakepred = fakeclf.predict(X_test)
print('모든 예측을 0으로 하여도 정확도는:{:.3f}'.format(accuracy_score(y_test , fakepred))) #90%가 나옴 => 모든 레이블 0~9까지가 동일한 비율로 있었구나~

from sklearn.metrics import confusion_matrix

# 앞절의 예측 결과인 fakepred와 실제 결과인 y_test의 Confusion Matrix출력
confusion_matrix(y_test , fakepred)

from sklearn.metrics import accuracy_score, precision_score , recall_score

print("정밀도:", precision_score(y_test, fakepred))
print("재현율:", recall_score(y_test, fakepred))



from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix

def get_clf_eval(y_test , pred): #한번에 구하는 함수
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    print('오차 행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(accuracy , precision ,recall))


import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Null 처리 함수
def fillna(df):
    df['Age'].fillna(df['Age'].mean(),inplace=True)
    df['Cabin'].fillna('N',inplace=True)
    df['Embarked'].fillna('N',inplace=True)
    df['Fare'].fillna(0,inplace=True)
    return df

# 머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)
    return df

# 레이블 인코딩 수행. 
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin','Sex','Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 앞에서 설정한 Data Preprocessing 함수 호출
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
    
    # 원본 데이터를 재로딩, 데이터 가공, 학습데이터/테스트 데이터 분할. 
titanic_df = pd.read_csv('C:/jeon/titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived', axis=1)
X_titanic_df = transform_features(X_titanic_df)

X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, \
                                                    test_size=0.20, random_state=11)

lr_clf = LogisticRegression()

lr_clf.fit(X_train , y_train)
pred = lr_clf.predict(X_test)
get_clf_eval(y_test , pred)

pred_proba = lr_clf.predict_proba(X_test) #pred_proba에는 개별 데이터별 예측 확률이 담김
pred  = lr_clf.predict(X_test) #pred에는 예측값이 담김
print('pred_proba()결과 Shape : {0}'.format(pred_proba.shape))
print('pred_proba array에서 앞 3개만 샘플로 추출 \n:', pred_proba[:3])

# 예측 확률 array 와 예측 결과값 array 를 concatenate 하여 예측 확률과 결과값을 한눈에 확인
pred_proba_result = np.concatenate([pred_proba , pred.reshape(-1,1)],axis=1) #column방향으로 옆으로 쫙 이어 붙임
#reshape(-1,1) 안해주면 오류남. pred : (179,) 1차원, pred_proba : (179, 2) 2차원. pred.reshape(-1,1)로 2차원으로 바꿔줘서 concatenate 할 수 있게 됨
print('두개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \n',pred_proba_result[:3])




from sklearn.preprocessing import Binarizer #threshold 바꾸기 위함

X = [[ 1, -1,  2],
     [ 2,  0,  0],
     [ 0,  1.1, 1.2]]

# threshold 기준값보다 같거나 작으면 0을, 크면 1을 반환
binarizer = Binarizer(threshold=1.1)                     
print(binarizer.fit_transform(X))



from sklearn.preprocessing import Binarizer

#Binarizer의 threshold 설정값. 분류 결정 임곗값임.  
custom_threshold = 0.5

# predict_proba( ) 반환값의 두번째 컬럼 , 즉 Positive 클래스 컬럼 하나만 추출하여 Binarizer를 적용
pred_proba_1 = pred_proba[:,1].reshape(-1,1) #pred_proba[:,1] : threshold 기준값보다 커서 1이 나올 확률들만 셀렉트

binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) 
custom_predict = binarizer.transform(pred_proba_1)

get_clf_eval(y_test, custom_predict) #한번에 구하는 함수
'''정확도: 0.8492, 정밀도: 0.7742, 재현율: 0.7869    => 위에서 Binarizer, predict_proba() 없이 구한 값과 똑같아야 함'''

# Binarizer의 threshold 설정값을 0.4로 설정. 즉 분류 결정 임곗값을 0.5에서 0.4로 낮춤  
custom_threshold = 0.4 #positive를 늘리겠다 == negative를 줄이겠다 == FN을 줄이겠다
pred_proba_1 = pred_proba[:,1].reshape(-1,1) #1이 될 확률만 따로 넣어줌
binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) #여기부터 뭔가 바뀜
custom_predict = binarizer.transform(pred_proba_1)

get_clf_eval(y_test , custom_predict) #한번에 구하는 함수
'''정확도: 0.8380, 정밀도: 0.7286, 재현율: 0.8361    => 정확도, 정밀도는 떨어지고 재현율은 올라갔다'''



# 테스트를 수행할 모든 임곗값을 리스트 객체로 저장. 
thresholds = [0.4, 0.45, 0.50, 0.55, 0.60] #정확도, 정밀도, 재현율 즉 오차행렬의 변화가 어떤 경향을 띠는지 파악하자!

def get_eval_by_threshold(y_test , pred_proba_c1, thresholds):
    # thresholds list객체내의 값을 차례로 iteration하면서 Evaluation 수행.
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) 
        custom_predict = binarizer.transform(pred_proba_c1)
        print('임곗값:',custom_threshold)
        get_clf_eval(y_test , custom_predict)
get_eval_by_threshold(y_test ,pred_proba[:,1].reshape(-1,1), thresholds )
'''threshold값이 커질수록 재현율은 줄어들고 정밀도는 늘어난다. 여기서는 정확도도 늘어남'''



# ** precision_recall_curve( ) 를 이용하여 임곗값에 따른 정밀도-재현율 값 추출 **
from sklearn.metrics import precision_recall_curve

# 레이블 값이 1일때의 예측 확률을 추출 
pred_proba_class1 = lr_clf.predict_proba(X_test)[:, 1] 

# 실제값 데이터 셋과 레이블 값이 1일 때의 예측 확률을 precision_recall_curve 인자로 입력 
precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1 )
print('반환된 분류 결정 임곗값 배열의 Shape:', thresholds.shape)   #x축이 threshold일 때
print('반환된 precisions 배열의 Shape:', precisions.shape)        #정밀도(precisions)는 상승그래프
print('반환된 recalls 배열의 Shape:', recalls.shape)              #재현율(recalls)은 하강그래프

print("thresholds 5 sample:", thresholds[:5])
print("precisions 5 sample:", precisions[:5])
print("recalls 5 sample:", recalls[:5])

#반환된 임계값 배열 로우가 147건이므로 샘플로 10건만 추출하되, 임곗값을 15 Step으로 추출. 
thr_index = np.arange(0, thresholds.shape[0], 15) #np.arange(0, 143, 15) → [0 15 30 ... ] ndarray형으로
print('샘플 추출을 위한 임계값 배열의 index 10개:', thr_index)
print('샘플용 10개의 임곗값: ', np.round(thresholds[thr_index], 2))

# 15 step 단위로 추출된 임계값에 따른 정밀도와 재현율 값 
print('샘플 임계값별 정밀도: ', np.round(precisions[thr_index], 3))
print('샘플 임계값별 재현율: ', np.round(recalls[thr_index], 3))



# ** 임곗값의 변경에 따른 정밀도-재현율 변화 곡선을 그림 **
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def precision_recall_curve_plot(y_test , pred_proba_c1):
    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. 
    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)
    
    print(precisions.shape) #144
    print(recalls.shape)    #144
    print(thresholds.shape) #143
    
    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시
    plt.figure(figsize=(8,6))
    threshold_boundary = thresholds.shape[0] #x축으로 들어갈 thresholds의 크기를 저장 
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision') #plt.plot(x축, y축)
    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall') #x축 값이 143, y축 값이 144로 다름. x크기만큼 y크기 정해줘야 오류안남
    
    # threshold 값 X 축의 Scale을 0.1 단위로 변경
    start, end = plt.xlim()
    print(thresholds[0], thresholds[-1])    #element의 최소/최댓값
    print(start , end)                      #x축의 최소/최댓값 : 얘가 조금 더 범위가 넓어서 그래프가 꽉 차지 않은 것
    plt.xticks(np.round(np.arange(start, end, 0.1),2))
    
    # x축, y축 label과 legend, 그리고 grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend()
    plt.grid() #격자무늬
    plt.show()
    
precision_recall_curve_plot( y_test, lr_clf.predict_proba(X_test)[:, 1] )



# ### 3.4 F1 Score
from sklearn.metrics import f1_score 
f1 = f1_score(y_test , pred)
print('F1 스코어: {0:.4f}'.format(f1))

def get_clf_eval(y_test , pred):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    # F1 스코어 추가
    f1 = f1_score(y_test,pred)
    print('오차 행렬')
    print(confusion)
    # f1 score print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1:{3:.4f}'.format(accuracy, precision, recall, f1))
    '''
    마지막 두 케이스를 보자. 
    threshold=0.55일 때 정확도: 0.8659, 정밀도: 0.8364, 재현율: 0.7541, F1:0.7931
    threshold=0.6일 때 정확도: 0.8771, 정밀도: 0.8824, 재현율: 0.7377, F1:0.8036
    정밀도와 재현율의 차이가 더 심해졌지만, 재현율이 안좋아진 것 보다 정밀도가 훨씬 크게 좋아졌기 때문에 F1도 조금 커짐. 
    
    그렇다고 F1이 좋다고 무조건 좋은 건 아님. 극단적인 경우를 거르기 위해서 사용하자. 
    '''

thresholds = [0.4 , 0.45 , 0.50 , 0.55 , 0.60]
pred_proba = lr_clf.predict_proba(X_test)
get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)



# ## 3-5 ROC Curve와 AUC
from sklearn.metrics import roc_curve

# 레이블 값이 1일때의 예측 확률을 추출 
pred_proba_class1 = lr_clf.predict_proba(X_test)[:, 1] 

fprs , tprs , thresholds = roc_curve(y_test, pred_proba_class1)
# 반환된 임곗값 배열에서 샘플로 데이터를 추출하되, 임곗값을 5 Step으로 추출. 
# thresholds[0]은 max(예측확률)+1로 임의 설정됨. 이를 제외하기 위해 np.arange는 1부터 시작
thr_index = np.arange(1, thresholds.shape[0], 5)
print('샘플 추출을 위한 임곗값 배열의 index:', thr_index)
print('샘플 index로 추출한 임곗값: ', np.round(thresholds[thr_index], 2))

# 5 step 단위로 추출된 임계값에 따른 FPR, TPR 값
print('샘플 임곗값별 FPR: ', np.round(fprs[thr_index], 3))
print('샘플 임곗값별 TPR: ', np.round(tprs[thr_index], 3))


def roc_curve_plot(y_test , pred_proba_c1):
    # 임곗값에 따른 FPR, TPR 값을 반환 받음. 
    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba_c1)

    # ROC Curve를 plot 곡선으로 그림. 
    plt.plot(fprs , tprs, label='ROC')
    # 가운데 대각선 직선을 그림. 
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    
    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1),2))
    plt.xlim(0,1); plt.ylim(0,1)
    plt.xlabel('FPR( 1 - Sensitivity )'); plt.ylabel('TPR( Recall )')
    plt.legend()
    plt.show()
    
roc_curve_plot(y_test, lr_clf.predict_proba(X_test)[:, 1] )


from sklearn.metrics import roc_auc_score

### 아래는 roc_auc_score()의 인자를 잘못 입력한 것으로, 책에서 수정이 필요한 부분입니다. 
### 책에서는 roc_auc_score(y_test, pred)로 예측 타겟값을 입력하였으나 
### roc_auc_score(y_test, y_score)로 y_score는 predict_proba()로 호출된 예측 확률 ndarray중
###                                                Positive 열에 해당하는 ndarray입니다. 

#pred = lr_clf.predict(X_test)
#roc_score = roc_auc_score(y_test, pred)

pred_proba = lr_clf.predict_proba(X_test)[:, 1]
roc_score = roc_auc_score(y_test, pred_proba)
print('ROC AUC 값: {0:.4f}'.format(roc_score))

■■■■■■■■■■■■■0506 수업中 새로 알게된것■■■■■■■■■■■■■

###################피마 인디언 당뇨병 예측##################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler, Binarizer
from sklearn.linear_model import LogisticRegression

diabetes_df = pd.read_csv('C:/jeon/diabetes.csv')
print(diabetes_df['Outcome'].value_counts())
diabetes_df.info( )



each_mean=[]
cng_threshold=False
#정확도, 정밀도, 재현율, F1, AUC 구하기 
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
    if cng_threshold:
        mean = (accuracy + precision + recall + f1 + roc_auc)/5
        each_mean.append(mean)

X = diabetes_df.iloc[:, :-1]
y = diabetes_df.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156)

# 로지스틱 회귀로 학습,예측 및 평가 수행. 
lr_clf = LogisticRegression()
lr_clf.fit(X_train , y_train)
pred = lr_clf.predict(X_test)
# roc_auc_score 수정에 따른 추가
pred_proba = lr_clf.predict_proba(X_test)[:, 1]
get_clf_eval(y_test , pred, pred_proba)



#precisions, recalls 그래프그리기
precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba) # 이 세 개의 size가 다름. 가장 작은 thresholds값까지만 그려야 함
plt.plot(thresholds, precisions[0:thresholds.shape[0]]) #plt.plot(x축, y축)
plt.plot(thresholds, recalls[0:thresholds.shape[0]]) #x축 값이 143, y축 값이 144로 다름. x크기만큼 y크기 정해줘야 오류안남
plt.xlabel('Threshold value'); plt.ylabel('Precision, Recall')



#['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']각 컬럼에서 0값이 차지하는 비율 소수점 둘째자리까지
check_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for x in check_columns:
    new_column = X[x].astype(bool)
    cnt=0
    for idx, i in enumerate(new_column):
        if i==False:
            cnt+=1
            X[x][idx] = X[x].agg('mean')
    print(x, '에서 0이 차지하는 비율: ', round((cnt / new_column.shape[0])*100 , 2))



#StandardScaler - 평균 0 분산 1 표준화 지원
X = diabetes_df.iloc[:, :-1]
y = diabetes_df.iloc[:, -1]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 156, stratify=y)
    #y(레이블값)의 분포 비율과 같게 split해줘~!



#logisticRegression 이용 학습 및 예측
lr_clf = LogisticRegression()
lr_clf.fit(X_train , y_train)
pred = lr_clf.predict(X_test) # 1 0 1 0 .. 예측값
pred_proba = lr_clf.predict_proba(X_test)[:, 1] #확률값


def get_eval_by_threshold(y_test , pred_proba_c1, thresholds):
    # thresholds list객체내의 값을 차례로 iteration하면서 Evaluation 수행.
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) 
        custom_predict = binarizer.transform(pred_proba_c1)
        get_clf_eval(y_test, custom_predict, pred_proba_c1) #AUC는 x축이 FPR로, 이미 threshold의 변화를 통해 설정되는 값이므로 threshold가 아무리 변해도 AUC는 같다. 
    

cng_threshold=True
thresholds = [0.3, 0.33, 0.36, 0.39, 0.42, 0.45, 0.48, 0.50]
get_eval_by_threshold(y_test ,pred_proba.reshape(-1,1), thresholds) #reshape안해주면 오류뜸. 걍 해주기
print(round(max(each_mean), 4))
cng_threshold=False



#[선생님 풀이] - 나랑 값 다름...ㅜ

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

diabetes_df = pd.read_csv('C:/jeon/diabetes.csv')
print(diabetes_df['Outcome'].value_counts())

X = diabetes_df.drop('Outcome', axis = 1 )
y = diabetes_df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\
                                                    random_state = 156, stratify=y)

lr_clf = LogisticRegression()
lr_clf.fit(X_train , y_train)
pred = lr_clf.predict(X_test)

#accuracy = accuracy_score(y_test , pred)
#precision = precision_score(y_test , pred)
#recall = recall_score(y_test , pred)

pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)

print(len(thresholds))

plt.plot(thresholds , precisions[ 0:len(thresholds)] )
plt.plot(thresholds , recalls[ 0:len(thresholds)] )

zero_features = ['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI']

for row in zero_features:
    temp = X[X[row]==0][row]
    print(round((temp.shape[0]/X.shape[0]) * 100 , 2))
    X[row] = X[row].replace(0, X[row].mean())
    
for row in zero_features:
    temp = X[X[row]==0][row]
    print(round((temp.shape[0]/X.shape[0]) * 100 , 2))
    
    
scaler = StandardScaler( )
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2,\
                                                    random_state = 156, stratify=y)

lr_clf = LogisticRegression()
lr_clf.fit(X_train , y_train)
pred = lr_clf.predict(X_test)
pred_proba_c1 = lr_clf.predict_proba(X_test)[:,1]
pred_proba_c1 = pred_proba_c1.reshape(-1,1)
thresholds = [0.3 , 0.33 ,0.36,0.39, 0.42 , 0.45 ,0.48, 0.50]

from sklearn.preprocessing import Binarizer

for row in thresholds:
    binarizer = Binarizer(threshold=row).fit(pred_proba_c1) 
    custom_predict = binarizer.transform(pred_proba_c1)
    print('임곗값:',row)
    confusion = confusion_matrix( y_test, custom_predict)
    accuracy = accuracy_score(y_test , custom_predict)
    precision = precision_score(y_test , custom_predict)
    recall = recall_score(y_test , custom_predict)
    f1 = f1_score(y_test,custom_predict)
    roc_auc = roc_auc_score(y_test, pred_proba_c1)
    temp = (accuracy + precision + recall + f1 + roc_auc) / 5
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}, 5개평균:{5:.4f}'.format(accuracy,\
        precision, recall, f1, roc_auc, temp))
    
■■■■■■■■■■■■■0507 수업中 새로 알게된것■■■■■■■■■■■■■

결정트리
- 한 쪽으로 label값이 완전히 몰려서( ;균일도가 높다 ) 다른 한 쪽이 0이 되는 순간(label값이 하나만 남는 순간)까지 끝까지 트리 타고 내려감 => 무조건 Overfitting
- Overfitting을 막기 위해 전에 배웠던 min_samples_split, max_depth 등의 제약을 미리 정해줘서 어느 이상 파고들지 못하게!!
- 성능 : 어떤 기준으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가
	-최대한 많은 label값을 한쪽으로 몰 수 있는 규칙 노드(Decision Node) == 높은 균일도
	-깊지 않은 트리의 깊이(depth)
- 균일도 측정
	1. 정보 이득이 높아야 한다. (정보 이득) = 1 - (엔트로피)
		엔트로피 = - ∑ p(i|t) x log₂p(i|t) 	//p(i|t) : 특정 노드(t)에서 i가 차지하는 비율
	2. 지니 계수가 낮아야 한다(?). 0_평등-------------불평등_1
		지니 계수 = ∑ p(i|t) x (1-p(i|t))  => i에 대한 이차함수 (2진분류이든, 3진분류이든)
- 결정트리라는 estimator 안에 최대한 균일도를 높일 수 있는 결정 노드가 되어 줄 column, 그에 해당하는 조건(ex_ '< 0.1')을 구해주는 함수가 자동적으로 구현되어 있다. 
- 균일도만 신경 쓰면 돼서 보통 피처 스케일링/정규화 전처리 필요 없음

========================================================

#불순도 지표로 3가지 각조건을 사용하였을때의 결과

import matplotlib.pyplot as plt
import numpy as np

#지금 여기에서는 gini, entropy가 2진분류라고 가정하고 있다.
def gini(p):
    return p * (1 - p) + (1 - p) * (1 - (1 - p)) 


def entropy(p):
    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))


def error(p):
    return 1 - np.max([p, 1 - p])

x = np.arange(0.0, 1.0, 0.01)

ent = [entropy(p) if p != 0 else None for p in x]

sc_ent = [e * 0.5 if e else None for e in ent]

err = [error(i) for i in x]

fig = plt.figure()

ax = plt.subplot(111)

for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], 
                          ['Entropy', 'Entropy (scaled)', 
                           'Gini Impurity', 'Misclassification Error'],
                          ['-', '-', '--', '-.'],
                          ['black', 'lightgray', 'red', 'green', 'cyan']):
    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)


ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),
          ncol=5, fancybox=True, shadow=False)

ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')
ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')

plt.ylim([0, 1.1])
plt.xlabel('p(i=1)')
plt.ylabel('Impurity Index')
plt.show()

==========================================================

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# DecisionTree Classifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리
iris_data = load_iris()
X_train , X_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target,
                                                       test_size=0.2,  random_state=11)

# DecisionTreeClassifer 학습. 
dt_clf.fit(X_train , y_train)


from sklearn.tree import export_graphviz

# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함. 
export_graphviz(dt_clf, out_file="C:/jeon/tree.dot", class_names=iris_data.target_names , \
                feature_names = iris_data.feature_names, impurity=True, filled=True)
    #"C:/jeon/tree.dot"경로에 dt_clf estimator 만들어주세요~!
    #class_names, feature_names 지정 안해주면 svg파일 만들어질 때 클래스명 안 나오고 피처 컬럼네임 X[1], X[2], ...이런 식으로 나옴

'''
digraph Tree {
node [shape=box, style="filled", color="black"] ;
0 [label="petal length (cm) <= 2.45\ngini = 0.667\nsamples = 120\nvalue = [41, 40, 39]\nclass = setosa", fillcolor="#fffdfd"] ;
1 [label="gini = 0.0\nsamples = 41\nvalue = [41, 0, 0]\nclass = setosa", fillcolor="#e58139"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.55\ngini = 0.5\nsamples = 79\nvalue = [0, 40, 39]\nclass = versicolor", fillcolor="#fafefc"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.25\ngini = 0.051\nsamples = 38\nvalue = [0, 37, 1]\nclass = versicolor", fillcolor="#3ee684"] ;
2 -> 3 ;
4 [label="gini = 0.0\nsamples = 37\nvalue = [0, 37, 0]\nclass = versicolor", fillcolor="#39e581"] ;
3 -> 4 ;
...
이런 식으로 word파일에 저장됨
'''

import graphviz

# 위에서 생성된 tree.dot 파일을 Graphviz 읽어서 Jupyter Notebook상에서 시각화 
with open("C:/jeon/tree.dot") as f:
    dot_graph = f.read()
g = graphviz.Source(dot_graph)
g.format = 'svg'
g.filename = 'test_tree'
g.directory = 'C:/jeon'
g.render()

■■■■■■■■■■■■■0510 수업中 새로 알게된것■■■■■■■■■■■■■

일부 이상치(outlier) 데이터까지 분류해서 과적합되지 않도록 파라미터 설정 필요!

min_samples_split : sample의 개수가 최소 min_samples_split는 되어야 leaf 만들어짐
min_samples_leaf : default=1, 만들어진 마지막 leaf노드에서 sample의 개수가 min_samples_leaf 이상이어야 함
		한쪽으로 sample 몰려있으면 무조건 leaf, 그렇지 않더라도 min_samples_leaf에 따라 leaf가 될 수 있음 
	ex_ min_samples_split = 4, min_samples_leaf = 2
	(1,2,2) --- sample의 개수가 4 이상이라 자식 노드 만들 수 있지만, 동시에 나뉜 sample 중에서 2보다 작은 1이 있으므로, 얘는 leaf노드가 될 수 없다. 여기서 멈추고 이전 단계를 leaf노드로 정한다. 

상관관계 : 일반적으로 한 쌍의 변수가 선형 적으로 관련된 정도
y = 2x : 양의 상관관계가 있다
y = -x : 음의 상관관계가 있다
y = 3 : 상관계수가 있다

xx, yy = make_classification(조건) : 사이킷런에서 분류를 위한 테스트용 데이터를 내 마음대로 쉽게 만들 수 있도록 하는 함수

==========================================================

# ### 결정 트리(Decision TREE) 과적합(Overfitting)

# In[ ]:


from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

plt.title("3 Class values with 2 Features Sample data creation")

# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성. 
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,    #(row size - default 100, column size - n_features 2), n_features 2개 중에 y_labels과 상관관계가 있는 피처는 n_informative 2개.
                             n_classes=3, n_clusters_per_class=1,random_state=0)            #label값 n_classes 3개, n_clusters_per_class=1 : y_labels는 '하나의 label데이터 -- 하나의 군집을 이루었다'
    #make_classification(기준) : 기준에 맞는 데이터 리턴

# plot 형태(scatter:점찍는 그래프)로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨. 
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k')
            # x축             y축               동그라미로표기   y_label로 클래스 나눔     다른 색깔로 구분


# In[ ]:


import numpy as np

test=[]
test1=[]
test2=[]
test3=[]
test4=[]
test5=[]
test6=[]
test7=[]
test8=[]

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    '''
    model : 점 찍은 그래프. 아까 그걸로 decisiontree estimator dt_clf로 학습을 시켜서(ex_너 x 3보다 크니?...계속 트리 타고 내려감) 이 모델 만듦
    X : 피처값 2개(x,y) ----------3개까지만 3차원 그래프로 표현 가능하고, 대부분 더 많은 피처값 가져서 시각화 불가능!
    y : 레이블값 3개(0,1,2)-색깔
    '''
    global test
    global test1
    global test2
    global test3
    global test4
    global test5
    global test6
    global test7
    global test8
    
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot(점찍기)으로 나타내기 - 여기까지만 하면 위에서 그려줬던 그냥 점만 찍힌 것과 똑같음
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim() #최솟값, 최댓값
    ylim_start , ylim_end = ax.get_ylim()
    test = ax.get_xlim()
    test1 = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    test2 = np.linspace(xlim_start,xlim_end, num=200) #xlim_start,ylim_start부터 xlim_end,ylim_end까지 똑같은 step으로 200개 만들어줘라
    test3 = np.linspace(ylim_start,ylim_end, num=200)
    test4, test5 = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) 
    '''
    meshgrid(x, y)의 결과
    xx : x 한 row로 두고 그걸 y 크기만큼 아래로 쫙 복사 -> shape : (y크기, x크기)
    yy : y 한 column으로 두고 그걸 x 크기만큼 오른쪽으로 쫙 복사 -> shape : (y크기, x크기)
    '''
    test6 = xx.ravel() #xx를 쫙 한 줄로 이어 붙임 : 0~199인덱스가 반복
    test7 = yy.ravel() #yy를 쫙 한 줄로 이어 붙임 : 0~199인덱스 동일-첫번째y값, 200~399인덱스 동일-두번째y값, ...
    test8 = np.c_[xx.ravel(), yy.ravel()] #xx.ravel();40000크기(1차원데이터) , yy.ravel();40000크기(1차원데이터) 두 개를 묶어서 (40000,2) 만들어짐
                                          #대응되는 좌표(인덱스)의 x,y끼리 예측해서 그 예측값을 reshape
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) #애초에 model이 앞에서 make_classification에서 100개의 scatter 데이터 그래프로 학습시킨 것. 
                                                                       #그러면 이제는 모~든 좌표! 40000개의 각각의 좌표를 피쳐로 삼아서 학습시킴. - 예측값의 유니크한 값 : 0,1,2
    
    # 자! 이제 모든 좌표에 대해 등고선 그래프 그려보자~
    # contourf() 를 이용하여 class boundary 를 visualization 수행.  //contour:컨투어, 윤곽, 등고선
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)


# In[ ]:

#과적합 해결 전 - min_samples_leaf = 1(default)

from sklearn.tree import DecisionTreeClassifier

# 특정한 트리 생성 제약없는 결정 트리의 Decsion Boundary 시각화.
dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)#dt_clf estimator 만들고 그걸로 X_features, y_labels fit까지 다 시켜줌
visualize_boundary(dt_clf, X_features, y_labels)


# In[ ]:

#과적합 해결 - min_samples_leaf = 6으로 설정!!!  //영역 구분이 명확해졌다

# min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화
dt_clf = DecisionTreeClassifier( min_samples_leaf=6).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)

■■■■■■■■■■■■■0511 수업中 새로 알게된것■■■■■■■■■■■■■

############## UCI HAR Dataset.zip 건드려보기 _ 1 #################


# ### 결정 트리 실습 - Human Activity Recognition

# In[7]:


import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.
feature_name_df = pd.read_csv('C:/jeon/human_activity/features.txt',sep='\s+',      # \s+ : 정규식, 공백 1개 이상 (\x의 의미 : 공백. +의 의미 : 바로앞의 것이 반복되는 것도 해당)
                        header=None,names=['column_index','column_name'])

print(len(feature_name_df)) #컬럼의 개수만 561개나 된다..!

# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출
feature_name = feature_name_df.iloc[:, 1].values.tolist()
print('전체 피처명에서 10개만 추출:', feature_name[:10])
feature_name_df.head(20)

'''
X_train_test = pd.read_csv('C:/jeon/human_activity/features.txt',sep='\s+',
                        header=None,names=feature_name)
 
→ ValueError: Duplicate names are not allowed. 안에 중복된 컬럼명(피처명)이 있다는 뜻! 
→ names=feature_name 지정 안해주면 잠깐은 ㄱㅊ 
→ 해결 : 중복처리 !!!
'''

# 우선 중복된 피처명이 얼마나 있는지 알아보기
feature_dup_df = feature_name_df.groupby('column_name').count() #만약 column_name 중에서 중복이 있으면 그것끼리 묶일 것. feature_dup_df의 column_index 피쳐는 중복 개수를 나타낸다. (1 넘으면 중복 있는것)
print(feature_dup_df[feature_dup_df['column_index']>1].count()) #불린인덱싱. feature_dup_df['column_index']>1이 True인 로우만 모아 그 개수 출력  //42개구나~!
feature_dup_df[feature_dup_df['column_index']>1].head()

# ### 수정 버전 01: 날짜 2019.10.27일
# 
# **원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생.**  
# **중복 feature명에 대해서 원본 feature 명에 '_1(또는2)'를 추가로 부여하는 함수인 get_new_feature_name_df() 생성**

# In[5]:

test10 = []
test11 = []
test12 = []
test13 = []

def get_new_feature_name_df(old_feature_name_df):
    global test10
    global test11
    global test12
    global test13
    
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
    test10 = feature_dup_df  #원본데이터랑 크기 same.
    ''' Age     count   cumcount
    0   10      3       0(번째 10)
    1   15      2       0
    2   10              1(번째 10)
    3   10              2(번째 10)
    4   16      1       0
    5   15              1
    '''
    
    feature_dup_df = feature_dup_df.reset_index()
    test11 = feature_dup_df
    
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    test12 = old_feature_name_df.reset_index()
    test13 = new_feature_name_df
    ''' merge
            - 특정 key값기준으로 붙이기(how = inner:교집합, outer:합집합) - index 정보(key값)을 기준으로
            - index 삭제 등으로 차이가 있을 때
                inner : 교집합, merge 결과물에서 그 index는 빠짐
                outer : 합집합, merge 결과물에서 그 index에 해당하는 값을 채워서 넣음
        concatenate
            - axis를 줘서 물리적으로 어떻게 붙일 것인지(row방향 or column방향으로 갖다붙이기)
    '''
    
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) if x[1] >0 else x[0] ,  axis=1) #중복된 피처명은 뒤에 _1, _2를 붙여서 수정해준다
    #x : new_feature_name_df(test13로 확인 가능) 안에서 'column_name', 'dup_cnt' 두 컬럼만을 모은 DataFrame이 있고, 그 안에서 row 하나 하나씩 담아옴.
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df
#test_set = get_new_feature_name_df(feature_name_df)

# In[11]:


pd.options.display.max_rows = 999
new_feature_name_df = get_new_feature_name_df(feature_name_df)
new_feature_name_df[new_feature_name_df['dup_cnt'] > 0]


# **아래 get_human_dataset() 함수는 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df() 함수를 반영하여 수정**

# In[12]:


import pandas as pd

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('C:/jeon/human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    
    # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. 
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('C:/jeon/human_activity/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('C:/jeon/human_activity/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('C:/jeon/human_activity/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('C:/jeon/human_activity/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test


X_train, X_test, y_train, y_test = get_human_dataset()


# In[13]:


print('## 학습 피처 데이터셋 info()')
print(X_train.info())


# In[14]:


print(y_train['action'].value_counts())


# In[15]:


print(X_train.isna().sum().sum())
#------------- null값이면 True, 아니면 False 담은 DataFrame
#------------------- 컬럼별 True(null값)의 개수
#------------------------- 총 True(null값)의 개수


# In[16]:


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정
dt_clf = DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train , y_train)
pred = dt_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred)
print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))

# DecisionTreeClassifier의 하이퍼 파라미터 - 그냥 파라미터 default 설정이 뭔지 확인하기 위함
print('DecisionTreeClassifier 기본 하이퍼 파라미터:\n', dt_clf.get_params())
'''
 {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None,       // 'criterion': 'gini' - 결정트리에서 트리를 뻗어나가는 근간이 되는 불평등지수가 'gini'로 default되어 있구나. 'entropy'넣어도 되겠다~.
  'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 
  'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 
  'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 156, 'splitter': 'best'}
'''

# In[17]:


from sklearn.model_selection import GridSearchCV #파라미터 설정하기 위함

'''
어떤 파라미터 설정이 최고로 우수한지 알아내기
 - GridSearchCV : 내가 바꾸고 싶은 모든 파라미터 설정값들을 params에 담아주면, 알아서 "k폴드 교차 검증" (test데이터에 Overfitting 되는거 막으려고) 기반으로 최고의 조합을 찾아줌
 - 그렇지만 파라미터 설정 max_depth만 해줌.(사용 x)
'''
params = {
    'max_depth' : [ 6, 8 ,10, 12, 16 ,20, 24]
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 ) #교차검증 fold수 = 5   →   561개 데이터를 파라미터 7번 바꿔가면서 5폴드 교차검증 = 561 x 7 x 5   →   오래 걸림!
grid_cv.fit(X_train , y_train)
print('GridSearchCV 최고 평균 정확도 수치:{0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)


# ### 수정 버전 01: 날짜 2019.10.27일  
# 
# **사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다.**  
# **기존 코드에서 오류가 발생하시면 아래와 같이 'mean_train_score'를 제거해 주십시요**
# 

# In[19]:


# GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. 
cv_results_df = pd.DataFrame(grid_cv.cv_results_)

# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출
# 사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다
# cv_results_df[['param_max_depth', 'mean_test_score', 'mean_train_score']]

# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출
cv_results_df[['param_max_depth', 'mean_test_score']]


# In[20]:

'''
어떤 파라미터 설정이 최고로 우수한지 알아내기
 - 얘는 GridSearchCV 안쓰고 노가다 할 뿐 아니라, 교차검증도 하지 않음.(사용 x) -> 교차검증 안했기 때문에 결과 다름
'''
max_depths = [ 6, 8 ,10, 12, 16 ,20, 24]
# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정
for depth in max_depths:
    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)
    dt_clf.fit(X_train , y_train)
    pred = dt_clf.predict(X_test)
    accuracy = accuracy_score(y_test , pred)
    print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy))  #max_depth = 8인 경우가 최적의 파라미터


# In[21]:

'''
BEST 1. 깊이 줄여서 정확도 성능 올리기
'''
params = {
    'max_depth' : [ 8 , 12, 16 ,20], 
    'min_samples_split' : [16,24],
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )
grid_cv.fit(X_train , y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)


# In[22]:

'''
BEST 2. BEST 1에서 구한 최적의 파라미터로 학습 완료된 estimator 객체로 test데이터 예측 수행
'''
best_df_clf = grid_cv.best_estimator_

pred1 = best_df_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred1)
print('결정 트리 예측 정확도:{0:.4f}'.format(accuracy))


# In[23]:


import seaborn as sns

ftr_importances_values = best_df_clf.feature_importances_

# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )

# 중요도값 순으로 Series를 정렬   --- 상위에 있는 feature : 트리 내릴 때 위쪽에 있는 조건일 확률 높음
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]
plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()
'''
4차원 넘는 애들을 그래프 그리기 위해서 차원 줄일 때 feature(컬럼)을 무작정 삭제하는 게 아니다. 
feature가 1, label이 1개 있을 때 연관관계가 1이라면, 두 개의 scale은 당연히 다를 수 있지만
feature     label       ←이런 식으로 완벽히 똑같이 바뀐다는 뜻. 
1           10
10          100
이런 애들을 합치는 방향으로 생각하는 것이다. 
'''

■■■■■■■■■■■■■0512 수업中 새로 알게된것■■■■■■■■■■■■■

딥러닝			- 비정형 데이터 (이미지, 영상, 음성 등 오직 하나의 요소만 보면 별 의미 없는 것들)
앙상블(특히 LightGBM)	- 정형 데이터

앙상블 학습
- 여러 개의 분류기(Classifier에서 나온 estimator) - 그 예측들을 결합 (집단지성)
- 보팅 (Voting)	: 서로 다른 알고리즘 estimator로 같은 Data
		- 하드 보팅	다수의 classifier 간 다수결로 최종 class값 결정 (estimator(분류기) 후지더라도, 개수가 많으면 걔네의 정답 비율이 50넘을 확률 커진다. 정답값에 수렴하게 됨) 
		- 소프트 보팅 多	다수의 classifier들의 class확률(predict_proba)을 레이블값별로 낸 평균이 최고인 레이블값으로 최종 class값 결정
		이항분포 확률질량함수, 누적 이항분포 확률질량함수 > 1. 분류기가 많을 때는 더 나은 퍼포먼스를 낼 수 있음을 알기
							    > 2. 그걸 실제 코드로 확인하기
- 배깅 (Bagging)	: 같은 유형 알고리즘 estimator로 다른 Data
		ex_랜덤 포레스트 알고리즘	하나의 데이터세트에서 중복 가능하게 데이터샘플링하여 학습
					n_estimators : 결정트리 개수. 10개가 default. 너무 많아도 느려저서 안좋음
					max_features : feature 개수 줄이는 파라미터. feature가 너무 많으면 시간/공간복잡도 커짐. 시간복잡도가 지수함수 형태로 올라갈 수도 있음. → 그래서 상관관계가 있는 애들끼리 합쳐서 차원 축소 하는 것 .
					max_depth
					min_samples_leaf
- 부스팅 (Boosting)	: 매번 다음 분류기에게 가중치 부여해서 앞에서 학습한 분류기가 예측 틀린 데이터에 대해 다음부터 올바르게 학습하도록. 
		보팅, 배깅(병렬적인 동시 수행 가능)과 다르게 순차적으로 돌아가기 때문에 시간 느림. LightGBM 예외. 
		ex_AdaBoost	영역 분류(조건식을 준다=영역 구분한다) 기준을 분류기로 두고, 잘못 분류된 오류 데이터에 가중치를 부여하면서 부스팅 수행. 그럼 다음번 수행에서는 가중치가 붙은 곳을 조금 더 신경 써서 구분할 테니까!
		ex_그래디언트 부스팅 알고리즘, XGBoost, LightGBM
		
- 스태킹 (Stacking)	: 여러 다른 모델의 예측 결괏값을 다시 학습데이터로 만들어서 다른 모델(메타모델)로 재학습

이항분포 확률질량함수	= nCk * p^k * (1-p)^(n-k)	    (n번 시행했을 때 k번 성공할 확률)		cf. 에러율도 똑같이 구함
누적 이항분포 확률질량함수	= ∑ nCk * p^k * (1-p)^(n-k)     (k에대한 시그마 → n번 시행했을 때 k번 이상 성공할 확률)

pmf(k, n, p) : 이항분포 확률질량함수
cdf(n, p, k) : 누적 이항분포 확률질량함수 (0~k까지)
rvs(n, p, size) : 성공확률이 p인 게임을 n번 시행하는 행위를 size만큼 했을 때 모든 성공 횟수

히스토그램 == 도수분포표
Distribute히스토그램 - distplot() : 값의 개수를 막대그래프로 나타내줌

이산값-질량, 연속값-밀도. 확률밀도 : 일정범위 안에 x데이터가 포함될 확률
실제 데이터는 대부분 연속적이고, 어떤 분포를 따르는지 알 수 없다. (정규분포는 아주 소수)
→ 데이터 분포를 추정하여 확률밀도함수 P(x)를 추정하는 방법들. P(x) = K / NV = (해당 범위 안의 데이터 수) / (전체 데이터 크기 x 범위의 부피)
	1. histogram			...살짝 애매한 게 histogram은 bin값을 정해주기 때문에 본래 이산값을 위한 그래프. 그래도 그냥 kde=True로 파라미터 설정해줘서 그걸 분포 그래프로 생각하면 안될까? (ㅇㅇ 그렇게함. 가장 직관적인 방법이지만, 젤 좋은 방법은 아님.)
	2. kde (kernel density estimator)	...연속적인 수에 대해서는 histogram보다 kde가 더 타당함. 
		ex_Parzen Window ( XXX 몰라도됨 XXX. 경계값처리 모호해서 안씀. 그냥 설명하려고 가져오심)
			k(u) =  1 if |ui| ≤ h/2,   i=1,2,...D(차원)
			         0 else
			K=∑(n=1,2,..N) k(x-xn)	u에 x-xn 대입
						xn : n번째 데이터
						전체 x들 x1, x2, x3, ..., xn의 전체 범위 안에서 h라는 범위를 막 갖다댔을 때 특정 x가 그 h 안에 포함될 확률
			V = ∫ k(u)du = h^D	D차원일 때 범위 (1차원-길이 h¹, 2차원-넓이 h², 3차원-부피 h³, ...)
		ex_Gaussian Kernel ♣多
			K : Gaussian Kernel에 의거한 전체 x들 x1, x2, x3, ..., xn의 전체 범위 안에서 h라는 범위를 막 갖다댔을 때 특정 x가 그 h 안에 포함될 확률
			V : 1
			각각의 x에서 Gaussian Kernel 식에 의거해서 그래프 그림 + 전부 합침!!
			if 1차원이라면, 
			  ∩     ∩∩   ∩             ∩
			--1-----3-3.5--5------------9---
			         ┌─┐ ┌		
			  ∩    ┘    └┘└          ∩
			--1-----3-3.5--5------------9---   => 요것이 히스토그램 kde=True 했을 때 나온 추세곡선

	3. K-nearest neighbor
→ 확률밀도함수 추정도 확률질량함수 이론값 nCk * p^k * (1-p)^(n-k) 처럼, Try 많이 할수록 실제로 수렴하기 때문에 의미가 있다. 

============================================================

from scipy.special import comb
import math

   #ensemble : 앙상블
def ensemble_error(n_classifier, error): #에러발생에 관한 누적 이항분포 확률질량함수 계산
    k_start = int(math.ceil(n_classifier / 2.)) #ceil() : 소수점 올림, n_classifier=11이므로 k_start=6  →  과반수 이상이 error일 확률
    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k) for k in range(k_start, n_classifier + 1)]
    # comb(n, k) : combination, nCk
    return sum(probs)

ensemble_error(n_classifier=11, error=0.25)

import numpy as np

error_range = np.arange(0.0, 1.01, 0.01) #에러발생확률 p  //0.0부터 1.01까지 0.01step으로 ndarray 만들어줌
ens_errors = [ensemble_error(n_classifier=11, error=error) for error in error_range]
             # error값이 변화함에 따라, 전체 11번의 시행 중에서 과반수 이상이 error일 확률

import matplotlib.pyplot as plt

plt.plot(error_range, #x축
         ens_errors,  #y축         - error발생확률이 0.5보다 작으면 누적 이항분포 확률질량함수(과반수 이상이 error일 확률)가 작고, 아니면 0.5보다 높아진다. 
         label='Ensemble error', # - error와 performance(성공)확률은 반대. performance에 대한 누적 이항분포 확률질량함수는 훨씬 높아진다. (p = 1-p)
         linewidth=2)

plt.plot(error_range, 
         error_range, #y=x그래프
         linestyle='--',
         label='Base error',
         linewidth=2)

plt.xlabel('Base error')
plt.ylabel('Base/Ensemble error')
plt.legend(loc='upper left')
plt.grid(alpha=0.5)
plt.show()

==========================================================

import numpy as np
import pandas as pd
import scipy as sp
from scipy import stats  #scipy 패키지 : 통계, 분석 관련 多多 쓰임
from matplotlib import pyplot as plt
import seaborn as sns
sns.set()
# cf. MathWorks : MATLABdp 

print(sp.stats.binom.pmf(k=1, n=2, p=0.5)) #pmf : 이항분포 확률질량함수 구하는 함수 : 전체 시행횟수가 n이고 성공 확률이 p인 이항분포 게임을 k번 성공할 확률

np.random.seed(1) #시드값 : random 사용할 때 사실은 무작위로 뱉어주기 위한 규칙값이 있다. 그 규칙값 설정해주는 게 seed()
print(sp.stats.binom.rvs(n = 10, p =0.2, size=5)) #얘만 실행하면 값 실행마다 바뀜. 위의 seed코드까지 같이 실행해야 계속 일정
                    #rvs(n, p, size) : 전체 시행횟수가 n이고 성공 확률이 p인 이항분포 게임을 총 size만큼 수행한 성공횟수들을 뱉어줌


# ↓ sp.stats.binom.rvs(n = 10, p = 0.2, size=10) 를 풀어보면
binomial = sp.stats.binom(n=10, p=0.2) #전체 시행횟수가 n이고 성공 확률이 p인 이항분포 게임 객체 binomial을 만듦
np.random.seed(1)
rvs_binomial = binomial.rvs(size=10000) #size가 10, 100, 1000, 10000번 커질수록 pmf_binomial 그래프와 비슷해진다 (pmf값에 점점 수렴한다) → 확률통계의 막강한 힘 (이론과 실제 현실은 다르지만, 시행횟수를 늘리면 결국 같아진다 (이제는 시드값 바꿔도 별 차이 안남))
print(rvs_binomial)
print(rvs_binomial.mean())

m = np.arange(0,10,1)
pmf_binomial = binomial.pmf(k = m) #성공 확률이 0.2인 게임을 10번 했을 때 0번~9번 성공할 확률






#Distribute "히스토그램" distplot() : 값의 개수를 막대그래프로 나타내줌
sns.distplot(rvs_binomial, bins = m , kde = False, norm_hist = True, color = 'gray') #bins : 이산적인 값에 대한 히스토그램에서 x축 몇 토막으로 구분할지 
                                                                                     #y축 : rvs_biomial에서 x축(0~9)에 해당하는 값이 각각 몇 개 있는가 Count => 정규화해서 스케일링 1/10 해줌
                                                                                     #kde(kernel density estimator) = True로 하면 히스토그램의 밀도 추세 곡선(연속된 값으로) 그려줌
plt.plot(m, pmf_binomial, color = 'black') #얜 우리가 계속 보던 그냥 그래프. x축-m, y축-pmf_binomial(각각의 m의 element만큼 성공할 확률)

print(1 - sp.stats.binom.cdf(n = 10000, p =0.51, k=4999)) #cdf : 누적 이항분포 확률질량함수 구하는 함수 : 0 ~ k번 성공할 확률 다 더한 것
                                                #0 ~ 4999번의 성공 확률을 다 더함!
'''
성공 확률이 51%인 이항분포 게임을 10000번 시행했을 때, 0~4999번(5000 case) 성공 확률 각각을 더한 것. 
이 때 0~4999번 성공했다는 것은 성공보다 실패를 더 많이 했다는 것으로, '다수결'로 보면 그냥 실패한 것이다. 
즉, 과반수 이하의 성공 확률을 더한 것. 이 실패 확률을 cdf(누적)으로 다 더한 '에러율'이다. 
만약 p=0.61이면 k=5999라고 해야 모든 실패율을 더한 것!
그것을 1에서 소거하면 결국 "과반수 이상의 성공 확률을 더한 것"이다.
    => Hard Voting !!!

성공 확률이 51%밖에 되지 않는 분류기지만, 내가 만약 걔네를 10000개 가지고 Hard Voting 앙상블로 판단한다면 성공 확률이 97%나 된다는 것!!!
'''

■■■■■■■■■■■■■0513 수업中 새로 알게된것■■■■■■■■■■■■■

import numpy as np

np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) # argmax: 가장 큰 포지션에 있는 아이의 idx 리턴, bincount: 유니크한 원소 개수, weights: 가중치
'''1'''
print(np.bincount([0, 0, 1, 3, 3, 3])) # 0→2, 1→1, 2→0, 3→3  -> [2 1 0 3]
print(np.bincount([0, 0, 1])) # 0→2, 1→1  -> [2 1]
print(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) #각 포지션의 weight을 부여하면 0→0.4, 1→0.6  -> [0.4 0.6]
print(np.bincount([0, 0, 1, 3, 3], weights=[0.2, 0.3, 0.6, 0.1, 0.1])) # -> [0.5 0.6 0.  0.2]


ex = np.array([[0.9, 0.1],
               [0.8, 0.2],
               [0.4, 0.6]])

p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) #axis=0 : 보통 row방향(아래방향) => 그냥 평균 내면 [0.7 0.3]
print(p)    #가중치를 냈으니까 가중평균 -> 0.9*0.2, 0.8*0.2, 0.4*0.6 더해주면 끝(가중치 자체가 확률이니까) => [0.58 0.42]
p = np.average(ex, axis=0)
print(p)


from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
import six
from sklearn.base import clone
from sklearn.pipeline import _name_estimators #__init__에서 사용. estimator 넣어주면 상세 정보 list(zip(names, estimators))를 리턴
import numpy as np
import operator

test=[]
test1=[]
test2=[]

class MajorityVoteClassifier(BaseEstimator, ClassifierMixin): #BaseEstimator 상속 -> MajorityVoteClassifier 클래스를 커스터마이징한 estimator처럼 사용 가능 (fit, predict, predict_proba 메소드 보면 알수있듯)
    global test
    global test1
    global test2
    
    def __init__(self, classifiers, vote='classlabel', weights=None):

        self.classifiers = classifiers
        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)} #딕셔너리
        self.vote = vote
        self.weights = weights

    def fit(self, X, y):
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError("vote는 'probability' 또는 'classlabel'이어야 합니다."
                             "; (vote=%r)이 입력되었습니다."
                             % self.vote)

        if self.weights and len(self.weights) != len(self.classifiers):
            raise ValueError('분류기 개수와 가중치 개수는 동일해야 합니다.'
                             '; %d개의 가중치와, %d개의 분류기가 입력되었습니다.'
                             % (len(self.weights), len(self.classifiers)))

        # self.predict 메서드에서 np.argmax를 호출할 때 
        # 클래스 레이블이 0부터 시작되어야 하므로 LabelEncoder를 사용합니다. ->레이블인코딩 : LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. 
        # 레이블 인코딩에서 fit(A) - A 안의 유니크한 레이블값에 숫자 부여한 테이블 하나 만듦, B=transform(A) - fit로 만든 레이블 테이블을 가져와서 A에 적용시킨 0 1 0 1 2 1... 값을 B에 담음. fit_transform() 로 한번에 해도 됨
        self.lablenc_ = LabelEncoder() #밖에서 우리가 labelencoding 해줬기 때문에 사실 필요 없는데 그냥 해줌
        self.lablenc_.fit(y) #y: 아래에서 붖꽃데이터 test데이터 레이블인코딩 끝내고 더 아래에서 train_test_split해줘서 나온 y_train 데이터. 50개이고 비율은 0:1=1:1 
        self.classes_ = self.lablenc_.classes_ #레이블 인코딩 된 아이의 유니크한 값이 들어감. 여기선 0, 1 두개가 들어감
        self.classifiers_ = []
        for clf in self.classifiers: #클래스 객체 만들 때 받아온 estimator 3개 담긴 리스트. dtype:list, 각각의 요소 dtype:estimator
            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y)) #clone():원본말고 복사, X:feature데이터, 이제 fitted_clf : 학습된 estimator가 담겨 있음!!!
                                    #fit(X_train, y_train)해준것 => 결과물 : 학습된 estimator
                                    #근데 교차검증이기 때문에 45개의 데이터만 요기서 함. 이제 나머지 5개의 test데이터로 predict 해봐야 함.
                                    #근데 처음에 교차검증 scoring='roc_auc'(x축이 FPR로, threshold값의 변화에 따라 그래프 그려짐)를 해주었기 때문에, 확률값이 필요하다. predict가 아니라, predict_proba를 해줘야 함. 그래서 자동으로 그렇게 실행되므로 22222222222222222222222가 출력된다. 
                                    #roc_auc - threshold값 변화 - predict_proba에서 레이블값 나누는 비율 변화
            self.classifiers_.append(fitted_clf) #그 estimator가 들어감. 즉, for문 끝나면 classifiers_ = [pipe1으로 학습된 estimator, clf2로 학습된 estimator, pipe3로 학습된 estimator]
        return self

    def predict(self, X): #교차검증이기 때문에 X는 5개의 데이터(shape:5x2, cv=10이었기 때문에)
        print('111111111111111111111111111111')
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else:  # 'classlabel' 투표 - default가 여기임. 

            #  clf.predict 메서드를 사용해 결과를 모읍니다.
            predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T #.T 붙이면 전치행렬! 행렬 뒤집기. 우리가 원하는 건 하나의 estimator예측이 하나의 column에 들어가길 원한다. 
            test1.append(predictions.T) #이미 한번 전치행렬 해둬서 AT 됐으니까 (AT)T 해서 다시 A 볼 수 있음. 
            test2.append(predictions) #3개의 estimator가 5개의 데이터를 예측한 예측값을 5x3형태로 담음
            
            maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions)
            #보통 df.apply(lambda x: ~~) 하면 df에서 row 하나씩 x에 들어가는데, 여기서는 특정짓지 않고 np라고 해주었다. 대신 뒤에 arr = predictions라고 해줘서 predictions에 적용하는 것을 알 수 있다. 
            '''
            이게 Hard Voting과 연결되는 이유!!! ★★★
            3개의 estimator가 각각의 데이터에 예측한 값에서 다수결로 최종 결정한다. 
            '''
        maj_vote = self.lablenc_.inverse_transform(maj_vote) 
        #레이블 인코딩해서 조금 달라진 값을 다시 원래대로 0, 1 -> 1, 2. 근데 지금 보면 이미 밖에서 레이블 인코딩 끝난 뒤에 train_test_split해주었다. 그리고 이 위에서 혹시 빠진 경우가 있을까봐 한번 더 레이블인코딩 해주었다. (여기선 별로 의미 없었지만)
        #그래서 지금 inverse_transform해봤자, 위에 두 번째로 수행한 레이블 인코딩만 다시 원래대로 돌려놓게 된다. 그래봤자 돌려놓은 상태도 그대로 0, 1 -> 0, 1인 걸!!!
        #여기선 별 의미 없지만, 다른 케이스들을 생각해보면 (레이블인코딩 한 번만 수행했고, 그게 split 이후라면) 이 코드는 유의미해진다!
        return maj_vote

    def predict_proba(self, X): #교차검증이기 때문에 X는 5개의 test 데이터(shape:5x2, cv=10이었기 때문에)
        print('222222222222222222222222222222')
        probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) #probas : 3x5x2 3차원 데이터, 각 estimator별로 predict_proba한 결과 확률값 5x2데이터가 3번 담기니까. 
        test=probas #3차원 데이터인 것 확인~
        avg_proba = np.average(probas, axis=0, weights=self.weights) #3차원이기 때문에 axis=0,1,2까지 가능. axis=0이면 3x5x2에서 3에 해당. 3개에서 같은 위치에 있는 애들끼리 평균 내고, 결괏값은 5x2 shape.
        '''
        이게 Soft Voting과 연결되는 이유!!! ★★★
        3개의 classifier들의 각각의 피쳐 데이터에 해당하는 확률을 레이블값별로 낸 평균을 구함. 
        여기서 최고인 레이블값으로 최종 class값 결정하기만 하면 Soft Voting!
        '''
        return avg_proba

    def get_params(self, deep=True):
        """GridSearch를 위해서 분류기의 매개변수 이름을 반환합니다"""
        if not deep:
            return super(MajorityVoteClassifier, self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(step.get_params(deep=True)):
                    out['%s__%s' % (name, key)] = value
            return out

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
X, y = iris.data[50:, [1, 2]], iris.target[50:] #여기까지 했을 때 y에는 1,1,1,1,1,..2,2,2,2,2...만 들어가있고 0은 안들어감.
le = LabelEncoder()     #y 안의 값은 모두 숫자인데 레이블인코딩 해주는 이유 : 나중에 bincount할 때 1부터 시작하면 애매하기 때문에 그냥 바꿔줌
                        #bincount([0,0,1,3,3,3]):[2 1 0 3], argmax(bincount([0,0,1,3,3,3])):3(최곳값 있는 인덱스)
y = le.fit_transform(y) #레이블인코딩 끝난 후 y에는 0,0,0,0,0,.1,1,1,1,1...로 바뀌어 들어가있음

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)
#test_size=0.5, stratify=y로 보면 train, test 데이터는 X row의 반이니까 50의 크기일 것이고, 그 비율은 둘 다 y의 비율을 따라 1:1일 것이다. 

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# estimator 객체 3개 만듦
clf1 = LogisticRegression(solver='liblinear',
                          penalty='l2', 
                          C=0.001,
                          random_state=1)

clf2 = DecisionTreeClassifier(max_depth=1,  #지금 일부러 max_depth를 1로 줘서 너무 얕게 만듦 -> 일부러 약한 분류기를 만들었구나~!
                              criterion='entropy', #get_param()으로 까보면 criterion default값 : gini    //gini, entropy, 불순물지수 전부 비슷한 목적이다. 같다고 생각.
                              random_state=0)

clf3 = KNeighborsClassifier(n_neighbors=1,
                            p=2,
                            metric='minkowski')

pipe1 = Pipeline([['sc', StandardScaler()], #그냥 clf1 이름이 pipe1로 바꼈구나~라고 생각
                  ['clf', clf1]])
pipe3 = Pipeline([['sc', StandardScaler()], #그냥 clf3 이름이 pipe3으로 바꼈구나~라고 생각
                  ['clf', clf3]])
'''
cf. 파이프라인
pipe1, pipe3 얘네는 나중에 estimator처럼 사용할 것이다. (fit, predict)
estimator는 fit(X_train, y_train)해줘야 하는데, Pipiline은 일단 스케일링 해주고 clf1으로 학습해준다. 
'''

clf_labels = ['Logistic regression', 'Decision tree', 'KNN']

print('10-겹 교차 검증:\n')
for clf, label in zip([pipe1, clf2, pipe3], clf_labels): #1st - pipe1,'Logistic regression', 2nd - clf2,'Decision tree', 
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') # cross_val_score() : 자동적으로 Stratified K폴드 교차 검증
                    #for문 돌면서 clf에 순차적으로 3개 estimator 전부 들어감    #scoring : 판단 기준
                                                                        
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]"
          % (scores.mean(), scores.std(), label))




# 다수결 투표 (클래스 레이블 카운트)

mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3]) #리스트 안에 estimator도 요소로 넣을 수 있다. classifiers라는 파라미터 설정으로 이것 넣어줌

#__init()__에서 뭐 담겨있는지 그냥 확인
print(mv_clf.named_classifiers)
print(mv_clf.vote)
print(mv_clf.weights)


clf_labels += ['Majority voting'] #뒤에 새로 이름 하나 추가
all_clf = [pipe1, clf2, pipe3, mv_clf] #마지막 요소 : 방금 MajorityVoteClassifier로 만든 estimator 객체

for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='accuracy') #얘가 accuracy로 바뀌면 predict_proba 말고 predict가 실행되어 11111111111111111111이 출력된다. 
    #지금 voting방식을 사용하는 estimator pipe1, clf2, pipe3, mv_clf를 train데이터로 교차 검증을 수행하고 있는 중!
    #50개의 데이터가 10조각으로 fold되어 45개, 5개로 나누어 교차검증
    
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    '''
    ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
    ROC AUC: 0.87 (+/- 0.18) [Decision tree]
    ROC AUC: 0.85 (+/- 0.13) [KNN]
    ROC AUC: 0.98 (+/- 0.05) [Majority voting]     - 마지막 mv_clf가 가장 좋긴 함. 근데 여기서 중요한 건 그게 아님.
    
    1. 우선 train_test_split
    2. fit()메소드에서 cv=10이므로 10개로 나눠서 1개씩 빼놓고 교차검증
    '''

■■■■■■■■■■■■■0514 수업中 새로 알게된것■■■■■■■■■■■■■

# ## 4.3 앙상블 학습 개요

# ### Voting Classifier

# **위스콘신 유방암 데이터 로드** : 종양의 데이터를 학습시켜서 이 종양이 악성인지 아닌지 판단하는 모델링

# In[1]:


import pandas as pd

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

cancer = load_breast_cancer() #sklearn에서 제공해주는 데이터 하나 가져옴 - 위스콘신 유방암 데이터 세트

data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
data_df.head(3)


# **VotingClassifier로 개별모델은 로지스틱 회귀와 KNN을 보팅방식으로 결합하고 성능 비교**

# In[ ]:


# 개별 모델은 로지스틱 회귀와 KNN 임. 
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

# 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 
vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' ) #아까 만든 두 estimator로 soft voting 할게~

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, 
                                                    test_size=0.2 , random_state= 156)

# VotingClassifier 학습/예측/평가. 
vo_clf.fit(X_train , y_train)
pred = vo_clf.predict(X_test)
print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred))) #voting의 퍼포먼스가 이 케이스에서 조금 더 괜찮구나~

# 개별 모델의 학습/예측/평가.
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train , y_train)
    pred = classifier.predict(X_test)
    class_name= classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred)))

=============================================================

# ## 4.4 Random Forest

# **결정 트리에서 사용한 사용자 행동 인지 데이터 세트 로딩**

# ### 수정 버전 01: 날짜 2019.10.27일
# 
# **원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생.**  
# **중복 feature명에 대해서 원본 feature 명에 '_1(또는2)'를 추가로 부여하는 함수인 get_new_feature_name_df() 생성**

# In[2]:


import pandas as pd

def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) 
                                                                                           if x[1] >0 else x[0] ,  axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df


# In[3]:


import pandas as pd

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('C:/jeon/human_activity/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    
    # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. 
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('C:/jeon/human_activity/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('C:/jeon/human_activity/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('C:/jeon/human_activity/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('C:/jeon/human_activity/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test


X_train, X_test, y_train, y_test = get_human_dataset()


# **학습/테스트 데이터로 분리하고 랜덤 포레스트로 학습/예측/평가**

# In[4]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환
X_train, X_test, y_train, y_test = get_human_dataset()

# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가
rf_clf = RandomForestClassifier(random_state=0)
rf_clf.fit(X_train , y_train)
pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test , pred)
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))


# **GridSearchCV 로 교차검증 및 하이퍼 파라미터 튜닝**

# In[5]:


from sklearn.model_selection import GridSearchCV

params = { # 1x4x3x3 = 36
    'n_estimators':[100],
    'max_depth' : [6, 8, 10, 12], 
    'min_samples_leaf' : [8, 12, 18 ],
    'min_samples_split' : [8, 16, 20]
}
# RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)
grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) #GridSearchCV cv=2
grid_cv.fit(X_train , y_train)

print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))


# **튜닝된 하이퍼 파라미터로 재 학습 및 예측/평가**

# In[7]:


rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0)
rf_clf1.fit(X_train , y_train)
pred = rf_clf1.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))


# **개별 feature들의 중요도 시각화**

# In[8]:


import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns  )
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]

plt.figure(figsize=(8,6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20 , y = ftr_top20.index)
plt.show()

# ## 4.5 GBM(Gradient Boosting Machine)

# In[9]:


from sklearn.ensemble import GradientBoostingClassifier
import time
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = get_human_dataset()

# GBM 수행 시간 측정을 위함. 시작 시간 설정.
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train , y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print("GBM 수행 시간: {0:.1f} 초 ".format(time.time() - start_time))

# In[ ]:
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators':[100, 500], #엄청 많아서 엄청 오래 돈다. 30분~1시간 정도?.. 걍 넘어가자 ㅎㅎ
    'learning_rate' : [ 0.05, 0.1]
}
grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)
grid_cv.fit(X_train , y_train)
print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))

# In[ ]:
scores_df = pd.DataFrame(grid_cv.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score',
'split0_test_score', 'split1_test_score']]


# In[22]:

# GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. 
gb_pred = grid_cv.best_estimator_.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print('GBM 정확도: {0:.4f}'.format(gb_accuracy))

■■■■■■■■■■■■■0517 수업中 새로 알게된것■■■■■■■■■■■■■

부스팅 Boosting의 종류
1. AdaBoost - 프린트물 나눠주심
	순서1. 가중치 벡터 w 동일하게, 모든 가중치의 합=1
	순서2. estimator 총 개수 m번 반복 중 j번째에서 다음을 수행
		a. 가중치가 부여된 약한 학습기 훈련 Cj = train(X_train, y_train, w)
		b. 예측값 y^ = predict(Cj, X_test)
		c. 가중치 적용된 에러율 계산 ε = w 내적 (y^≠y : 틀리면 1 맞으면 0)
		d. 학습기 가중치 계산 
		e. 가중치 업데이트 (예측 틀린 데이터 가중치 높여줘서 다음부터 샘플링 더 빈번하게 되도록)
		f. 업데이트된 가중치의 합이 다시 1이 되도록 정규화
	순서3. 최종 예측 계산

2. GBM : 경사하강법 + Boosting
	경사하강법♣♣♣	- Deep Learning의 핵심!
			- 2차함수인 비용함수(에러율이라 생각) 그래프에서 미분값(접선의 기울기)이 0이 되도록, 즉 에러율이 최솟값이 되도록...
	비용함수	- 얼마만큼의 오차가 났느냐! 어떻게 오차를 줄일 것인가!
	하이퍼 파라미터	- loss : 비용함수 뭐 사용할건지
			- learning_rate : step. 너무 좁으면 오래걸리고, 너무 넓으면 발산해버려서 왔다갔다 이상하게 움직임
			- n_estimators : estimator 개수. 너무 많으면 오래걸림
			- subsample : 전체 데이터 中 몇퍼센트만 샘플링 할건지 (Overfitting 예방)

3. XGBoost ♣♣♣
파이썬 래퍼 XGBoost
	import xgboost as xgb : sklearn기반이 아님. (최근에 나온 거라 안 들어있음)
	단점	1. fit() → predict() / predict_proba() 이 플로우를 따르지 않음. 사용 못함. 
		2. cross_val_score(), GridSearchCV, Pipeline 사용 못함. 
		3. 판다스의 dtype으로 바로 넘겨주지 못하는 경우 있음.
		4. variable explorer로 까보지 못하는 경우 있음. 
	=> 사이킷런 래퍼 XGBoost 개발 (래퍼 클래스 XGBClassifier, XGBRegressor)

사이킷런 래퍼 XGBoost
	sklearn과 호환 가능한 estimator 만듦, 파이썬 래퍼 XGBoost 단점 모두 해결
	eval_matric (학습 태스크 파라미터 中) : 검증에 사용되는 함수 정의, 회귀 default : rmse, 분류default : error
	
4. LightGBM ♣♣♣
	Early Stopping(조기중단)♣ : estimator 하나씩 지나가며 검증데이터로 검증하다가 eval_metric 검증함수값이 early stopping만큼 계속 나빠지면 거기서 조기 중단!
	ex_	n_estimators=200, early_stoppings=50
		10번째 estimator까지 퍼포먼스 좋았다가, 그때부터 50번만큼 계속 10번째보다 나쁘면(60번째estimator까지) 조기 중단!
	bagging_fraction : 학습할 때 데이터 일부만 샘플링해서 과적합 막기
	feature_fraction : 학습할 때 피처 몇 개만 선택해서 과적합 막기

	잔차 = y - y^ = y - f(x) = 실제값과 예측값의 차이
	estimator1 = f1(x)			: |잔차|의 합이 가장 작은 모델
	estimator2 = f2(x) = y - f1(x)		: 첫 번째에서 구한 모델과의 잔차2(예측값> f1(x))를 새 그래프에 찍고, |잔차2|의 합이 가장 작은 모델
	estimator3 = f3(x) = y - f1(x) - f2(x)	: 두 번째에서 구한 모델과의 잔차3(예측값> f1(x) - f2(x))를 새 그래프에 찍고, |잔차3|의 합이 가장 작은 모델
		:
	결국 GBM에서 estimator가 늘어날 수록 잔차가 최소화되는 모델 만들어짐
	4차원 이상(label이 4개 이상)인 데이터는 잔차를 내 머리로 상상해야 쉽다. 
	
	어느정도 피처의 column양이 커야 효과 좋아짐. 

■■■■■■■■■■■■■0518 수업中 새로 알게된것■■■■■■■■■■■■■

# * XGBoost 버전 확인

# In[1]:


import xgboost

print(xgboost.__version__)


# ### 파이썬 래퍼 XGBoost 적용 – 위스콘신 Breast Cancer 데이터 셋
# 
# ** 데이터 세트 로딩 **

# In[2]:


import xgboost as xgb
from xgboost import plot_importance

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()
X_features= dataset.data
y_label = dataset.target

cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_df['target']= y_label
cancer_df.head(3)


# In[3]:


print(dataset.target_names)
print(cancer_df['target'].value_counts())


# In[4]:


# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,
                                         test_size=0.2, random_state=156 )
print(X_train.shape , X_test.shape)


# ** 학습과 예측 데이터 세트를 DMatrix로 변환 **

# In[5]:

#여기부터 이제 생소해짐!
dtrain = xgb.DMatrix(data=X_train , label=y_train) #xgboost의 DMatrix() → variable explorer에서 dtrain, dtest 못까본다. 우선 type은 core.DMatrix이다. 
dtest = xgb.DMatrix(data=X_test , label=y_test) #X_test(피처값), y_test(레이블값(정답))이 합쳐졌기 때문에 검증데이터로 사용 가능하다. 


# ** 하이퍼 파라미터 설정 **

# In[6]:


params = { 'max_depth':3,
           'eta': 0.1,
           'objective':'binary:logistic',
           'eval_metric':'logloss', #검증에 사용되는 함수를 logloss로 줌. 매 step마다 logloss가 얼마나 발생했는지 리턴할 것
           'early_stoppings':100
        }
num_rounds = 400 #부스팅 반복횟수(estimator) 400개..!


# ** 주어진 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달하고 학습 **

# In[7]:


# train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. 
wlist = [(dtrain,'train'),(dtest,'eval') ] #검증데이터 1개, 2개, 3개, ... 개수는 상관 없음. 

# 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달
xgb_model = xgb.train(params = params , dtrain=dtrain , num_boost_round=num_rounds , evals=wlist ) #sklearn의 fit()처럼 학습시켜서 모델 던져줌
#원래는 검증데이터, 테스트데이터, 트레인데이터 따로따로 나눠 하는 게 맞는데, 여기선 그냥 검증데이터와 테스트데이터를 같은 느낌으로 넣어주었다고 한다. 
#요기선 early_stopping_rounds = early_stoppings 안해줬넹~

'''
evals : 검증데이터. dtrain, dtest 2개를 넣어주었다. 
사실 이렇게 하면 안됨. train할 때 학습 데이터로 dtrain을 넣어주었는데, 검증을 똑같이 dtrain으로 해줘버리면 안 됨. 
지금 출력된 결과도 train-logloss는 급격히 줄어들어 좋은 결과가 나오지만, eval-logloss는 그것보다는 좋지 않은 결과가 나옴. 
두 결과 중 dtest(eval-logloss)만을 봐야 함. 
'''


# ** predict()를 통해 예측 확률값을 반환하고 예측 값으로 변환 **

# In[8]:


pred_probs = xgb_model.predict(dtest)
print('predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')
print(np.round(pred_probs[:10],3))

# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 
preds = [ 1 if x > 0.5 else 0 for x in pred_probs ]
print('예측값 10개만 표시:',preds[:10])


# ** get_clf_eval( )을 통해 예측 평가 **

# In[9]:


from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

# 수정된 get_clf_eval() 함수 
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))


# In[10]:


get_clf_eval(y_test , preds, pred_probs)


# ** Feature Importance 시각화 **

# In[ ]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(xgb_model, ax=ax) #plot_importance : 얘도 당연히 sklearn이 아니라 xgboost에서 가져옴. 트리 만들 때 가장 많이 기여한 순서
                #모델정보, 그래프정보


# ### 사이킷런 Wrapper XGBoost 개요 및 적용 
# 
# ** 사이킷런 래퍼 클래스 임포트, 학습 및 예측 **

# In[11]:


# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트
from xgboost import XGBClassifier

evals = [(X_test, y_test)]

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) #learning_rate : step
xgb_wrapper.fit(X_train , y_train,  early_stopping_rounds=400,eval_set=evals, eval_metric="logloss",  verbose=True)
                #X_train , y_train은 당연히 pandas DataFrame이군! 여기선 다 쓸수 있다~
                #근데 어차피 n_estimators=400인데 여기서 early_stopping_rounds=400이라 하면 아무 의미 없다. 
                #eval_set : 검증데이터, verbose : True-트래킹메시지(loss 출력메시지) 띄워줌, False-출력아무것도없이 실행만됨

w_preds = xgb_wrapper.predict(X_test)
w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]

# In[12]:


get_clf_eval(y_test , w_preds, w_pred_proba)


# ** early stopping을 100으로 설정하고 재 학습/예측/평가 **

# In[13]:


from xgboost import XGBClassifier

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)

evals = [(X_test, y_test)]
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", 
                eval_set=evals, verbose=True)

ws100_preds = xgb_wrapper.predict(X_test)
ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]


# In[14]:


get_clf_eval(y_test , ws100_preds, ws100_pred_proba)


# ** early stopping을 10으로 설정하고 재 학습/예측/평가 **

# In[15]:


# early_stopping_rounds를 10으로 설정하고 재 학습. 
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, #early_stopping_rounds를 너무 작게 줌. 충분히 개선될 여지가 있는데 그러지 못함. 
                eval_metric="logloss", eval_set=evals,verbose=True)

ws10_preds = xgb_wrapper.predict(X_test)
ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
get_clf_eval(y_test , ws10_preds, ws10_pred_proba)


# In[16]:


from xgboost import plot_importance
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

fig, ax = plt.subplots(figsize=(10, 12))
# 사이킷런 래퍼 클래스를 입력해도 무방. 
plot_importance(xgb_wrapper, ax=ax)

=========================================================

# * LightGBM 버전 확인
import lightgbm

print(lightgbm.__version__)


# ### LightGBM 적용 – 위스콘신 Breast Cancer Prediction

# In[3]:


# LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트
from lightgbm import LGBMClassifier

import pandas as pd
import numpy as np

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

dataset = load_breast_cancer()
ftr = dataset.data
target = dataset.target


# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 )

# 앞서 XGBoost와 동일하게 n_estimators는 400 설정. 
lgbm_wrapper = LGBMClassifier(n_estimators=400)

# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. 
evals = [(X_test, y_test)]
lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", 
                 eval_set=evals, verbose=True)

preds = lgbm_wrapper.predict(X_test)
pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]

# In[5]:

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score

# 수정된 get_clf_eval() 함수 
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

# In[6]:

get_clf_eval(y_test, preds, pred_proba)

# In[7]:

from lightgbm import plot_importance
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

fig, ax = plt.subplots(figsize=(10, 12))
# 사이킷런 래퍼 클래스를 입력해도 무방. 
plot_importance(lgbm_wrapper, ax=ax)

print(dataset.feature_names)

■■■■■■■■■■■■■0520 수업中 새로 알게된것■■■■■■■■■■■■■

############### Kaggle_분류실습_산탄데르 은행 고객 만족 예측 ################

# ### 데이터 전처리

# In[1]:


import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import matplotlib

cust_df = pd.read_csv("C:/jeon/santander_train.csv",encoding='latin-1')
print('dataset shape:', cust_df.shape)
cust_df.head(3)


# In[2]:


print(cust_df.info())


# In[3]:


print(cust_df['TARGET'].value_counts())

unsatisfied_cnt = cust_df[cust_df['TARGET'] == 1]['TARGET'].count() # 1-만족, 0-불만족
total_cnt = cust_df['TARGET'].count() #null이 아닌 값 전부 세주는데, 여기선 사실상 아까 null 없는거 확인했으니까 전체 size와 같음

print('unsatisfied 비율은 {0:.2f}'.format((unsatisfied_cnt / total_cnt)))


# In[4]:


print(cust_df.describe( ))
'''     var3
min     -999999     엥?? -> 갑자기 뜬금없이 0이나 -1, 또는 이런 숫자가 나오면 '누락된 데이터'일 확률!. 전처리 해주는 게 좋음
25%     2           대부분의 데이터가 2이구나~    
50%     2
75%     2
max     238         계속 2였다가 마지막에 갑자기 팍 올라가는 그래프이구나~
'''

# In[5]:


print(cust_df['var3'].value_counts( )[:10])


# In[6]:


# var3 피처 값 대체 및 ID 피처 드롭
cust_df['var3'].replace(-999999, 2, inplace=True) #여기선 그냥 2라고 치환하는 전처리 해줌. inplace=True : 원본데이터 자체를 수정해주세요. (default:False)
cust_df.drop('ID',axis=1 , inplace=True)

# 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.iloc[:, -1]
print('피처 데이터 shape:{0}'.format(X_features.shape))


# In[9]:


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0) # stratify=y_labels 하면 target값 0의 비율 둘다 아예 똑같이 0.04정도로 나옴. 
train_cnt = y_train.count()
test_cnt = y_test.count()
print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape))

print(' 학습 세트 레이블 값 분포 비율')
print(y_train.value_counts()/train_cnt)
print('\n 테스트 세트 레이블 값 분포 비율')
print(y_test.value_counts()/test_cnt)


# In[10]:


from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

# n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정. 
xgb_clf = XGBClassifier(n_estimators=500, random_state=156) #XGBClassifier 사용해서 sklearn과 연동

# 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행. 
xgb_clf.fit(X_train, y_train, early_stopping_rounds=100,
            eval_metric="auc", eval_set=[(X_train, y_train), (X_test, y_test)]) # eval_metric의 검증 평가지표(얘가 달라지면 언제 중단될지도 달라짐)로 eval_set의 검증 데이터를 검증하겠다.
                                #검증데이터는 꼭 train, test 모두 필요. 검증데이터 2개니까 조기중단도 둘 모두의 변화를 함께 판단해서 멈춘다. 

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))

# In[11]:


from sklearn.model_selection import GridSearchCV

# 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소
xgb_clf = XGBClassifier(n_estimators=100)

params = {'max_depth':[5, 7] , 'min_child_weight':[1,3] ,'colsample_bytree':[0.5, 0.75] }

# 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않음. 
gridcv = GridSearchCV(xgb_clf, param_grid=params)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc",
           eval_set=[(X_train, y_train), (X_test, y_test)])

print('GridSearchCV 최적 파라미터:',gridcv.best_params_) 

xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))


# In[12]:


# n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함. 
xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=5,                        min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03)

# evaluation metric을 auc로, early stopping은 200 으로 설정하고 학습 수행. 
xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, 
            eval_metric="auc",eval_set=[(X_train, y_train), (X_test, y_test)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))


# In[13]:


# n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함. 
xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7,                        min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03)

# evaluation metric을 auc로, early stopping은 200 으로 설정하고 학습 수행. 
xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, 
            eval_metric="auc",eval_set=[(X_train, y_train), (X_test, y_test)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))


# In[14]:


from xgboost import plot_importance
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

fig, ax = plt.subplots(1,1,figsize=(10,8))
plot_importance(xgb_clf, ax=ax , max_num_features=20,height=0.4)


# ### LightGBM 모델 학습과 하이퍼 파라미터 튜닝

# In[15]:


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=500)

evals = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals,
                verbose=True)

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))


# In[16]:


from sklearn.model_selection import GridSearchCV

# 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소
LGBM_clf = LGBMClassifier(n_estimators=200)

params = {'num_leaves': [32, 64 ],
          'max_depth':[128, 160],
          'min_child_samples':[60, 100],
          'subsample':[0.8, 1]}


# 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않습니다. 
gridcv = GridSearchCV(lgbm_clf, param_grid=params)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc",
           eval_set=[(X_train, y_train), (X_test, y_test)])

print('GridSearchCV 최적 파라미터:', gridcv.best_params_)
lgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))


# In[19]:


lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100,
                          max_depth=128)

evals = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals,
                verbose=True)

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))

■■■■■■■■■■■■■0521 수업中 새로 알게된것■■■■■■■■■■■■■

############### Kaggle_분류실습_캐글 신용카드 사기 검출 ################

#!/usr/bin/env python
# coding: utf-8

# ### 데이터 일차 가공 및 모델 학습/예측/평가
# 
# ** 데이터 로드 **

# In[1]:


import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
get_ipython().run_line_magic('matplotlib', 'inline')

card_df = pd.read_csv('C:/jeon/creditcard.csv')
card_df.head(3)

print(card_df.info()) #null값 없음. 'Class' column만 int, 나머지는 float
test = card_df.describe() #'Class' column 보니까 값이 0과 1 뿐일 확률이 있구나!
print(card_df['Class'].value_counts()) #그게 맞다! +극단적으로 1에 해당하는 데이터가 적구나(사기case니까 당연)

# In[2]:


print(card_df.shape)


# ** 원본 DataFrame은 유지하고 데이터 가공을 위한 DataFrame을 복사하여 반환 **

# In[3]:


from sklearn.model_selection import train_test_split

# 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환
def get_preprocessed_df(df=None):
    df_copy = df.copy() #복사본
    df_copy.drop('Time', axis=1, inplace=True) #정확한 어떤 정보인지는 고객정보니까 알려주지 않았지만, 그냥 계속 증가하는데 별 의미 없어 보임. 그래서 걍 삭제.
    return df_copy


# ** 학습과 테스트 데이터 세트를 반환하는 함수 생성. 사전 데이터 처리가 끝난 뒤 해당 함수 호출  **

# In[4]:


# 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수.
def get_train_test_dataset(df=None):
    # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환
    df_copy = get_preprocessed_df(df)
    
    # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들
    X_features = df_copy.iloc[:, :-1]
    y_target = df_copy.iloc[:, -1]
    
    # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할
    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)
    
    # 학습과 테스트 데이터 세트 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)


# In[5]:


print('학습 데이터 레이블 값 비율')
print(y_train.value_counts()/y_train.shape[0] * 100)
print('테스트 데이터 레이블 값 비율')
print(y_test.value_counts()/y_test.shape[0] * 100)


# In[6]:


from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score

# 수정된 get_clf_eval() 함수 
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    # ROC-AUC 추가 
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    '''
    [[85282    13]
     [   56    92]]
    정확도: 0.9992, 정밀도: 0.8762, 재현율: 0.6216, F1: 0.7273, AUC:0.9582
    카드사기이기 때문에 재현율이 낮으면 안됨. FN(사기가 Positive인데 Negative라고 잘못 예측하면 x!!)
    '''
    # ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))


# In[7]:


from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression()

lr_clf.fit(X_train, y_train)

lr_pred = lr_clf.predict(X_test)
lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]

# 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행. 
get_clf_eval(y_test, lr_pred, lr_pred_proba)


# ** 앞으로 피처 엔지니어링을 수행할 때마다 모델을 학습/예측/평가하므로 이를 위한 함수 생성 ** 

# In[8]:


# 인자로 사이킷런의 Estimator객체와, 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행.
def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    pred_proba = model.predict_proba(ftr_test)[:, 1]
    get_clf_eval(tgt_test, pred, pred_proba)
    '''
    아래에서 LightGbm 써서 오차행렬 보여주니까
    [[85290     5]
     [   36   112]]
    정확도: 0.9995, 정밀도: 0.9573, 재현율: 0.7568, F1: 0.8453, AUC:0.9790
    재현율 확 올라감. 게다가 정밀도까지 올라감. 
    '''


# ** LightGBM 학습/예측/평가.**
# 
# (boost_from_average가 True일 경우 레이블 값이 극도로 불균형 분포를 이루는 경우 재현률 및 ROC-AUC 성능이 매우 저하됨.)  
#    LightGBM 2.1.0 이상 버전에서 이와 같은 현상 발생 

# In[9]:


from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) #n_jobs=-1: estimator 돌릴 때 모든 cpu 사용해서 한번에 돌려주세요~ 컴퓨터는 느려지지만 속도 빨라짐!
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)


# ### 중요 데이터 분포도 변환 후 모델 학습/예측/평가
# 

# ** 중요 feature의 분포도 확인 **

# In[10]:


import seaborn as sns

plt.figure(figsize=(8, 4))
plt.xticks(range(0, 30000, 1000), rotation=60) #rotation=60 : x축 좌표 표기들을 60도 각도 틀어서 써줌
sns.distplot(card_df['Amount']) #Amount : 신용카드 결제 금액. 결제 금액 별 금융사기가 발생한 히스토그램(distplot:몇번 발생했는지 도수분포)
    #kde=True로 default. --> 히스토그램의 밀도 추세 곡선(연속된 값으로) 그려줌. 얘는 금액이기 때문에 이산적인 값이 아니라 연속된 값으로 봐야 함. 
    #이산적인 값 : 확률질량함수
    #연속된 값 : 적분!(dx : x를 굉장히 작게 자른 한 부분) -> 확률밀도함수
# ** 데이터 사전 가공을 위한 별도의 함수에 StandardScaler를 이용하여 Amount 피처 변환 **

# In[11]:


from sklearn.preprocessing import StandardScaler

# 사이킷런의 StandardScaler를 이용하여 정규분포 형태로 Amount 피처값 변환하는 로직으로 수정. 
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler = StandardScaler()
    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1)) #fit_transform() 안에 가로로 된 series(1차원 데이터)가 아닌 세로로 된 2차원 데이터가 들어가주어야 해서 reshape해줌. 
    
    # 변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n) #cf. append는 맨 뒤에만 갖다 붙일 수 있다. 
    
    # 기존 Time, Amount 피처 삭제
    df_copy.drop(['Time','Amount'], axis=1, inplace=True) #Time은 애초에 의미 없었고, 기존의 Amount도 필요 없어졌으므로 drop
    return df_copy


# ** StandardScaler 변환 후 로지스틱 회귀 및 LightGBM 학습/예측/평가 **

# In[12]:


# Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행. 
X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능 ###')
lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능 ###')
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)


# ** Amount를 로그 변환 **

# In[13]:


def get_preprocessed_df(df=None):
    df_copy = df.copy()
    # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환 
    amount_n = np.log1p(df_copy['Amount'])
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    return df_copy


# In[14]:


# log1p 와 expm1 설명 
'''
Log Scale
: 데이터 차이가 극단적으로 차이 많이 나는 경우 log 취해주면 한결 보기 편해짐
ex_         x = 1,000,000   10,000      100     10  →  그래프 그리면 1,000,000을 제외하고 모두 다닥다닥 바닥에 붙어버림
       log(x) =     6           4       2       1
log1p
: log(1+x) 사용. log(1)==0이므로 log scale 결괏값이 음수가 나오지 않게 하기 위해 1+x를 넣어줌. 
'''
import numpy as np

print(1e-1000 == 0.0)

print(np.log(1e-1000))

print(np.log(1e-1000 + 1))
print(np.log1p(1e-1000))


# In[15]:


var_1 = np.log1p(100)
var_2 = np.expm1(var_1)
print(var_1, var_2)


# In[16]:


X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능 ###')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능 ###')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)


# ### 이상치 데이터 제거 후 모델 학습/예측/평가

# ** 각 피처들의 상관 관계를 시각화. 결정 레이블인 class 값과 가장 상관도가 높은 피처 추출 **

# In[21]:


import seaborn as sns

plt.figure(figsize=(9, 9))
corr = card_df.corr() 
'''
corr() : 상관계수
column1이 1-2-3-4-5-6 증가하는데 column2도 2-4-6-8-10-12 같이 증가하는 경향성을 보인다면 '양의 상관관계가 있으며 상관계수는 1이다'

완전 똑같이 움직이면                     corr() = 1
완전 반대로 움직이면                     corr() = -1   (어쨌든 얘도 상관관계가 ↑높은↑ 것!!)
방향은 같은데 정도가 다르면 소수점.        corr() = 0.~~
하나는 바뀌는데 하나는 값의 변화가 없다면   corr() = 0
'''
sns.heatmap(corr, cmap='RdBu') #
'''
시각화. 각 피처별로 모든 상관도 관계성을 보여줌. 
RdBu : Red-down Blue-up. 자신과 자신의 corr()값은 1이므로 가장 진한 파란색으로 표현. 

'Class'와 어떤 다른 column이 양의 상관관계/음의 상관관계가 있는 것이 있나?
=> V14, V17이 그나마 음의 상관관계가 진하다!
=> 그럼 얘네한테서 이상치(Outlier) 제거해주는 게 의미가 있겠구나!
'''


# ** Dataframe에서 outlier에 해당하는 데이터를 필터링하기 위한 함수 생성. outlier 레코드의 index를 반환함. **

# In[30]:


import numpy as np

def get_outlier(df=None, column=None, weight=1.5): #이상치(Outlier) 인덱스 리턴 함수
    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. 
    fraud = df[df['Class']==1][column]
    quantile_25 = np.percentile(fraud.values, 25)
    quantile_75 = np.percentile(fraud.values, 75)
    
    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. 
    iqr = quantile_75 - quantile_25
    iqr_weight = iqr * weight
    lowest_val = quantile_25 - iqr_weight
    highest_val = quantile_75 + iqr_weight
    
    # 최대값 보다 크거나, 최소값 보다 작은 값을 boolean index로 접근해서 아웃라이어로 설정하고 걔네의 DataFrame index만을 반환. 
    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index
    
    return outlier_index
'''
[시각화 - 박스 플롯]
이상치     : 최댓값 이상
최댓값     : 3/4 + 1.5*IQR
IQR       : 3/4, Q3(75%)            //IQR = Q1+Q2+Q3. 사분위(Quantile)값의 편차
            2/4, Q2(50%)
            1/4, Q1(25%)
최솟값     : 1/4 - 1.5*IQR
이상치     : 최솟값 이하
'''


# In[28]:


#np.percentile(card_df['V14'].values, 100)
np.max(card_df['V14'])


# In[31]:


outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)
print('이상치 데이터 인덱스:', outlier_index)


# **로그 변환 후 V14 피처의 이상치 데이터를 삭제한 뒤 모델들을 재 학습/예측/평가**

# In[32]:


# get_processed_df( )를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경. 
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    df_copy.insert(0, 'Amount_Scaled', amount_n) #아까랑 똑같이 Amount_Scaled
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    
    # 이상치 데이터 삭제하는 로직 추가
    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)
    df_copy.drop(outlier_index, axis=0, inplace=True) #axis=0 : 로우방향. 로우를 날려줌. 
    return df_copy

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능 ###')
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능 ###')
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
'''
[[85290     5]
 [   25   121]]
정확도: 0.9996, 정밀도: 0.9603, 재현율: 0.8288, F1: 0.8897, AUC:0.9780
=> 재현율: 아까 최대로 올린 0.7635에서 0.8288까지 확 올라옴! FN이 확 줄었구나. 

근데 outlier 삭제해서 더 안 좋아지는 경우도 있음. 모든 게 다 케바케. 
'''

# ### SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가

# In[33]:


from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train)
print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())


# In[34]:


print(y_train.value_counts())


# ** 로지스틱 회귀로 학습/예측/평가 **

# In[35]:


lr_clf = LogisticRegression()
# ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의
get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)
'''
[[82937  2358]
 [   11   135]]
정확도: 0.9723, 정밀도: 0.0542, 재현율: 0.9247, F1: 0.1023, AUC:0.9737
=> 오버샘플링으로 재현율을 좋아졌지만, 정밀도가 5%로 크게 줄어들음. 원래도 재현율, 정밀도가 trade-off 관계였지만 이번엔 좀 심함
'''

# ** Precision-Recall 곡선 시각화 **

# In[36]:


import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.metrics import precision_recall_curve
get_ipython().run_line_magic('matplotlib', 'inline')

def precision_recall_curve_plot(y_test , pred_proba_c1):
    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. 
    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)
    
    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시
    plt.figure(figsize=(8,6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')
    
    # threshold 값 X 축의 Scale을 0.1 단위로 변경
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1),2))
    
    # x축, y축 label과 legend, 그리고 grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend(); plt.grid()
    plt.show()
    


# In[37]:


precision_recall_curve_plot( y_test, lr_clf.predict_proba(X_test)[:, 1] )
#지금 왜 정밀도가 그렇게 낮은지. 로지스틱 회귀 모델에 어떤 문제가 발생하고 있는지 확인하기 위해 재현율, 정밀도 그래프 그리기.


# ** LightGBM 모델 적용!!!! **

# In[38]:


lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test,
                  tgt_train=y_train_over, tgt_test=y_test)
'''
[[85283    12]
 [   22   124]]
정확도: 0.9996, 정밀도: 0.9118, 재현율: 0.8493, F1: 0.8794, AUC:0.9814
=> LightGBM 모델로  SMOTE로 오버샘플링된 데이터를 학습/예측/평가하니까
    정밀도는 사알짝 줄었지만 재현율도 엄청 오름. 정밀도, 재현율이 우수함!!
=> 물론 신용카드 사기 검출 모델은 재현율이 훨씬 중요하지만, 아무리 그래도 아까 정밀도가 5%인 것은 좀 아니었다^^
    카드 긁을 때마다 매번 "본인이 결제하시는 것 맞으세요?" 확인전화 오면 안 됨. 실생활 적용 불가능. 
'''

■■■■■■■■■■■■■0524 수업中 새로 알게된것■■■■■■■■■■■■■

스태킹 앙상블 Stacking Ensemble

- cv 이용 → 교차검증 기반의 스태킹 앙상블
	1. 우선 얘도 앙상블 기법의 하나니까 estimator 여러개 쓰임
	2. estimator 각각 교차검증
	3. train, test데이터로 나누고, train데이터는 cv만큼 잘라 1개를 검증데이터, 나머지를 train데이터로 사용
	4. 학습된 estimator는 메타 모델이 됨
	5. 교차검증 1번 끝날 때마다(cv로 나눈 검증데이터가 바뀔 때마다) "모든 검증데이터에 대한 예측 레이블값" + "test데이터에 대한 예측 레이블값" 얻음
	6. cv만큼 나온 "모든 검증데이터에 대한 예측 레이블값"은 하나의 column으로 완성, "test데이터에 대한 예측 레이블값"은 cv크기만큼 나옴. 
	7. "test데이터에 대한 예측 레이블값"은 row끼리 평균 내서 하나의 column으로 만듦
	8. 그걸 총 estimator 개수만큼 해서 두 개씩 나온 column들을 concatinate로 column방향으로 합침 => estimator개수:column개수
				each_estimator_train, each_estimator_test  ───(concatinate)──→  Stack_final_X_train, Stack_final_X_test
	9. 최종적으로 	메타모델.fit(Stack_final_X_train, y_train)
			stack_final = 메타모델.predict(Stack_final_X_test)
			accuracy_score(y_test, stack_final)

==========================================================

# ** 교차검증 없는 Basic 스태킹 모델 **
# 
# 데이터 로딩

# In[9]:


import numpy as np

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

cancer_data = load_breast_cancer()

X_data = cancer_data.data
y_label = cancer_data.target

X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)


# ** 개별 Classifier와  최종 Stacking 데이터를 학습할 메타 Classifier 생성 **

# In[10]:


# 개별 ML 모델을 위한 Classifier 생성.
knn_clf  = KNeighborsClassifier(n_neighbors=4)
rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)
dt_clf = DecisionTreeClassifier()
ada_clf = AdaBoostClassifier(n_estimators=100)

# 최종 Stacking 모델을 위한 Classifier생성. 
lr_final = LogisticRegression(C=10)


# ** 개별 Classifier 학습/예측/평가 **

# In[11]:


# 개별 모델들을 학습. 
knn_clf.fit(X_train, y_train)
rf_clf.fit(X_train , y_train)
dt_clf.fit(X_train , y_train)
ada_clf.fit(X_train, y_train)


# In[12]:


# 학습된 개별 모델들이 각자 반환하는 예측 데이터 셋을 생성하고 개별 모델의 정확도 측정. 
knn_pred = knn_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)
dt_pred = dt_clf.predict(X_test)
ada_pred = ada_clf.predict(X_test)

print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))
print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))
print('에이다부스트 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))


# ** 개별 모델의 예측 결과를 메타 모델이 학습할 수 있도록 스태킹 형태로 재 생성 ** 

# In[13]:


pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])
print(pred.shape)   #X_test 크기가 (114, 30)이었으므로, 각각의 예측 결괏값들은 크기 (114,)인 series. 걔네 4개가 묶임. 
                    #그냥 각 예측 결괏값들이 (1, 114)인 2차원 데이터였다면 이 묶는 행위는 concatinate(덩어리 땅! 갖다붙임)에 가까움. 
                    #concatinate : 덩어리 땅! 옆으로든 아래로든 갖다 붙임. stack은 merge보다 concatinate에 가까움. 
                    #merge : 어떤 key값을 기준으로 합집합 / 교집합

# transpose(전치행렬)를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. 
pred = np.transpose(pred)
print(pred.shape)


# ** 메타 모델 학습/예측/평가 **

# In[14]:


lr_final.fit(pred, y_test) #stacking을 하기 위해 4개의 분류기를 위에서 학습/예측했고, 그 결괏값들을 합친 데이터와 실제 결괏값을 X_train, y_train자리에 넣어 새로 학습.
final = lr_final.predict(pred) 
#근데 여기서 pred로 학습시킨 estimator를 또 다시 pred로 예측하면 당연히 퍼포먼스가 좋은 게 아닌가? ㅇㅇ 안됨!
#이 과적합을 개선하기 위해서 CV 셋 기반의 Stacking으로 해주는 거임~~~~~! 아래에서 살펴보자!

print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test , final)))


# ### CV 셋 기반의 Stacking

# In[16]:


from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. 
def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):
    # X_train_n (455, 30)   y_train_n (455,)   X_test_n (114, 30)   X_test_n (114,)
    
    # 지정된 n_folds값으로 n_folds개의 폴드 세트로 분리하는 KFold 객체 생성.
    kf = KFold(n_splits=n_folds, shuffle=False)
    
    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 
    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))
    test_pred = np.zeros((X_test_n.shape[0],n_folds))
    print(model.__class__.__name__ , ' model 시작 ')
    
    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): 
        # folder_counter : enumerate해줬으니까 loop의 idx가 들어감. 0~6
        # train_index, valid_index : kfold.split(X)는 폴드 세트를 n_folds번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환됨.
        
        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 
        print('\t 폴드 세트: ',folder_counter,' 시작 ')
        X_tr = X_train_n[train_index] 
        y_tr = y_train_n[train_index] 
        X_te = X_train_n[valid_index]  
        
        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.
        model.fit(X_tr , y_tr)
        
        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.
        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #model.predict(X_te)만 하면 1차원데이터 (65,) 즉 옆으로 된 series이므로 세로로 된 2차원 데이터로 만들어줌. 
                                                                            #valid_index는 순서대로 0~64, 65~129, 130~ ...
        
        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. 
        #매 교차 검증마다 X_test_n데이터를 집어넣음. 여기선 총 7번. 이거는 그 estimator의 교차검증 끝나면 row끼리(column방향으로) 평균 내서 결괏값 만들어낼 것임. 
        test_pred[:, folder_counter] = model.predict(X_test_n)
            
    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 
    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)     #얘도 np.mean(test_pred, axis=1)만 하면 1차원데이터 (65,) 즉 옆으로 된 series이므로 세로로 된 2차원 데이터로 만들어줌. 
    
    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터
    return train_fold_pred , test_pred_mean


# In[17]:

#4개의 estimator로 만들어진 4개의 train_fold_pred , test_pred_mean를 받음
knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)
rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)
dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test,  7)    
ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)


# In[18]:

#스태킹(Stacking) : 그 4개를 concatenate(모양 그대로 column방향:axis=1 / row방향:axis=0 으로 갖다 붙임)
Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)    #새로 만들어진 Stack_final_X_train는 어쨌든 train 데이터 기반으로 만들어졌다. 
Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)         #새로 만들어진 Stack_final_X_test는 어쨌든 test 데이터 기반으로 만들어졌다. 
print('원본 학습 피처 데이터 Shape:',X_train.shape, '원본 테스트 피처 Shape:',X_test.shape)
print('스태킹 학습 피처 데이터 Shape:', Stack_final_X_train.shape,
      '스태킹 테스트 피처 데이터 Shape:',Stack_final_X_test.shape)


# In[19]:


lr_final.fit(Stack_final_X_train, y_train) #Stack_final_X_train를 X_train 자리에, 그리고 label은 원본의 y_train으로!
stack_final = lr_final.predict(Stack_final_X_test) #Stack_final_X_test를 X_test 자리에. 
'''
새로 만들어진 Stack_final_X_train는 어쨌든 train 데이터 기반으로 만들어졌다. 
새로 만들어진 Stack_final_X_test는 어쨌든 test 데이터 기반으로 만들어졌다. 

=> 이제는 train데이터와 test데이터가 완벽히 분리되었다! ㅎㅎㅎ
'''

print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))

■■■■■■■■■■■■■0525 수업中 새로 알게된것■■■■■■■■■■■■■

[편미분]
z = 4x² + 3xy	(x, y : 종속변수, z : 독립변수)
dz/dx = 8x + 3y
 → z그래프를 x를 기준으로 편미분. y는 무의미해져서 상수 취급한다. 
 → 기하학적으로 보면 3차원 그래프 z를 xz평면에 투영하고, 그 그래프의 기울기를 얻기 위해 미분하는 것이다. 
dz/dy = 3x
 → z그래프를 y를 기준으로 편미분. x는 무의미해져서 상수 취급한다. 
 → 기하학적으로 보면 3차원 그래프 z를 yz평면에 투영하고, 그 그래프의 기울기를 얻기 위해 미분하는 것이다. 

dy/dx = dy/dt * dt/dx

회귀
- 분류-예측값이 이산적, 회귀-예측값이 연속적
- 최적의 가중치 찾기
- 여러 독립변수와 하나의 종속변수 간의 상관관계를 모델링
	종속변수 Y
	독립변수 X1, X2, X3, ..., Xn
	회귀계수 W1, W2, W3, ..., Wn 	=>최적의 회귀계수(ex_가중치) 찾기
- feature 데이터, label 데이터 간에 상관관계가 있으면 very good! )))
  feature 데이터, feature 데이터 간에 상관관계가 있으면 bad! (((
	1. 독립변수와 종속변수 경계가 흐릿해지면서 가중치 계산이 힘듦 ㅜㅜ!
		→ 상관관계 多 해결 	>>> 독립적인 중요한 피처만 남기고 제거 / 규제 적용
	2. 분산이 매우 커져서 오류에 민감해지는 다중 공선성 문제 ㅜㅜ!
		→ 다중 공선성 해결 	>>> PCA로 차원 축소
- 독립변수      1개 > 단일회귀	여러개 > 다중회귀
  회귀계수      선형 > 선형회귀(多)	비선형 > 비선형회귀

다항 회귀 Polynomial Regression
- 다항 회귀도 선형 회귀이다. 선형/비선형은 회귀 계수의 선형/비선형 여부이지, 독립변수의 선형/비선형 여부와는 무관하다. 
	z = y^ = w0 + w1*x1 + w2*x1*x2 + w3*x2² 에서 새로운 변수 Z를 z=[x1, x1*x2, x2²]로 두면
	z = y^ = w0 + w1*z1 + w2*z2 + w3*z3 으로 표현되니까 여전히 선형 회귀!
	여기서 z는 활성화 함수의 input값! Φ(z)처럼 들어감... z = y^ = wTㆍx
- sklearn.preprocessing import PolynomialFeatures : fit(), transform()으로 피처를 다항식 피처로 변환해주는 클래스
	단항값 [x1, x2] 를 2차 다항값으로 변환하면 [1, x1, x2, x1², x1*x2, x2²]
	단항값 [x1, x2, x3]을 2차 다항값으로 변환하면 [1, x1, x2, x3, x1*x2, x2*x3, x1*x3, x1², x2², x3²,]

- 규제 : 선형회귀에서 과적합 해결하기 위해 회귀계수에 페널티 적용
	규제o	L1규제 (라쏘 Lasso)
		L2규제 (릿지 Ridge)
		L1L2규제 (엘라스틱넷 ElasticNet)
	규제x	일반 선형 회귀 : RSS(==잔차²==제곱오차합)를 최소화할 수 있도록 회귀계수 최적화
	규제o/x	로지스틱 회귀 : 이름은 회귀지만 사실은 강력한 분류)

분류 평가 지표 : 높을수록 Good
- 정확도
- 정밀도
- 재현율
- F1 스코어
- ROC-AUC

회귀 평가 지표 : 낮을수록 Good
- MAE	절댓값 (absolute)
- MSE	제곱 (square)
- RMSE	√MSE : 다시 루트
- R²	분산, 설명력 : 우리가 만든 모델이 실제 데이터를 얼마나 잘 설명하느냐. 
		R² = (예측값의 분산) / (실제값의 분산)
		1에 가까울수록 정확도 높지만, 또 너무 똑같으면 overfitting일 확률↑

뇌의 신경세포 "뉴런" 응용해서 인공신경망 만들어짐
1. 다양한 input값 → 2. 가중치, 단위계단함수 → 3. output값
	1. 다양 : x0, x1, x2, ... => vector x = [x0 x1 x2 ...] : 공간낭비 x
	2-1. 가중치도 똑같이 vector w = [w0 w1 w2 ...]
	2-2. 단위계단함수 Φ(z) = z≥0일때 1, 아니면 -1 (z = wTㆍx)
		- wT로 전치행렬 해줘야 (1x3)ㆍ(3x1)에서 3, 3이 같아지면서 내적 가능해진다. wㆍx는 (3x1)ㆍ(3x1)로 내적 불가능. 
		- 참고로 (1x3)ㆍ(3x1)의 결과는 (1x1), (3x1)ㆍ(1x3)의 결과는 (3x3)이다. 
	3. 출력신호 : 1, -1

퍼셉트론(Perceptron) 알고리즘 학습
- 최적의 가중치를 찾는 과정. 
- 나름대로 최적의 가중치 w를 찾으면 학습이 끝나고, output은 "단위 계단 함수" Φ(wTㆍx) = 1 or -1.
- 단점 : 두 클래스가 선형적(1차함수)으로 구분되고 학습률이 작을 때만 가능. 잘못하면 가중치 업데이트 무한정...
	해결 1. 에포크(epoch) 지정 : 업데이트 횟수. 즉 훈련 반복할 최대 횟수.
	해결 2. 분류 허용 오차 지정 : 이정도 오차면 멈춰라~

초기 퍼셉트론 학습 규칙
	1. 가중치 초기화 - 0 또는 랜덤한 작은 값
	2. 각 훈련 샘플 x(i)에서
		a. 출력값 y^ 계산(단위계단함수로 예측한 클래스 레이블)
		b. "동시에" 모든 가중치 업데이트 wj = wj + Δwj
			Δwj = η( y(i) - y^(i) ) * xj(i)	, η=learning rate, y^(i)=Φ(wTㆍx), xj(i)=i행j열의 x피쳐데이터.
				i : row!!!!		x1     x2     x3     x4     |     y
						───────────────
						x1(1)  x2(1)  x3(1)  x4(1)  |  y(1)
						x1(2)  x2(2)  x3(2)  x4(2)  |  y(2)
						x1(3)  x2(3)  x3(3)  x4(3)  |  y(3)
						x1(4)  x2(4)  x3(4)  x4(4)  |  y(4)

퍼셉트론 ex_ Φ(z) = z≥0일때 1이고 아니면 -1인 함수, w=1, x=1, output은 1과 -1, η=0.1이라고 그냥 가정.
     if_____________y=-1인데, y^=1로 잘못 예측____________
	a.  Δw = 0.1(-1 - 1)*1 = -0.2
	b.  w = w+Δw = 1-0.2 = 0.8
	c.  w가 작아지면서 wTㆍx도 더 작아짐
	d.  Φ(wTㆍx)값이 -1이 될 확률↑  ==  Φ(wTㆍx)가 y^니까 y^=-1이 될 확률↑
	▶ y==y^1이 될 확률↑
     if___________y=-1인데, y^=-1로 제대로 예측___________
	a.  Δw = 0.1(-1 - -1)*1 = 0
	b.  w = w+Δw = 1+0 = 0
	▶ 잘 예측했으니까 가중치 그대로
     if_____________y=1인데, y^=-1로 잘못 예측____________
	a.  Δw = 0.1(1 - -1)*1 = 0.2
	b.  w = w+Δw = 1+0.2 = 1.2
	c.  w가 커지면서 wTㆍx도 더 커짐
	d.  Φ(wTㆍx)값이 1이 될 확률↑ ==  Φ(wTㆍx)가 y^니까 y^=1이 될 확률↑
	▶ y==y^1이 될 확률↑

아달린(Adaline) 알고리즘 학습
- 연속함수로 비용함수 정의해서 최적의 가중치를 찾는 과정. 퍼셉트론의 향상된 버전. 
- 나름대로 최적의 가중치 w를 찾으면 학습이 끝나고, output은 "선형 활성화 함수" Φ(wTㆍx) = wTㆍx.

퍼셉트론 vs. 아달린
1. 아달린은 활성화 함수를 사용한다. 
2. 오차를 피드백하는 위치가 다르다. 가중치 업데이트 위해서 w = w+Δw 해주는데, 여기서 Δw 계산법이 다름. 
	퍼셉트론	- 단위계단함수 Φ(wTㆍx) = 1 or -1 적용 → 오차 피드백 → Δw를 구함
	아달린	- 선형 활성화 함수 Φ(wTㆍx) = wTㆍx 적용 → 오차 피드백 → 단위계단함수 Φ(wTㆍx) = 1 or -1로 최종 예측 → Δw를 구함
3. 가중치 업데이트 방법이 다르다. 
	퍼셉트론	- epoch 안에서 또 한번 for문 돌리며 각 샘플(row)마다 가중치 업데이트를 해줌
	아달린	- epoch 안에서 전체 X데이터와 error를 내적하면서 전체 가중치 업데이트 해줌
4. 아달린이 퍼셉트론보다 빠름! 퍼셉트론은 이중 for문으로, 가중치 업데이트를 너무 일일이 해주기 때문. 

아달린의 비용함수 (비용==오차)
- 실제값과 예측값 사이의 "제곱 오차합(RSS)"으로 비용함수 J 정의 (음수와 양수가 서로 상쇄되지 않도록)
- 비용함수는 항상 "미분값"이 중요♣
 	J(w) = 1/2 * Σ( y(i) - y^(i) )^2     (이 때 y^(i) = Φ(wTㆍx)의 wTㆍx)	//비용함수는 '값'이 아니라 '경향성'이 중요!
								//제곱 미분하면 앞에 2 곱해줄 텐데, 그때 1로 편하게 만들어주기 위해 앞에 1/2 곱한 것. 신경쓰지 마!
	ΔJ(w) = dJ/dwj = J'(w) = ㅡΣ( y(i) - y^(i) ) * xj(i) = ㅡ(Y-Y^)ㆍX	//비용함수에서 j번째 column이 가지는 가중치 wj에 해당하는 비용함수의 기울기
			       ↑미분하면서 음수가 됨			//퍼셉트론에서 Δwj = -ηΔJ(w) = η( y(i) - y^(i) ) * xj(i)
- 단위계단함수 대신 연속적인 선형 활성화 함수를 사용하기 때문에 비용 함수가 미분 가능해졌다. 
- 비용함수 그래프 x축 : w, y축 : 비용 J(w)
- 정확도/재현율/정밀도/F1스코어/ROC-AUC : 클수록 좋음   vs.   비용함수 : 작을수록 좋음. 

경사하강법 알고리즘♣
- 가중치 업데이트 w = w - J'(w)
- 비용함수 그래프가 최소일 때 w를 찾아라!
	wj일 때 비용함수 기울기 J'(wj) < 0 ------> wj를 키워야 함. J'(wj)를 빼주면 결국 커짐. 
	wj일 때 비용함수 기울기 J'(wj) > 0 ------> wj를 줄여야 함. J'(wj)를 빼주면 결국 작아짐. 
- 딥러닝의 기초 알고리즘!

확률적 경사하강법♣(가장 多)
- epoch 끝날 때마다 X데이터를 다시 섞고, 거기서 샘플링해서 계산. 

■■■■■■■■■■■■■0526 수업中 새로 알게된것■■■■■■■■■■■■■

import numpy as np

#### 퍼셉트론 !!!
class Perceptron(object):
    """퍼셉트론 분류기

    매개변수
    ------------
    eta : float
      학습률 learning rate (0.0과 1.0 사이)
      Δwj = η( y(i) - y^(i) ) * xj(i)에서   η
    n_iter : int
      훈련 데이터셋 반복 횟수. 에포크(epoch)값
    random_state : int
      가중치 무작위 초기화를 위한 난수 생성기 시드

    속성
    -----------
    w_ : 1d-array
      학습된 가중치
    errors_ : list
      에포크마다 누적된 분류 오류

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1): 
        #아무것도 안 넣어줬을 땐 이렇지만, 지금은 객체 생성하면서 eta=0.1, n_iter=10 넣어줬음. 
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """훈련 데이터 학습

        매개변수
        ----------
        X : {array-like}, shape = [n_samples, n_features]
          n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련 데이터
        y : array-like, shape = [n_samples]
          타깃값

        반환값
        -------
        self : object

        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # normal(loc=평균, scale=표준편차, size=몇개의 데이터를 뱉어줄까?) : 평균이 loc이고 표준편차가 scale인 정규분포(평균:0 분산:1) 데이터를 size만큼 뱉어 달라!
        print(self.w_) # 3개의 랜덤값으로 가중치 3개 초기화 '''[ 0.01624345 -0.00611756 -0.00528172]'''
        
        self.errors_ = []

        for _ in range(self.n_iter): # 10바퀴만 돈다. 
            errors = 0
            for xi, target in zip(X, y): # xi = X row 한줄씩, target = y 하나씩 100바퀴 돈다. (xi, target은 늘 같은 row)
                # 가중치 변화량 Δwj = η( y(i) - y^(i) ) * xj(i)
                update = self.eta * (target - self.predict(xi)) # η( y(i) - y^(i) ) 까지만 구함. xi: X row 하나씩 들어감(vector).
                                                                # target, self.predict(xi), self.eta 모두 scalar => update도 scalar값. 
                self.w_[1:] += update * xi # update는 scalar, xi는 vector. xi 요소 하나하나에 update 곱해줌. (동시에 모든 가중치 업데이트. wj = wj + Δwj)
                self.w_[0] += update # w0은 x값과 곱해주지 않고 x와는 무관하므로 xi 곱해주면 안됨!
                errors += int(update != 0.0) # y != y^인 경우 update != 0.0. 두 번째 for문 안에서 100번 도는 중에, 몇 번 잘못 예측했는가 cnt. 
            self.errors_.append(errors)
        print(self.errors_)
        return self

    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0] # np.dot(A, B) → A와 B 행렬 내적 (==행렬 곱)
        # z = wTㆍx에서 w : w_[1:], x : X
        # self.w_[0] : 편차. 상수값. 해당 그래프를 얼마만큼 shift시키는지. ax+b에서 b, ax^2+bx+c에서 c. 보통 이것도 같이 설정해준다. 
        # x1*w1 + x2*w2 + w0 (w0 : x와는 무관. Deep learning에는 나중에 bias(얼마나 원점에서 shift되는가)라는 개념이 나오는데, 그것과 비슷한 개념)
        
    def predict(self, X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.net_input(X) >= 0.0, 1, -1) # z값 (self.net_input(X)) 구해서 단위계단함수 Φ(z) = z≥0일때 1이고 아니면 -1인 함수 만듦. 리턴되는 값은 예측값 Φ(z)



v1 = np.array([1, 2, 3])
v2 = 0.5 * v1
np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))


import pandas as pd

df = pd.read_csv('https://archive.ics.uci.edu/ml/'
        'machine-learning-databases/iris/iris.data', header=None)
df.tail()


%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# setosa와 versicolor를 선택합니다
y = df.iloc[0:100, 4].values # 총 150개 중 100개 데이터만, 4번째 column(붓꽃데이터 레이블값 str 그대로 받아옴)
y = np.where(y == 'Iris-setosa', -1, 1) # y에 'Iris-setosa'라면 -1, 그게 아니면 1을 담아라.

# 꽃받침 길이와 꽃잎 길이를 추출합니다
X = df.iloc[0:100, [0, 2]].values # 총 150개 중 100개 데이터만, 컬럼(feature)은 0번째, 2번째만.

# scatter(): 점 찍는 그래프를 그립니다
plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')
plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')

plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

plt.show() #그리기~



ppn = Perceptron(eta=0.1, n_iter=10)  # 이제 우리가 정의한 class 사용!

ppn.fit(X, y)

plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') # 아까 fit함수에서 print(self.errors_)로 출력된 '''[1, 3, 3, 2, 1, 0, 0, 0, 0, 0]'''를 그래프로!
plt.xlabel('Epochs')
plt.ylabel('Number of errors')

plt.show() # 10(epoch == n_iter)번의 가중치 업데이트를 진행하면서 100번 中 error가 발생한 횟수. 
           # 중간에 6번째부터 y축이 0이 되었으므로, 이제는 업데이트가 되지 않으므로 끝까지 쭉 0이다. 


from matplotlib.colors import ListedColormap


def plot_decision_regions(X, y, classifier, resolution=0.02):

    # 마커와 컬러맵을 설정합니다
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # 결정 경계를 그립니다
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    '''
    meshgrid(x, y)의 결과
    xx : x 한 row로 두고 그걸 y 크기만큼 아래로 쫙 복사 -> shape : (y크기, x크기)
    yy : y 한 column으로 두고 그걸 x 크기만큼 오른쪽으로 쫙 복사 -> shape : (y크기, x크기)
    '''
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # 샘플의 산점도를 그립니다
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='black')


plot_decision_regions(X, y, classifier=ppn)
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

plt.show()





#### 아달린 !!!

test=[]
test1=[]
test2=[]
class AdalineGD(object):
    global test
    global test1
    global test2
    
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        #아무것도 안 넣어줬을 땐 이렇지만, 지금은 객체 생성하면서 각각 n_iter=10, eta=0.01, n_iter=10, eta=0.0001 넣어줬음. 
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X) #row 하나씩 넣어준 퍼셉트론과 달리 여기선 (100, 2) shape의 행렬 그대로 들어감. 
            test.append(net_input)
            
            output = self.activation(net_input) #활성화함수로 net_input값 들어간다. 
            
            errors = (y - output) # output y^는 np.dot(X, self.w_[1:]) + self.w_[0] 의 결괏값 계속 그대로 가지고 있음. (퍼셉트론 y^ = 1 or -1 과 다르군!)
                                  # dJ/dw = 	J'(w) = - Σ( y(i) - y^(i) ) * xj(i) 에서 y(i) - y^(i)을 나타낸 code!
                                  # y(i) - y^(i)는 개별 요소를 연산하지만, y - output은 벡터를 연산하는 것!!!!
            test1.append(errors)
            
            self.w_[1:] += self.eta * X.T.dot(errors)   # X.T.dot(errors) : X를 전치행렬시키고, 그 아이와 errors의 내적을 구해라!
                                                        # dJ/dw = 	J'(w) = - Σ( y(i) - y^(i) ) * xj(i) 를 나타낸 코드. 
            test2.append(X.T.dot(errors)) #X의 0번 column과 error와의 내적, X의 1번 column과 error와의 내적 결과인 scalar값이 요소 2개로 저장됨. 
            
            self.w_[0] += self.eta * errors.sum() # w0은 X를 곱할 필요가 없음. 그러니까 X와 내적 없이 그냥 전체를 더해주면 됨. 
            cost = (errors**2).sum() / 2.0 # 아달린의 비용함수 J(w) = 1/2 * Σ( y(i) - y^(i) )^2 를 나타낸 코드. 
            self.cost_.append(cost) #for문 끝날 때 append해줘서, epoch 별로 비용함수가 어떻게 변하는지 담음. 
        return self

    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
        '''
        np.dot(X, self.w_[1:]) 를 어떻게 구해야 할까??
        => X의 각각의 row와의 내적을 구한다! => 결괏값의 shape는 (100,)일 것. 
        X           self.w_[1:]     np.dot(X, self.w_[1:]) __내적
        5.1 1.4     w1              5.1*w1 + 1.4*w2
        4.9 1.4     w2              4.9*w1 + 1.4*w2
        4.7 1.3                     4.7*w1 + 1.3*w2
        ...                         ...
        
        np.dot(X, self.w_[1:]) + self.w_[0] 를 어떻게 구해야 할까??
        => np.dot(X, self.w_[1:])로 나온 (100,) 의 요소 각각에 self.w_[0]를 더해준다! => 결괏값의 shape는 (100,)일 것. 
        np.dot(X, self.w_[1:])  self.w_[0]      np.dot(X, self.w_[1:]) + self.w_[0]
        5.1*w1 + 1.4*w2         w0              5.1*w1 + 1.4*w2 + w0
        4.9*w1 + 1.4*w2                         4.9*w1 + 1.4*w2 + w0
        4.7*w1 + 1.3*w2                         4.7*w1 + 1.3*w2 + w0
        ...                                     ...
        
        '''

    def activation(self, X):
        """선형 활성화 계산"""
        return X # 아~무것도 안 함. 아달린에서 쓰이는 선형 활성화 함수가 Φ(wTㆍx) = wTㆍx 이므로. 다른 활성화 함수는 이 절차에서 다른 output 나올 거임!

    def predict(self, X):
        """최종 예측"""
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)


fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)
ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o') 
# x축은 epoch 개수만큼 1~10, y축은 아까 구한 epoch별 비용을 log10 (log scale)로 차이 줄여서 그래프로 보기 편하게 보여줌. 
# 근데, 결과 그래프 보면 epoch가 늘어날 수록 error가 "증가하는 방향" 으로 발산해버림;; => 문제가 있구나...!
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Sum-squared-error)')
ax[0].set_title('Adaline - Learning rate 0.01')

ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y) #eta, 즉 learning rate step을 확 줄여버림. 
ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')
# 얘는 결과 그래프 보면 epoch가 늘어날 수록 error가 "감소하는 방향" => Good~~~ :)
# 아까는 learning rate step 값이 너무 커서 발산하고 막 튕겨 버렸구나!
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Sum-squared-error')
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()
'''
아달린이 퍼셉트론보다 빠름! 
퍼셉트론은 이중 for문으로, 가중치 업데이트를 너무 일일이 해주기 때문. 
'''

■■■■■■■■■■■■■0527 수업中 새로 알게된것■■■■■■■■■■■■■

# ## 5.3 Gradient Descent

# ** 실제값을 Y=4X+6 시뮬레이션하는 데이터 값 생성 **

# In[ ]:


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

np.random.seed(0)
# y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듦
X = 2 * np.random.rand(100,1) # 0~1 사이에서 균일분포 기반으로 100개 랜덤값 샘플링. 각각의 요소에 x2 해줌.  
y = 6 + 4*X + np.random.randn(100,1)
'''
random.randint(m, n) : m ~ n-1 사이의 랜덤 숫자 1개 뽑기. m 안써주면 0부터~
random.rand(m, n) : 0 ~ 1 의 '균일분포 표준정규분포' 랜덤 숫자를 (m, n) 크기로 생성.
        균일분포 == 상수함수(연속) : 샘플링한 값의 range가 0 ~ 1이고 0.3 ~ 0,4 사이의 그래프와 x축간의 면적은 "0.3 ~ 0.4 사이의 값이 뽑힐 확률"
                                    떨어진 간극만 같다면 걔네의 면적, 즉 확률도 같다. 
random.randn(m, n) : 평균 0, 표준편차 1의 '가우시안 표준정규분포' 랜덤 숫자를 (m, n) 크기로 생성
        정규분포 : 0.3 ~ 0.4 사이의 값이 뽑힐 확률과 0.4 ~ 0.5 사이의 값이 뽑힐 확률이 다르다. ex_히스토그램 distplot(정규분포, kde=True)
random.normal(평균, 표준편차, size) : 주어진 평균, 표준편차의 정규분포를 size만큼 생성
'''
#test = np.random.rand(100000,1)
#test2 = np.random.randn(1000,1)
#sns.distplot(test, kde=False)  #균일분포
#sns.distplot(test2, kde=False) #정규분포

# X, y 데이터 셋 scatter plot으로 시각화
plt.scatter(X, y)


# In[ ]:


X.shape, y.shape


# ** w0과 w1의 값을 최소화 할 수 있도록 업데이트 수행하는 함수 생성.**
# 
# * 예측 배열 y_pred는 np.dot(X, w1.T) + w0 임
# 100개의 데이터 X(1,2,...,100)이 있다면 예측값은 w0 + X(1)*w1 + X(2)*w1 +..+ X(100)*w1이며, 이는 입력 배열 X와 w1 배열의 내적임.
# * 새로운 w1과 w0를 update함
# ![](./image01.png)

# In[ ]:


# w1 과 w0 를 업데이트 할 w1_update, w0_update를 반환. 
def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    N = len(y)
    # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화
    w1_update = np.zeros_like(w1) # np.zeros_like(변수) : 그 변수와 같은 size를 0으로 채워서 초기화하고 리턴.  
    w0_update = np.zeros_like(w0)
    # 예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(X, w1.T) + w0 # [x1*w1 + w0, x2*w2 + w0, ... , x100*w100 + w0] → (100, 1) shape
    diff = y-y_pred
         
    # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 
    w0_factors = np.ones((N,1))

    # w1과 w0을 업데이트할 w1_update와 w0_update 계산  // Δwj = -ηΔJ(w) = -ηJ'(w)
    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff)) # ΔJ(w) = dJ/dwj = J'(w) = ㅡΣ( y(i) - y^(i) ) * xj(i) = ㅡ(Y-Y^)ㆍX  ==  np.dot(X.T, diff)
    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))    
    
    return w1_update, w0_update


# In[ ]:


w0 = np.zeros((1,1))
w1 = np.zeros((1,1))
y_pred = np.dot(X, w1.T) + w0
diff = y-y_pred
print(diff.shape)
w0_factors = np.ones((100,1))
w1_update = -(2/100)*0.01*(np.dot(X.T, diff))
w0_update = -(2/100)*0.01*(np.dot(w0_factors.T, diff))   
print(w1_update.shape, w0_update.shape)
w1, w0


# ** 반복적으로 경사 하강법을 이용하여 get_weigth_updates()를 호출하여 w1과 w0를 업데이트 하는 함수 생성 **

# In[ ]:


# 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함. 
def gradient_descent_steps(X, y, iters=10000):
    # w0와 w1을 모두 0으로 초기화. 
    w0 = np.zeros((1,1))
    w1 = np.zeros((1,1))
    
    # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. 
    for ind in range(iters):
        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) # Δw1, Δw2 받아옴
        w1 = w1 - w1_update # w1 = w1 - Δw1
        w0 = w0 - w0_update # w0 = w0 - Δw0
        '''
        모든 가중치 (여기선 w1, w0 두 개)를 개별적으로 업데이트하는 게 아니라 iters(epoch)마다 한번에. 
        퍼셉트론 보다는 '아달린'에 가까움!!
        '''
    return w1, w0 # 최종 가중치 리턴 


# ** 예측 오차 비용을 계산을 수행하는 함수 생성 및 경사 하강법 수행 **

# In[ ]:


def get_cost(y, y_pred): #비용함수 구하는 함수. 우리가 줄여야 하는 타겟(비용). 
    N = len(y) 
    cost = np.sum(np.square(y - y_pred))/N  #비용함수 J(w) = 1/2 * Σ( y(i) - y^(i) )^2 를 나타낸 코드. square() : 제곱
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000) #최종 업데이트 끝난 가중치 받아옴. 
print("w1:{0:.3f} w0:{1:.3f}".format(w1[0,0], w0[0,0]))
y_pred = w1[0,0] * X + w0  #회귀 계수 : 가중치. x축-X, y축-y_pred인 하나의 1차함수
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))


# In[ ]:


plt.scatter(X, y)
plt.plot(X,y_pred) # X의 feature값이 2개였다면 (column이 2개) -> 3차원의 직선 그래프. 기울기는 얘도 역시 편미분이므로 2개. 


# ** 미니 배치 확률적 경사 하강법을 이용한 최적 비용함수 도출 **

# In[ ]:


def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000): ### ◆확률적 경사하강법◆ ###
    w0 = np.zeros((1,1))
    w1 = np.zeros((1,1))
    prev_cost = 100000
    iter_index =0
    
    for ind in range(iters):
        np.random.seed(ind) # ind라는 규칙(시드값)으로 랜덤하게 뽑겠다. 근데 이 코드 필요 없음. 괜히 쓴듯...ㅎ
        '''
        시드값 하나로 고정
            => for문 안에서 여러 번 permutation해도 매번 다른 샘플링
            => for문 돌 때마다 매번 다른 샘플링
            => but, 이 프로젝트 전체를 F5 눌러서 다시 실행 or 전원을 껐다 켜면  >  처음 나온 랜덤값들과 똑같이 나옴. 첫번째 랜덤값끼리, 두번째 랜덤값끼리 ...
        여기처럼 시드값이 계속 바뀜
            => 이 프로젝트 전체를 F5 눌러서 다시 실행 or 전원을 껐다 켜면  >  처음 나온 랜덤값들과 관계 없이, 아예 새로운 랜덤값 나옴.
            
        시드값을 바꾸려면 우리가 직접 코드를 수정해야 한다. 그건 불가능. 
        =====> 보통 어플 출시 전에 시드값으로 date 정보를 넣도록 애초에 코드를 짬. 
        
        '''
        # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장
        stochastic_random_index = np.random.permutation(X.shape[0]) #np.random.permutation(n) : 0 ~ n-1 숫자가 무작위로 섞인 배열을 만들어준다. epoch마다 랜덤하게 샘플링. 
        sample_X = X[stochastic_random_index[0:batch_size]] #상위에 있는 10개(==batch_size)만 샘플링해서 넘겨준다. 
        sample_y = y[stochastic_random_index[0:batch_size]]
        # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트
        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01) #추출된 데이터에서만 Δw1, Δw2 받아옴. 확률적 경사하강법도 역시 아달린~!
        w1 = w1 - w1_update
        w0 = w0 - w0_update
        '''
        '추출된'(확률적 경사하강법) 모든 가중치 (여기선 w1, w0 두 개)를 개별적으로 업데이트하는 게 아니라 iters(epoch)마다 한번에. 
        이것도 퍼셉트론 보다는 '아달린'에 가까움!!
        '''
    return w1, w0 # 최종 가중치 리턴


# In[ ]:

print(X.shape[0])
test = np.random.permutation(X.shape[0]) 
# np.random.permutation() : 무작위로 섞인 배열을 만들어준다. 
# stochastic 기법 자체가 모든 가중치를 업데이트하지 않음. epoch마다 각각 다르게 샘플링해서, 걔네에 해당하는 가중치들만 업데이트. 


# In[ ]:


w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)
print("w1:",round(w1[0,0],3),"w0:",round(w0[0,0],3))
y_pred = w1[0,0] * X + w0
print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
'''
퍼셉트론 vs 아달린 vs 확률적 경사하강법(stochastic)
cost : 퍼셉트론 < 아달린 < 확률적 경사하강법
여기서 구한 cost : 0.9935      0.9937     => 비용적으로만 보면 아달린이 더 좋긴 함. 그렇지만 big data 처리할 때 시간이 훨씬 빠름. 

'''
  
plt.scatter(X, y)
plt.plot(X,y_pred) # X의 feature값이 2개였다면 (column이 2개) -> 3차원의 직선 그래프. 기울기는 얘도 역시 편미분이므로 2개. 

■■■■■■■■■■■■■0528 수업中 새로 알게된것■■■■■■■■■■■■■

######### 다항 회귀 prac 01 #########

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


X = np.array([258.0, 270.0, 294.0, 
              320.0, 342.0, 368.0, 
              396.0, 446.0, 480.0, 586.0])[:, np.newaxis] 
            # np.newaxis : 차원(dimension)을 하나 더 높여줌. (10,) → (10, 1)

y = np.array([236.4, 234.4, 252.8, 
              298.6, 314.2, 342.2, 
              360.8, 368.0, 391.2, 390.8])
            # 얘는 여전히 (10,)

from sklearn.preprocessing import PolynomialFeatures #★★★

lr = LinearRegression()
pr = LinearRegression()
quadratic = PolynomialFeatures(degree=2) # degree=2 : 2차 다항식으로 만들어줄 것. ax+b → ax²+bx+c
X_quad = quadratic.fit_transform(X)
'''
    PolynomialFeatures(degree=2) : 2차 다항식으로 만들어줄 것. ax+b → ax²+bx+c
    만약 degree = 3, 4, 5, 6, ... 점점 커진다면?
    => MSE값은 완전 0에 가까워지지만, 이러면 overfitting 위험!! => "회귀 관점의 Overfitting :("
    => 그래서 규제가 있는 거임. 

    ax+b → ax²+bx+c
    ..................................
    x : [x]         → [x²,  x,  1]
    w : [w1, w0]    → [w2, w1, w0]      //가중치 4개 아니고 3개인 이유 : [w3, w2, w1, w0]으로 만들어졌는데, 사실상 w1은 계속 1만 곱해지기 때문에 w1, w0 모두 bias. 그래서 그냥 w1+w0로 합쳐버림. 
    Y는 일정
    
    y = w1*x + w0   → w2*x² + w1*x + w0. 
    이제 구해진 새로운 2차식 y = w2*x² + w1*x + w0 에서 퍼셉트론/아달린/경사하강법 등등을 사용해 최적의 w 구함.
    => 그래프로 보면, 1차식만 사용해 구하던 걸, 이제는 2차함수를 사용해 MSE값을 구한 것. 
'''


# 선형 특성 학습
lr.fit(X, y)
X_fit = np.arange(250, 600, 10)[:, np.newaxis] # (35,) → (35, 1)
y_lin_fit = lr.predict(X_fit) # X_fit는 X_test 데이터로 만들어준 거구나~!!   //지금 y_test데이터는 따로 분류 안해줌. 그냥 예측 어떻게 나오는지만 보자~

# 이차항 특성 학습
pr.fit(X_quad, y)
y_quad_fit = pr.predict(quadratic.fit_transform(X_fit))

# 결과 그래프
plt.scatter(X, y, label='training points')
plt.plot(X_fit, y_lin_fit, label='linear fit', linestyle='--')
plt.plot(X_fit, y_quad_fit, label='quadratic fit')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

==========================================================

######### 다항 회귀 prac 02 #########

# ## 5-5. 다항회귀 Polynomial Regression과 오버피팅/언더피팅 이해
# ### 다항회귀 Polynomial Regression 이해

# 다항회귀 PolynomialFeatures 클래스로 다항식 변환
# 
# ![](./image02.png)

# In[7]:


from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# 다항식으로 변환한 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성
X = np.arange(4).reshape(2,2)
print('일차 단항식 계수 feature:\n',X )

# degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환
poly = PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr = poly.transform(X)
print('변환된 2차 다항식 계수 feature:\n', poly_ftr)
'''
    x : [[x1, x2]         → [[1, x1, x2, x1², x1*x2, x2²]
         [x3, x4]]           [1, x3, x4, x3², x3*x4, x4²]]
    w : [w1, w2, w0]      → [w0, w1, w2, w3,    w4, w5] 
'''


# 3차 다항식 결정값을 구하는 함수 polynomial_func(X) 생성. 즉 회귀식은 결정값 y = 1+ 2x_1 + 3x_1^2 + 4x_2^3 

# In[8]:


def polynomial_func(X):
    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3  # z = y^ = 1+2x₁+3x₁²+4x₂³ 
    return y

X = np.arange(0,4).reshape(2,2) #[[0,1]   --> x₁
                                # [2,3]]  --> x₂

print('일차 단항식 계수 feature: \n' ,X)
y = polynomial_func(X) # y = 1+[0, 4]+[0, 12]+[4, 108] = 1+[4, 124] = [5, 125]
print('삼차 다항식 결정값: \n', y)

# 3 차 다항식 변환 
poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)
print('3차 다항식 계수 feature: \n',poly_ftr)

# Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인
model = LinearRegression()
model.fit(poly_ftr,y) # x_train, y_train. 근데 y값은 degree가 바뀌어도 안 바뀜. 그 y값을 x로 표현하는 방법만 바뀐 것임. 
                      # 입력값을 poly_ftr로 하고 출력값을 y라고 했을 때 가중치를 구하는 과정. 
print('Polynomial 회귀 계수\n' , np.round(model.coef_, 2)) #최적의 회귀 계수 이렇게 나왔다~
print('Polynomial 회귀 Shape :', model.coef_.shape)


# 3차 다항식 계수의 피처값과 3차 다항식 결정값으로 학습

# ** 사이킷런 파이프라인(Pipeline)을 이용하여 3차 다항회귀 학습 **  
# 
# 사이킷런의 Pipeline 객체는 Feature 엔지니어링 변환과 모델 학습/예측을 순차적으로 결합해줍니다. 

# In[9]:


from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

def polynomial_func(X):
    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 
    return y

# Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결
model = Pipeline([('poly', PolynomialFeatures(degree=3)), # pipeline 안에 리스트로 두 개의 튜플(두 개의 절차)이 묶여 들어있다. 하나는 PolynomialFeatures, 하나는 LinearRegression
                  ('linear', LinearRegression())])
X = np.arange(4).reshape(2,2)
y = polynomial_func(X)

model = model.fit(X, y)
print('Polynomial 회귀 계수\n', np.round(model.named_steps['linear'].coef_, 2))

'''
저번에 앙상블에서 사용한 pipe1, pipe3를 보면 

# estimator 객체 3개 만듦
clf1 = LogisticRegression(solver='liblinear',
                          penalty='l2', 
                          C=0.001,
                          random_state=1)

clf2 = DecisionTreeClassifier(max_depth=1,  #지금 일부러 max_depth를 1로 줘서 너무 얕게 만듦 -> 일부러 약한 분류기를 만들었구나~!
                              criterion='entropy', #get_param()으로 까보면 criterion default값 : gini    //gini, entropy, 불순물지수 전부 비슷한 목적이다. 같다고 생각.
                              random_state=0)

clf3 = KNeighborsClassifier(n_neighbors=1,
                            p=2,
                            metric='minkowski')

pipe1 = Pipeline([['sc', StandardScaler()], #그냥 clf1 이름이 pipe1로 바꼈구나~라고 생각
                  ['clf', clf1]])
pipe3 = Pipeline([['sc', StandardScaler()], #그냥 clf3 이름이 pipe3으로 바꼈구나~라고 생각
                  ['clf', clf3]])

#   cf. 파이프라인
#   pipe1, pipe3 얘네는 나중에 estimator처럼 사용할 것이다. (fit, predict)
#   estimator는 fit(X_train, y_train)해줘야 하는데, Pipiline은 일단 스케일링 해주고 clf1으로 학습해준다. 

이런식으로 스케일링을 해줬는데, 이번에는 여러 절차를 같이 담아서 해주었다. 
1. X를 먼저 3차 다항식으로 피처를 10개로 늘려주고, 
2. LinearRegression으로 최적의 가중치를 구해서 그 가중치가 들어간 식을 model에 리턴해서 넣어준다. 
'''

■■■■■■■■■■■■■0531 수업中 새로 알게된것■■■■■■■■■■■■■

다항 회귀에서 Underfitting, Overfitting
: 너무 낮은 차수 ex_ degree=1 - Underfitting, 너무 높은 차수 ex_ degree=15 - Overfitting

편향-분산 트레이드오프(Bias-Variance Trade Off) : 머신러닝의 문제점, 없애려면 => 규제
degree=1
	- 과소적합 (Underfitting)
	- 고편향	  (High Bias)
degree=15
	- 과적합    (Overfitting)
	- 고분산    (High Variance)

규제 선형 모델
- 차수를 줄이는 게 아니라, w에 대한 규제를 준다. 
	다항 회귀에서 차수를 키울 수록, MSE값을 줄이기 위해서는 |기울기| 즉 |가중치| 값을 올릴 수밖에 없다. 
- 비용 함수가 이제 바뀜! 우리가 원래 비교하던 RSS(W)값에 규제를 위한 식을 더해줌. 
- 라쏘 회귀/릿지 회귀/엘라스틱넷 회귀 수행할 때 input값으로 alpha 꼭 정해서 넣어줌. 
	alpha : 규제 모델에 얼마의 비중을 둘 건가
	alpha가 작으면 W가 커져도 어느 정도 ㄱㅊ, alpha가 크면 W가 커지는 게 상당히 부담!

L1규제 (라쏘 Lasso)
- 비용 함수 목표 = Min( RSS(W) + alpha*|W| )

L2규제 (릿지 Ridge)
- 비용 함수 목표 = Min( RSS(W) + alpha*|W|² )

L1L2규제 (엘라스틱넷 ElasticNet)
- 비용 함수 목표 = Min( RSS(W) + alpha1*|W| + alpha2*|W|² ) = Min( RSS(W) + a*L1 + b*L2 )
- alpha α = a+b
- l1_ratio = a / (a+b)

ERROR = Bias + Variance + ε(입실론, 자연발생해서 어떻게 할 수 없는 오류)
실제데이터     	      y = F*(x) + ε	 (F*(x) : 이상적인 회귀선, ε : 정규분포에 입각했다고 가정.)
DataSet1		F^(x)₁ = F*(x) + ε₁
DataSet2		F^(x)₂ = F*(x) + ε₂
...		        ...
평균		F ̄ (x)  = E(F^(x)) = (F^(x)₁+F^(x)₂+...) / n
ERROR 	= E( [y - F^(x)]² )
	= E( [F*(x) + ε - F^(x)]² )
	= E( [F*(x) - F^(x) + ε]² )
	       ----ⓐ-----    ⓑ 
	= E( [F*(x) - F^(x)]² )  +  E( [ε]² )  +  2 * E( [F*(x)-F^(x)] ) * E( [ε] )
			     ---분산--- 		          --평균:0--
	= E( [F*(x) - F^(x)]² )  +  var
	= E( [F*(x) - F ̄ (x) + F ̄ (x) - F^(x)]² )  +  var
	      -----ⓐ-----     -----ⓑ-----
	= E( [F*(x) - F ̄ (x)]² ) + E( [F ̄ (x) - F^(x)]² )  +  var
	  -------Bias---------   ------Variance------    --ε--

레이블값이 정규 분포가 아니라 왜곡된 형태의 분포도일 경우 → 데이터 변환해줘야 함. 
1. 스케일링 ♣ (트리, 분류는 꼭 안 해줘도 될 때가 많지만, 회귀는 무조건)
	1-1. StandardScaler (표준화시켜줌 - 평균0 분산1인 데이터세트로 바뀜)
	1-2. MinMaxScaler (정규화시켜줌 - 최소0 최대1인 데이터세트로 바뀜)
2. 다항식 피처로 변환 PolynomialFeatures(degree)
	: 스케일링 하고 나서. 컴퓨터입장에서는 feature개수↑ = 과적합↑ 이므로 차수 낮게 설정
3. 로그 변환 np.log1p( ) ♣♣♣
    
로그변환
- 비대칭도(skewness)가 높은 데이터 분포를 정규 분포로 바꾸는 방법
- 스케일이 변한 것이지, 분포 그래프의 모양이 바뀌는 게 아님.    // a > b 이면 log1p(a) > log1p(b)
        ex_ 데이터분포 그래프를 f(x)라 할 때
                     원본  : f(70) > f(5) > f(25) > f(100)
             log1p(원본) : f(70) > f(5) > f(25) > f(100) 그대로! 그냥 기울기가 완만해질 뿐!!      
- 로그변환 이후 히스토그램은 왜 모양 자체가 바뀔까??
        => 로그변환을 하면 f(x)값의 극단적인 비대칭성이 줄어들고, 새로 그리는 그래프는 step을 그 줄어든 비율만큼 짧게 잡을 것이다. 
            그러면 새로운 도수 분포는 f(x)값이 보다 고르게 퍼지고, 결국 히스토그램이 정규분포를 닮아간다. 

=============================================================

# ### Polynomial Regression 을 이용한 Underfitting, Overfitting 이해

# ** cosine 곡선에 약간의 Noise 변동값을 더하여 실제값 곡선을 만듬 **

# In[ ]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
get_ipython().run_line_magic('matplotlib', 'inline')

# random 값으로 구성된 X값에 대해 Cosine 변환값을 반환. 
def true_fun(X):
    return np.cos(1.5 * np.pi * X) # (0~1) * 1.5 * np.pi    => 0 ~ 1.5π 까지의 cosθ 그래프   => 실제 정답값. y_test!!

# X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다.  
np.random.seed(0)
n_samples = 30
X = np.sort(np.random.rand(n_samples)) # rand : 0~1 사이 균일분포 30개, 그 이후 오름차순 정렬

# y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. 
y = true_fun(X) + np.random.randn(n_samples) * 0.1 # randn : 0~1 사이 정규분포 30개
    # 지금 true_fun(X)에서 0 ~ 1.5π 까지의 cosθ 그래프를 구하고, 그 그래프에서 정규분포만큼의 noise를 줌. 
    # randn은 정규분포로 평균이 0이고 분산이 1이므로, 그런 데이터들을 더해주면 원본 데이터를 기준으로 분산 1만큼의 noise가 살짝씩 생김. (평균에 가까울수록 밀도 높음)

'''
호도법
- θ = 1 (rad) ,  π = 3.14 (rad)
- 대부분의 프로그래밍은 각도가 아니라 호도법(np.pi=3.14)사용!!

np.cos(90)은 90˚가 아니라 90 rad. 그래서 0이 나오지 않고, 쌩뚱맞은 값이 나온다. 
'''

# In[ ]:


plt.scatter(X, y)


# In[ ]:


plt.figure(figsize=(14, 5))
degrees = [1, 4, 15] # 나중에 PolynomialFeatures의 인수 degree에 넣어줄 애들이겠네~

# 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다. 
for i in range(len(degrees)):
    ax = plt.subplot(1, len(degrees), i + 1) #subplot(row, column, i) : 큰 그림판 안에서 그래프 여러 개 row x column만큼 그 위치에 그리는데, 그 중 i번째 그래프!
    plt.setp(ax, xticks=(), yticks=())
    
    # 개별 degree별로 Polynomial 변환합니다. 
    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) # 처음에 degree 1로 넣을 땐 아무 확장 안 됨. 
    linear_regression = LinearRegression()
    pipeline = Pipeline([("polynomial_features", polynomial_features),
                         ("linear_regression", linear_regression)])
    pipeline.fit(X.reshape(-1, 1), y)
    
    # 교차 검증으로 다항 회귀를 평가합니다. 
    scores = cross_val_score(pipeline, X.reshape(-1,1), y,scoring="neg_mean_squared_error", cv=10) #교차 검증 10번으로 나온 10개의 -MSE값. 
    coefficients = pipeline.named_steps['linear_regression'].coef_ # 교차 검증으로 총 학습된 가중치
    print('\nDegree {0} 회귀 계수는 {1} 입니다.'.format(degrees[i], np.round(coefficients),2))
    print('Degree {0} MSE 는 {1:.2f} 입니다.'.format(degrees[i] , -1*np.mean(scores)))
    '''
    Degree 1 회귀 계수는 [-2.] 입니다.
    Degree 1 MSE 는 0.41 입니다.

    Degree 4 회귀 계수는 [  0. -18.  24.  -7.] 입니다.
    Degree 4 MSE 는 0.04 입니다.

    Degree 15 회귀 계수는 [-2.98300000e+03  1.03900000e+05 -1.87417100e+06  2.03717220e+07
                      -1.44873987e+08  7.09318780e+08 -2.47066977e+09  6.24564048e+09
                      -1.15677067e+10  1.56895696e+10 -1.54006776e+10  1.06457788e+10
                      -4.91379977e+09  1.35920330e+09 -1.70381654e+08] 입니다.
    Degree 15 MSE 는 182815433.48 입니다.
    
    과적합 → 작아져야 하는 MSE값이 엄청 커짐. 
    '''
    
    # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다. 
    # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다.  
    X_test = np.linspace(0, 1, 100) # 0~1 사이에서 100등분해서 리턴. X_test = [0, 0.01, 0.02, ..., 0.99], 크기:(100,)
    # 예측값 곡선
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model") # (100,) -> (100, 1) 크기 바꿈. 
    # 실제 값 곡선
    plt.plot(X_test, true_fun(X_test), '--', label="True function") 
    '''
    y_train에는 noise를 섞었었지만, y_test에는 noise 섞지 않고 cos값 그대로 넣어주었다. 
    1. 우리의 궁극적인 목표인 본연의 cos함수를 구하는 모습을 보기 위해 일부러 y_train에는 noise 섞어줌. 
    2. 실무에서 자연발생 noise없는 데이터는 거의 x. 게다가 실제 데이터는 noise가 대부분 정규분포 x. noise평균이 0이 아니므로 bias가 생김. 얘네 제거해줘야 함. 
    3. 만약 y_train에 입혀준 noise가 정규분포가 아니라면, 구한 데이터가 한 쪽으로 shift된 그래프가 나올 것. 
        그러면 이걸 또 여러 번 모델링해서, 그 모델링들의 평균을 찾아내면 그 bias를 제거해줄 수도 있다. 
    '''
    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")
    
    plt.xlabel("x"); plt.ylabel("y"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc="best")
    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(degrees[i], -scores.mean(), scores.std()))

plt.show()

■■■■■■■■■■■■■0601 수업中 새로 알게된것■■■■■■■■■■■■■

# ## 5.4 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측

# In[  ]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns # DataFrame형태의 데이터를 그릴 때 seaborn 많이 사용. 
from scipy import stats
from sklearn.datasets import load_boston
get_ipython().run_line_magic('matplotlib', 'inline')

# boston 데이타셋 로드
boston = load_boston()

# boston 데이타셋 DataFrame 변환 
bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)

# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. 
bostonDF['PRICE'] = boston.target
print('Boston 데이타셋 크기 :',bostonDF.shape)
bostonDF.head()


# * CRIM: 지역별 범죄 발생률  
# * ZN: 25,000평방피트를 초과하는 거주 지역의 비율
# * NDUS: 비상업 지역 넓이 비율
# * CHAS: 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0)
# * NOX: 일산화질소 농도
# * RM: 거주할 수 있는 방 개수
# * AGE: 1940년 이전에 건축된 소유 주택의 비율
# * DIS: 5개 주요 고용센터까지의 가중 거리
# * RAD: 고속도로 접근 용이도
# * TAX: 10,000달러당 재산세율
# * PTRATIO: 지역의 교사와 학생 수 비율
# * B: 지역의 흑인 거주 비율
# * LSTAT: 하위 계층의 비율
# * MEDV: 본인 소유의 주택 가격(중앙값)




# * 각 컬럼별로 주택가격에 미치는 영향도를 조사   //각 컬럼별 가중치 구하기 전, 레이블과 무엇이 상관관계가 있는지. 

# In[  ]:


# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.
fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2) # 행 2개, 컬럼 4개. figsize는 가로 16, 세로 8로 그림판 axs를 크게 만들어달라는 말
lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']
for i , feature in enumerate(lm_features):
    row = int(i/4)  # 0 0 0 0 1 1 1 1 순서대로 들어감
    col = i%4       # 0 1 2 3 0 1 2 3 순서대로 들어감
    
    # 시본의 regplot(선형 회귀 직선 자동적으로 데이터 분포에 의해 알아서 그려줌 ><) 을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(x=feature , y='PRICE',data=bostonDF , ax=axs[row][col]) #8개의 각 그래프는 x축을 feature, y축을 'PRICE'로 둠. 
'''
그래프 대충 봤을 때 상관관계 높은 컬럼 
RM: 거주할 수 있는 방 개수  -> 양의 상관관계 높음
LSTAT: 하위 계층의 비율     -> 음의 상관관계 높음
'''


# ** 학습과 테스트 데이터 세트로 분리하고 학습/예측/평가 수행 **

# In[3]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error , r2_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False) # bostonDF['PRICE']가 실제 label값이므로 얘만 떼어넴!!!

X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156)

# Linear Regression OLS로 학습/예측/평가 수행. 
lr = LinearRegression()
lr.fit(X_train ,y_train )
y_preds = lr.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse) # np.sqrt(n) : √n (루트 씌워줌)

print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))
'''
MSE : 17.297 , RMSE : 4.159
Variance score : 0.757
'''


# In[4]:

# 회귀라는 관점에서 결국 모델링이라는 건, 각 feature(지금 마지막 하나 떼어냈으니까 13가지 컬럼에 곱할 가중치 w13, w12, ..., w1와 w0를 구하는 것! 즉, 13차원의 ~이런 기울기, ~이런 가중치를 갖는 그래프를 그리는 것!
print('절편 값:',lr.intercept_)
print('회귀 계수값:', np.round(lr.coef_, 1)) # lr.coef_를 소수점 첫째 자리까지 반올림해라 => 회귀계수(regression coefficient). 지금 이게 W값 구한 것. 
'''
절편 값: 40.995595172164336
회귀 계수값: [ -0.1   0.1   0.    3.  -19.8   3.4   0.   -1.7   0.4  -0.   -0.9   0.  -0.6]
                                      △ NOX컬럼은 전부 소수점으로, 애초에 데이터값 자체가 다른 컬럼에 비해 상대적으로 많이 작음. 
                                      지금은 전체 피처들을 SCALE해주지 않았기 때문에 얘만 이렇게 크게 나옴. 얘랑 그래프랑 비교하면 굳이 이렇게 높을 이유가 없음. 
                                      스케일링 (SCALE)이 필요하다..!
                                      그래도, 어쨌든 계산된 회귀 계수는 이렇게 하는 게 맞음. 
'''


# In[5]:


# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index가 컬럼명에 유의
coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns )
coeff.sort_values(ascending=False)


# In[6]:


from sklearn.model_selection import cross_val_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)
lr = LinearRegression() #선형회귀 기반 estimator

# cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시  RMSE 구함. 
# train데이터만을 사용해서 거기에서 일정의 train데이터, 일정의 검증데이터로 나눠 cv만큼 교차검증. 퍼포먼스의 우수성은 neg_mean_squared_error, 즉 MSE값이 가장 낮은 게 우수. 
neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
rmse_scores  = np.sqrt(-1 * neg_mse_scores) # scoring="neg_mean_squared_error" 로 잡아줬으니, 그냥 MSE 구하려면 다시 -1 곱해줘야 함. 
avg_rmse = np.mean(rmse_scores)

# cross_val_score(scoring="neg_mean_squared_error")로 반환된 값은 모두 음수 
print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))
'''
 5 folds 의 개별 Negative MSE scores:  [-12.46 -26.05 -33.07 -80.76 -33.31]
 5 folds 의 개별 RMSE scores :  [3.53 5.1  5.75 8.99 5.77]
 5 folds 의 평균 RMSE : 5.829 
'''


# ## 5-6. Regularized Linear Models – Ridge, Lasso
# ### Regularized Linear Model - Ridge Regression

# In[ ]:


# 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# boston 데이타셋 로드
boston = load_boston()

# boston 데이타셋 DataFrame 변환 
bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)

# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. 
bostonDF['PRICE'] = boston.target
print('Boston 데이타셋 크기 :',bostonDF.shape)

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)


ridge = Ridge(alpha = 10)
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
rmse_scores  = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)
print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))
'''
 5 folds 의 개별 Negative MSE scores:  [-11.422 -24.294 -28.144 -74.599 -28.517]
 5 folds 의 개별 RMSE scores :  [3.38  4.929 5.305 8.637 5.34 ]
 5 folds 의 평균 RMSE : 5.518
'''


# ** 다항 회귀를 이용한 보스턴 주택가격 예측 **

# In[]:


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error , r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

# boston 데이타셋 로드
boston = load_boston()

# boston 데이타셋 DataFrame 변환 
bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)

# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. 
bostonDF['PRICE'] = boston.target
print('Boston 데이타셋 크기 :',bostonDF.shape)

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)


X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156)

## Pipeline을 이용하여 PolynomialFeatures 변환과 LinearRegression 적용을 순차적으로 결합. 
p_model = Pipeline([('poly', PolynomialFeatures(degree=3, include_bias=False)),
                  ('linear', LinearRegression())])

p_model.fit(X_train, y_train)
y_preds = p_model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)


print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))


# In[]:


X_train_poly= PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_train, y_train)
X_train_poly.shape, X_train.shape
# ** alpha값을 0 , 0.1 , 1 , 10 , 100 으로 변경하면서 RMSE 측정 **

# In[ ]:


# Ridge에 사용될 alpha 파라미터의 값들을 정의
alphas = [0 , 0.1 , 1 , 10 , 100] #뒤에 200, 300, 700, ... 계속 넣어봐도 100보다 큼. alpha = 100 일 때 가장 작은 RMSE값!

# alphas list 값을 iteration하면서 alpha에 따른 평균 rmse 구함.
for alpha in alphas :
    ridge = Ridge(alpha = alpha)
    
    #cross_val_score를 이용하여 5 fold의 평균 RMSE 계산
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} '.format(alpha,avg_rmse))
    '''
    alpha 0 일 때 5 folds 의 평균 RMSE : 5.829 
    alpha 0.1 일 때 5 folds 의 평균 RMSE : 5.788 
    alpha 1 일 때 5 folds 의 평균 RMSE : 5.653 
    alpha 10 일 때 5 folds 의 평균 RMSE : 5.518 
    alpha 100 일 때 5 folds 의 평균 RMSE : 5.330
    '''

# ** 각 alpha에 따른 회귀 계수 값을 시각화. 각 alpha값 별로 plt.subplots로 맷플롯립 축 생성 **

# In[ ]:


# 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성  
fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)
# 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성  
coeff_df = pd.DataFrame()

# alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정
for pos , alpha in enumerate(alphas) :
    ridge = Ridge(alpha = alpha) # ridge도 결국엔 규제가 적용된 회귀 estimator. 최적의 가중치 찾아내야 하는 건 다른 estimator와 똑같음. 
    ridge.fit(X_data , y_target)
    
    # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가.  
    coeff = pd.Series(data=ridge.coef_ , index=X_data.columns ) #보스턴 주택가격 df의 column들이 RM, CHAS, RAD, ... 13개 있는데, alpha를 정해주면 각각의 column별로 최적의 가중치가 w1, w2, w3, ... 정해질 것이다. 그 가중치들을 coef로 불러와서 Series로 담아줌.. 
    colname='alpha:'+str(alpha) # 문자열 + 문자열 => 문자열 연결
    coeff_df[colname] = coeff # 비어있는 coeff_df 데이터프레임에 column 하나씩 집어넣음. 
    
    # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3,6)
    sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])

# for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시
plt.show()
'''
alpha값이 0 - 0.1 - 1 - 10 - 100 으로 커질 수록, 피처별 회귀 계수 w들의 |w| 크기가 다들 작아진다. 
'''


# ** alpha 값에 따른 컬럼별 회귀계수 출력 **

# In[ ]:


ridge_alphas = [0 , 0.1 , 1 , 10 , 100]
sort_column = 'alpha:'+str(ridge_alphas[0])
coeff_df.sort_values(by=sort_column, ascending=False)


# ### 라쏘 회귀

# In[ ]:


from sklearn.linear_model import Lasso, ElasticNet

# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True):
    coeff_df = pd.DataFrame()
    if verbose : print('####### ', model_name , '#######')
    for param in params:
        if model_name =='Ridge': model = Ridge(alpha=param)
        elif model_name =='Lasso': model = Lasso(alpha=param)
        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7) # 여기서는 단순히 alpha값의 변화만 살피기 위해 l1_ratio( ==a/(a+b) )는 미리 0.7로 고정해둠. 
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv = 5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))
        
        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출. 
        model.fit(X_data , y_target)
        # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. 
        coeff = pd.Series(data=model.coef_ , index=X_data.columns )
        colname='alpha:'+str(param)
        coeff_df[colname] = coeff
    return coeff_df
# end of get_linear_regre_eval


# In[ ]:


# 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출
lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
'''
#######  Lasso #######
alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.612 
alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.615 
alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.669 
alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.776 
alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.189 
'''


# In[ ]:


# 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력
sort_column = 'alpha:'+str(lasso_alphas[0]) # 0.07만!
coeff_lasso_df = coeff_lasso_df.sort_values(by=sort_column, ascending=False) # 0번째 column(alpha:0.07)을 기준으로 ascending=False : 값이 감소하는 거니까 '내림차순'


# ### 엘라스틱넷 회귀

# In[ ]:


# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출
# l1_ratio는 0.7로 고정
elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]
coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alphas, X_data_n=X_data, y_target_n=y_target)
'''
#######  ElasticNet #######
alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.542 
alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.526 
alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.467 
alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.597 
alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.068 
'''


# In[ ]:


# 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력
sort_column = 'alpha:'+str(elastic_alphas[0])
coeff_elastic_df = coeff_elastic_df.sort_values(by=sort_column, ascending=False)


# ### 선형 회귀 모델을 위한 데이터 변환

# In[ ]:


print(y_target.shape)
plt.hist(y_target, bins=10)


# In[ ]:


from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures

# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정
# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. 
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)
    else:
        scaled_data = input_data

    if p_degree != None: # 다항회귀로 만들어준다고 한다면
        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)
    
    return scaled_data


# In[ ]:


# Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. 
alphas = [0.1, 1, 10, 100]
#변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성
# 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 
scale_methods=[(None, None), ('Standard', None), ('Standard', 2), 
               ('MinMax', None), ('MinMax', 2), ('Log', None), ('Log', 2)]
for scale_method in scale_methods:
    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data)
    print('\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))
    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False) # 각 변환 유형마다 alpha값별 RMSE 출력
    '''
    THE BEST PERFORMANCE : ('Log', 2)
    
    ## 변환 유형:None, Polynomial Degree:None
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.788 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.653 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.518 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.330 

    ## 변환 유형:Standard, Polynomial Degree:None
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.826 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.803 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.637 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.421 

    ## 변환 유형:Standard, Polynomial Degree:2
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 8.827 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 6.871 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.485 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.634 

    ## 변환 유형:MinMax, Polynomial Degree:None
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.764 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.465 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.754 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 7.635 

    ## 변환 유형:MinMax, Polynomial Degree:2
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.298 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.323 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.185 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.538 

    ## 변환 유형:Log, Polynomial Degree:None
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 4.770 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.676 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.836 
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.241 

    ## 변환 유형:Log, Polynomial Degree:2
    alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 9.547 
    alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.847 
    alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.270  <---THE BEST!
    alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.559 
    '''


# In[ ]:



X = np.arange(6).reshape(3, 2)
poly = PolynomialFeatures(3)
poly.fit_transform(X)

■■■■■■■■■■■■■0602 수업中 새로 알게된것■■■■■■■■■■■■■

로지스틱 회귀
- 선형 회귀 방식을 분류에 적용한 알고리즘
- 활성화함수 자리에 시그모이드 함수 Φ(z) = 1 / (1+e^-z) 넣음

시그모이드 함수
- Φ(z) = 1 / (1+e^-z)
	z→-∞ 일 때 Φ(z)→0으로 수렴
	z→+∞ 일 때 Φ(z)→1으로 수렴
- 확률값을 뱉어줌_____z=z1일 때 Φ(z)=1일 확률은 Φ(z1), Φ(z)=0일 확률은 1-Φ(z1)이구나~!
- 실제 정답값 y가 연속이 아닌 "분류"일 때, 예측값 y^(==z)를 나타내려면
	- 1차, 2차, 3차, ...함수로는 어려움
	- 시그모이드함수로는 쉬움 ====> 이진분류 뿐 아니라 멀티분류도 가능. 
		 <멀티>	1차 학습	2차 학습	3차 학습
		    감 	(1)	(0)	(0)
		    귤 	(0)	(1)	(0)
		    배 	(0)	(0)	(1)
		1차 학습된 모델은 감이 될 확률이 0.45라고 한다. (일단 얘는 0.5도 안 됨^^)
		2차 학습된 모델은 귤이 될 확률이 0.67라고 한다. 
		3차 학습된 모델은 배이 될 확률이 0.61라고 한다.
		 => 셋 중 가장 높은 확률을 가진 귤을 선택. 예측값은 '귤'을 뱉는다!!

회귀 트리
- 트리를 기반으로 하는 회귀 : 회귀트리 이용
- 모든 트리 기반 알고리즘은 분류 트리, 회귀 트리 both 사용 가능. ex_결정트리, 랜덤포레스트, GBM, XGBoost, LightGBM

분류 트리 vs 회귀 트리
- 분류 트리 > 예측값→ 이산적인 결정값 (ex_setosa, versicolor, verginica)
	   > 노드→ 지니 계수를 최대한 낮추는 방향으로 설정
- 회귀 트리 > 예측값→ 연속적인 결정값들의 평균 (ex_0.2, 1.07, ...의 평균 A,  5.3, 3.9, ...의 평균 B,  ...) 
	   > 노드→ RSS(제곱오차합) 비용을 최대한 낮추는 방향으로 설정

===========================================================

# ## 로지스틱 회귀

# In[19]:


import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer = load_breast_cancer()


# In[20]:


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환
scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data)

X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)


# In[21]:


from sklearn.metrics import accuracy_score, roc_auc_score

# 로지스틱 회귀를 이용하여 학습 및 예측 수행. 
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_preds = lr_clf.predict(X_test)

# accuracy와 roc_auc 측정
print('accuracy: {:0.3f}'.format(accuracy_score(y_test, lr_preds)))
print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test , lr_preds)))


# In[22]:


from sklearn.model_selection import GridSearchCV # 교차검증 수행하며 자동적으로 최적의 파라미터 찾아줌

params={'penalty':['l2', 'l1'], 
        'C':[0.01, 0.1, 1, 1, 5, 10]} 
# 규제 유형을 l2(ridge) or l1(lasso) 로 하겠다
# C = 1/alpha  ==>  alpha = [100, 10, 1, 1, 0.2, 0.1]

grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )
grid_clf.fit(data_scaled, cancer.target)
print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))


# ## 5.8 회귀 트리

# In[ ]:


from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import numpy as np

# 보스턴 데이터 세트 로드
boston = load_boston()
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
y_target = bostonDF['PRICE'] # 레이블 데이터 세트 - 나중에 train_test_split 할 것
X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False) # 피처 데이터 세트 - 나중에 train_test_split 할 것


rf = RandomForestRegressor(random_state=0, n_estimators=1000) # 랜덤포레스트 트리를 기반으로 회귀 estimator rf 선언
neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring="neg_mean_squared_error", cv = 5) # rf로 k폴드 교차검증
rmse_scores  = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))
print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2))
print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))


# In[ ]:


def get_model_cv_prediction(model, X_data, y_target): # 각 모델, X_data, y_target마다 train_test_split 해서 교차검증 후 퍼포먼스 결과까지 출력하는 함수
    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv = 5) # 여기에서 5개의 교차검증마다 X_train, y_train, X_test, y_test 알아서 나눠지고 모델 만들어짐. 
    rmse_scores  = np.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores) # 교차검증이니까 각 fold마다 나온 결과를 평균해줌
    print('##### ',model.__class__.__name__ , ' #####')
    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))


# ** 사이킷런의 여러 회귀 트리 클래스를 이용하여 회귀 예측 **

# In[ ]:


from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)
gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(n_estimators=1000)
lgb_reg = LGBMRegressor(n_estimators=1000)

# 트리 기반의 회귀 모델을 반복하면서 평가 수행 
models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] # estimator 클래스 객체들 
for model in models:  
    get_model_cv_prediction(model, X_data, y_target) # 아까 위에서 X_data:레이블 데이터 세트, y_target:피처 데이터 세트 둘로 나눔 (train_test_split은 안에서!)
'''
#####  DecisionTreeRegressor  #####
 5 교차 검증의 평균 RMSE : 5.978 
#####  RandomForestRegressor  #####
 5 교차 검증의 평균 RMSE : 4.423 
#####  GradientBoostingRegressor  #####
 5 교차 검증의 평균 RMSE : 4.269 
#####  XGBRegressor  #####
 5 교차 검증의 평균 RMSE : 4.251 
#####  LGBMRegressor  #####
 5 교차 검증의 평균 RMSE : 4.646 
'''



# ** 회귀 트리는 선형 회귀의 회귀 계수 대신, 피처 중요도로 피처의 상대적 중요도를 알 수 있습니다. **

# In[ ]:


import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

rf_reg = RandomForestRegressor(n_estimators=1000)

# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   
rf_reg.fit(X_data, y_target)

feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )
    # feature_importances_ : 트리 만들 때 어떤 feature가 가장 결정적으로 영향을 미쳤는가 (예전에 이거 막대그래프로 만들어봤었음)
feature_series = feature_series.sort_values(ascending=False) # ascending :오름차순. True-오름차순, Falst-내림차순
sns.barplot(x= feature_series, y=feature_series.index) # 이번에도 순서대로 barplot 막대그래프 그림. 
'''
RM: 방개수, LSTAT: 빈곤층..
트리 기반의 회귀를 사용했고, 그 결과로 RM, LSTAT의 영향력이 아주 크다는 걸 알 수 있다. 
생각해보면 그 전 선형 회귀에서도 얘네가 꽤 높은 비중이었었다. 
'''

# ** 오버피팅을 시각화 하기 위해 한개의 피처 RM과 타겟값 PRICE기반으로 회귀 예측 수행 **

# In[ ]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

bostonDF_sample = bostonDF[['RM','PRICE']] # shape : (506, 2). 피처값은 연습이니까 보기 편하게 RM 하나만 가져옴. (PRICE는 target데이터)
bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0) # 랜덤하게 100개만 샘플링해서 리턴
print(bostonDF_sample.shape) # shape : (100, 2)
plt.figure()
plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c="darkorange") # x축-RM, y축-PRICE
# plt.scatter(bostonDF_sample['RM'] , bostonDF_sample['PRICE'],c="darkorange") 얘와 똑같음. 


# In[ ]:


import numpy as np
from sklearn.linear_model import LinearRegression

# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7
lr_reg = LinearRegression()
rf_reg2 = DecisionTreeRegressor(max_depth=2) # Classifier때와 똑같이 max_depth 정해줘서 과적합 막을 수 있음. 
rf_reg7 = DecisionTreeRegressor(max_depth=7)

# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. 
X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1) # X_test. 1차원데이터를 2차원으로 바꿈. 

# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출
X_feature = bostonDF_sample['RM'].values.reshape(-1,1) # X_train
y_target = bostonDF_sample['PRICE'].values.reshape(-1,1) # y_train

# 학습과 예측 수행. 
lr_reg.fit(X_feature, y_target) # RM에 따라 가장 낮은 MSE값을 갖도록 하는 회귀선 만들 것임. 
rf_reg2.fit(X_feature, y_target) # RM에다가 조건식(트리의 노드)을 막 부여해서 ~이만큼 잘라봐, ~이만큼 잘라봐.. ~요기의 평균값, ~요기의 평균값... 
rf_reg7.fit(X_feature, y_target)

pred_lr = lr_reg.predict(X_test)
pred_rf2 = rf_reg2.predict(X_test)
pred_rf7 = rf_reg7.predict(X_test)


# In[ ]:


fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3) # 하나의 그림판에 3개의 subplot 그릴 것임. 

# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화
# 선형 회귀로 학습된 모델 회귀 예측선 
ax1.set_title('Linear Regression')
ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange") # y_test를 scatter찍음. y_test는 동일하므로 셋 다 scatter는 똑같음. 
ax1.plot(X_test, pred_lr,label="linear", linewidth=2 )

# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 
ax2.set_title('Decision Tree Regression: \n max_depth=2')
ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax2.plot(X_test, pred_rf2, label="max_depth:3", linewidth=2 )
'''세 번 꺾인 것으로 보아 조건 3개로 나누어졌구나.'''

# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 
ax3.set_title('Decision Tree Regression: \n max_depth=7')
ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax3.plot(X_test, pred_rf7, label="max_depth:7", linewidth=2)
'''트리가 너~무 깊어서 조건이 너~무 세분화되었다. 과적합 Overfitting!'''

■■■■■■■■■■■■■0603 수업中 새로 알게된것■■■■■■■■■■■■■

################## 회귀 실습 - 자전거 대여 수요 예측 ##################

# ## 5.9 Regression 실습 - Bike Sharing Demand
# ### 데이터 클렌징 및 가공

# In[1]:


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)

bike_df = pd.read_csv('C:/jeon/bike_train.csv') # 여러 날씨 데이터 정보에 기반한 1시간 간격 동안의 자전거 대여 횟수
print(bike_df.shape)
bike_df.head(3)


# datetime: hourly date + timestamp  
# season: 1 = 봄, 2 = 여름, 3 = 가을, 4 = 겨울  
# holiday: 1 = 토, 일요일의 주말을 제외한 국경일 등의 휴일, 0 = 휴일이 아닌 날  
# workingday: 1 = 토, 일요일의 주말 및 휴일이 아닌 주중, 0 = 주말 및 휴일  
# weather:  
# • 1 = 맑음, 약간 구름 낀 흐림  
# • 2 = 안개, 안개 + 흐림  
# • 3 = 가벼운 눈, 가벼운 비 + 천둥  
# • 4 = 심한 눈/비, 천둥/번개  
# temp: 온도(섭씨)   
# atemp: 체감온도(섭씨)  
# humidity: 상대습도  
# windspeed: 풍속  
# casual: 사전에 등록되지 않는 사용자가 대여한 횟수  
# registered: 사전에 등록된 사용자가 대여한 횟수  
# count: 대여 횟수  

# In[2]:


bike_df.info()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10886 entries, 0 to 10885
Data columns (total 12 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   datetime    10886 non-null  object  # 얘만 string
 1   season      10886 non-null  int64  
 2   holiday     10886 non-null  int64  
 3   workingday  10886 non-null  int64  
 4   weather     10886 non-null  int64  
 5   temp        10886 non-null  float64
 6   atemp       10886 non-null  float64
 7   humidity    10886 non-null  int64  
 8   windspeed   10886 non-null  float64
 9   casual      10886 non-null  int64  
 10  registered  10886 non-null  int64  
 11  count       10886 non-null  int64  
dtypes: float64(3), int64(8), object(1)
'''


# In[3]:


# 문자열을 datetime 타입으로 변경. 
bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime) # bike_df.datetime == bike_df['datetime']
    # pd.to_datetime를 써서 → datetime type(pandas, 원하는 날짜와 시간만 뽑아 사용 가능)으로 바꿈. 
bike_df.info()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10886 entries, 0 to 10885
Data columns (total 12 columns):
 #   Column      Non-Null Count  Dtype         
---  ------      --------------  -----         
 0   datetime    10886 non-null  datetime64[ns]   # 이렇게 바뀜!
 1   season      10886 non-null  int64         
 2   holiday     10886 non-null  int64         
 3   workingday  10886 non-null  int64         
 4   weather     10886 non-null  int64         
 5   temp        10886 non-null  float64       
 6   atemp       10886 non-null  float64       
 7   humidity    10886 non-null  int64         
 8   windspeed   10886 non-null  float64       
 9   casual      10886 non-null  int64         
 10  registered  10886 non-null  int64         
 11  count       10886 non-null  int64         
dtypes: datetime64[ns](1), float64(3), int64(8)
'''


# In[4]:


# datetime 타입에서 년, 월, 일, 시간 추출
bike_df['year'] = bike_df.datetime.apply(lambda x : x.year) # bike_df 데이터프레임 안에서 datetime 컬럼의 요소들을 순차적으로 x에 넣어주고, 
                                                            # 그때 x.year를 새로 만든 bike_df['year'] 컬럼에 요소로 하나씩 넣어준다. 
                                                                # -> datetime 으로 타입 캐스팅 해줘서 굳이 인덱스 슬라이싱 안 해주고 편함!!
bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)
bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)
bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)
print(bike_df.info()) # 'year', 'month', 'day', 'hour' 4개의 컬럼 추가로 만들어짐. 
bike_df.head(3)


# In[5]:


drop_columns = ['datetime','casual','registered'] # 'datetime'-따로 담아줬으니까 삭제, 'casual','registered'-둘이 합친 게 count로 들어가 있고 딱히 필요없어 보이니까 삭제
bike_df.drop(drop_columns, axis=1,inplace=True)


# ### 로그 변환, 피처 인코딩, 모델 학습/예측/평가 

# In[6]:


from sklearn.metrics import mean_squared_error, mean_absolute_error

# log 값 변환 시 언더플로우 영향으로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산
def rmsle(y, pred): # log1p된 오류 값에 해당하는 rmse : 결괏값, 예측값에 각각 log를 씌움(loga-logb != log(a-b))
    log_y = np.log1p(y) # y 로그캐스팅
    log_pred = np.log1p(pred) # y^ 로그캐스팅
    squared_error = (log_y - log_pred) ** 2 # error == logy-logy^
    rmsle = np.sqrt(np.mean(squared_error)) # log1p된 오류 값에 해당하는 rmse
    return rmsle

# 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산
def rmse(y,pred):
    return np.sqrt(mean_squared_error(y,pred)) # sqrt: 루트. mean_squared_error(y, y^)로 MSE 구하고 루트씌움. 

# RMSLE, RMSE, MAE 를 모두 계산 
def evaluate_regr(y,pred):
    rmsle_val = rmsle(y,pred)
    rmse_val = rmse(y,pred)
    # MAE 는 scikit learn의 mean_absolute_error() 로 계산
    mae_val = mean_absolute_error(y,pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, mae_val))


# In[7]:


from sklearn.model_selection import train_test_split , GridSearchCV
from sklearn.linear_model import LinearRegression , Ridge , Lasso

y_target = bike_df['count']
X_features = bike_df.drop(['count'],axis=1,inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

evaluate_regr(y_test ,pred)
'''
RMSLE: 1.165, RMSE: 140.900, MAE: 105.924

MAE와 MSE를 바로 비교하면 절댓값과 제곱을 비교하는 것이기 때문에, 
지금처럼 단순 직접 비교로는 MAE와 RMSE를 비교하는 것이 맞다. 
'''


# In[8]:


def get_top_error_data(y_test, pred, n_tops = 5):
    # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. 
    result_df = pd.DataFrame(y_test.values, columns=['real_count']) # 실제값 y 담은 column 만듦
    result_df['predicted_count']= np.round(pred) # 예측값 y^ 담은 column 만듦
    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count']) # |y-y^|
    # 예측값과 실제값이 가장 큰 데이터 순으로 출력. 
    print(result_df.sort_values('diff', ascending=False)[:n_tops]) # 오류값 가장 큰 순(ascending=False:내림차순)으로 10개만 뽑아봄
    
get_top_error_data(y_test,pred,n_tops=10) # 초기값 5 있지만, 10 넣어줬으니 10이 우선. 


# In[9]:


y_target.hist() # y_target(=='count'컬럼)이 정규분포 이루는지 확인. 
plt.plot(range(0,100), y_target[0:100])
plt.scatter(range(0,100), y_target[0:100])

# In[10]:


y_log_transform = np.log1p(y_target)
y_log_transform.hist()
plt.plot(range(0,100), y_log_transform[0:100])
plt.scatter(range(0,100), y_log_transform[0:100])
'''
skewness(비대칭도)가 높다 == 데이터 분포가 한쪽으로 치우쳐있다
    → 이런 값들을 정규 분포로 바꾸는 방법 : 로그변환★
    
로그변환
- 스케일이 변한 것이지, 분포 그래프의 모양이 바뀌는 게 아님.    // a > b 이면 log1p(a) > log1p(b)
        ex_ 데이터분포 그래프를 f(x)라 할 때
                    원본 : f(70) > f(5) > f(25) > f(100)
              log1p(원본): f(70) > f(5) > f(25) > f(100) 그대로! 그냥 기울기가 완만해질 뿐!!      
- 로그변환 이후 히스토그램은 왜 모양 자체가 바뀔까??
        => 로그변환을 하면 f(x)값의 극단적인 비대칭성이 줄어들고, 새로 그리는 그래프는 step을 그 줄어든 비율만큼 짧게 잡을 것이다. 
           그러면 새로운 도수 분포는 f(x)값이 보다 고르게 퍼지고, 결국 히스토그램이 정규분포를 닮아간다. 
'''
#y_log_log_transform = np.log1p(y_log_transform)
#y_log_log_transform.hist()
#plt.plot(range(0,100), y_log_log_transform[0:100])
#plt.scatter(range(0,100), y_log_log_transform[0:100])

# In[11]:


# 타겟 컬럼인 count 값을 log1p 로 Log 변환
y_target_log = np.log1p(y_target)

# 로그 변환된 y_target_log를 반영하여 학습/테스트 데이터 셋 분할
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0) # X_features:피처데이터세트, y_target_log:
                                                                                                             # 로그변환된 아이들로 학습됐으므로, 그거에 맞춰서 w 학습됨. 
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

# 테스트 데이터 셋의 Target 값은 Log 변환되었으므로 다시 expm1를 이용하여 원래 scale로 변환
y_test_exp = np.expm1(y_test)

# 예측 값 역시 Log 변환된 타겟 기반으로 학습되어 예측되었으므로 다시 exmpl으로 scale변환
pred_exp = np.expm1(pred)

evaluate_regr(y_test_exp ,pred_exp)
'''
RMSLE: 1.017, RMSE: 162.594, MAE: 109.286

RMSLE는 줄었지만, RMSE는 오히려 더 늘어남. 
이유 찾아보자!
     ↓↓↓
'''


# In[12]:


coef = pd.Series(lr_reg.coef_, index=X_features.columns)
coef_sort = coef.sort_values(ascending=False)
sns.barplot(x=coef_sort.values, y=coef_sort.index)
'''
Year 피처의  회귀 계수 값이 독보적으로 크다. 
자전거 대여 횟수에 별 영향 없는 무의미한 값인데, 회귀에서는 숫자가 가중치 등에 큰 영향을 줌. 게다가 값이 2011, 2012로 엄청 큰 값이다. 
=> 원-핫 인코딩 해주자!

원-핫 인코딩 : 유니크한 요소의 개수만큼 컬럼 만들어서 각각 해당되면 1, 아니면 0 집어넣음. 
ex_ 2011, 2012 컬럼 추가해서 각 컬럼에 요소마다 0, 1 집어넣음
    year(2col), month(12col), day(31col), hour(24col) - 이렇게 한다면 총 69개의 컬럼 추가해서 원-핫 인코딩
    컬럼이 너무 늘어나긴 하지만,,,,ㅜㅜ
'''

# In[13]:


# 'year','month','hour','season','weather' feature들을 One Hot Encoding
X_features_ohe = pd.get_dummies(X_features, columns=['year','month','day','hour', 'holiday', 'workingday','season','weather']) # 새로운 df에 원-핫 인코딩 결과 컬럼 추가
    # 참고로, day 20부터는 bike_test.csv 파일에 있음. 
    # 지금은 bike_train.csv 파일 안에서만 실행하고 코드 다룰 것이기 때문에, bike_test.csv와 합칠 걱정 하지 않고 그냥 마음대로 원-핫 인코딩 해줬음. 

# In[14]:


# 원-핫 인코딩이 적용된 feature 데이터 세트 기반으로 다시 학습/예측 데이터 분할. 
X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0)

# 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환
def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):
    model.fit(X_train, y_train) # 로그변환된 아이들로 학습됐으므로, 그거에 맞춰서 w 학습됨.
    pred = model.predict(X_test) # 예측도 로그 변환 영향 받음. 
    if is_expm1 :
        y_test = np.expm1(y_test) # 로그함수 log(x)의 반대 지수함수 exp(x), log1p(x)의 반대 expm1(x)
        pred = np.expm1(pred)
    print('###',model.__class__.__name__,'###')
    evaluate_regr(y_test, pred)
# end of function get_model_predict    

# model 별로 평가 수행
lr_reg = LinearRegression()
ridge_reg = Ridge(alpha=10)
lasso_reg = Lasso(alpha=0.01)

for model in [lr_reg, ridge_reg, lasso_reg]:
    get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True)
'''
### LinearRegression ###
RMSLE: 0.590, RMSE: 97.688, MAE: 63.382
### Ridge ###
RMSLE: 0.590, RMSE: 98.529, MAE: 63.893
### Lasso ###
RMSLE: 0.635, RMSE: 113.219, MAE: 72.803
'''


# In[15]:


coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns)
coef_sort = coef.sort_values(ascending=False)[:10]
sns.barplot(x=coef_sort.values , y=coef_sort.index)
'''
이제는 좀 영향력 있는 컬럼 순으로 나옴. 
month_9 - month_8 - month_7 - month_5 - month_6 - month_4 - workingday_0 - workingday_1 - month_10 - month_11 - ....
'''


# In[16]:


from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor


# 랜덤 포레스트, GBM, XGBoost, LightGBM model 별로 평가 수행
rf_reg = RandomForestRegressor(n_estimators=500)
gbm_reg = GradientBoostingRegressor(n_estimators=500)
xgb_reg = XGBRegressor(n_estimators=500)
lgbm_reg = LGBMRegressor(n_estimators=500)

for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:
    get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True)
'''
### RandomForestRegressor ###
RMSLE: 0.355, RMSE: 50.321, MAE: 31.134
### GradientBoostingRegressor ###
RMSLE: 0.330, RMSE: 53.349, MAE: 32.744
### XGBRegressor ###
RMSLE: 0.342, RMSE: 51.732, MAE: 31.251
### LGBMRegressor ###
RMSLE: 0.319, RMSE: 47.215, MAE: 29.029

아까 LinearRegression, Ridge, lasso 로 평가한 것보다 훨~씬 결과 좋음!! ^-^ 확실히 최신에 나온 게 퍼포먼스가 좋음~~
'''

=============================================================



import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew # 비대칭도(skewness)에 대한 값 뱉어줌. 

sample_size = 500

X = np.random.normal(0, 5, sample_size) # 평균 0 표준편차 5인 데이터 500개 뽑아줌
    # random.randn(m, n) : 평균 0 표준편차 1인 정규분포를 (m,n)만큼 뽑아준다. 
    # random.normal(평균, 표준편차, size) : 구체적으로 평균과 표준편차를 알려줌!
sns.distplot(X)
print(abs(X.min()))
'''12.134544252814834'''

X = X + abs(X.min()) # |최솟값|을 X벡터에 모두 더해줌 => 최솟값이 0이 되도록 shift !
sns.distplot(X)

r = np.random.normal(0, 2, sample_size)
sns.distplot(r)

Y = X * 1.0 + r + abs(r.min()) 
    # y=x 그래프에 r라는 noise 정규분포를 또 줘서 scatter 주변에 막 찍히게 해줌. 
    # 대신 y값이 음수가 나오지 않도록 r의 최솟값도 또 더해줘서 shift시켜줌. 
plt.scatter(X,Y)

df = pd.DataFrame({'X':X, 'Y':Y})
print(df.head())
'''        X          Y
0  21.482822  31.258772
1  19.002501  26.392140
2  14.705231  25.367837
3  16.741191  22.972485
4  10.085656  22.446603
'''

sns.jointplot(x='X', y='Y', data=df, alpha=0.5)
#plt.savefig('../../assets/images/markdown_img/180605_1519_resolve_skewness_scatter_plot.svg')
plt.show()
'''
jointplot() : scatter 그래프에 X축, Y축에 대한 히스토그램까지 함께 보여줌. 
sample_size가 엄청 커지면 도수분포도 그냥 scatter 그래프로만은 판단하기 어렵기 때문에 유용. 
'''

sqr_vs = [0.2, 0.5, 1.0, 2.0, 3.0]

f, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, j in enumerate(sqr_vs):
    axes[i].set_title('X ** {} \nskewness = {:.2f}'.format(j, skew(df['X']**j)))
    sns.distplot(df['X']**j, ax=axes[i], kde=False)
#plt.savefig('../../assets/images/markdown_img/180605_1517_resolve_skewness_compare.svg')
plt.show()
'''
skewness의 절댓값이 커질수록 비대칭성 커짐. 
skewness가 0에 가까울수록 표준정규분포와 가까워짐. 
'''

■■■■■■■■■■■■■0604 수업中 새로 알게된것■■■■■■■■■■■■■

################ 회귀 실습 - 캐글 주택 가격: 고급 회귀 기법 ################

# ### 데이터 사전 처리(Preprocessing)

# In[1]:


import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

house_df_org = pd.read_csv('C:/jeon/house_price_train.csv')
house_df = house_df_org.copy()
house_df.head(3)


# 1stFlrSF: First Floor square feet
# 
# 2ndFlrSF: Second floor square feet
# 
# GrLivArea: Above grade (ground) living area square feet
#     
# Bedroom: Bedrooms above grade (does NOT include basement bedrooms)
# 
# LotArea: Lot size in square feet
#     
# GarageArea: Size of garage in square feet
#     
# OverallQual: Rates the overall material and finish of the house
# 
#        10	Very Excellent
#        9	Excellent
#        8	Very Good
#        7	Good
#        6	Above Average
#        5	Average
#        4	Below Average
#        3	Fair
#        2	Poor
#        1	Very Poor
# 	
# OverallCond: Rates the overall condition of the house
# 
#        10	Very Excellent
#        9	Excellent
#        8	Very Good
#        7	Good
#        6	Above Average	
#        5	Average
#        4	Below Average	
#        3	Fair
#        2	Poor
#        1	Very Poor
# 
# YearBuilt: Original construction date
# 
# Neighborhood: Physical locations within Ames city limits
# 
#    Blmngtn  Bloomington Heights
#    Blueste  Bluestem
#    BrDale   Briardale
#    .....
# 
# RoofMatl: Roof material
# 	ClyTile	Clay or Tile
# 	CompShg	Standard (Composite) Shingle
# 	Membran	Membrane
# 	Metal	Metal
# 	.....
# 
# RoofStyle: Type of roof
# 	Flat	Flat
# 	Gable	Gable
# 	Gambrel	Gabrel (Barn)
# 	.....

# ** 데이터 타입과 Null 값 갯수 확인 **

# In[2]:


house_df.info()


# In[3]:


print('데이터 세트의 Shape:', house_df.shape)
test = house_df.dtypes # index : df의 각 컬럼 네임, 시리즈 값 : 컬럼별 데이터 타입(하나의 컬럼에는 하나의 dtype만!)
print('\n전체 feature 들의 type \n',house_df.dtypes.value_counts())
'''
전체 feature 들의 type 
 object     43  => string은 그대로 못 넣으니까 뭔가 처리 해줘야겠군!
int64      35
float64     3
dtype: int64
'''
isnull_series = house_df.isnull() # 똑같은 데이터프레임 형태로, 각 요소에는 boolean값 들어감. (null이면 True, 아니면 False)
isnull_series = house_df.isnull().sum() # 컬럼별 True의 개수
print('\nNull 컬럼과 그 건수:\n ', isnull_series[isnull_series > 0].sort_values(ascending=False)) # 불린인덱싱
'''
Null 컬럼과 그 건수:
  PoolQC          1453
MiscFeature     1406
Alley           1369
Fence           1179
FireplaceQu      690
LotFrontage      259
GarageYrBlt       81
GarageType        81
GarageFinish      81
GarageQual        81
GarageCond        81
BsmtFinType2      38
BsmtExposure      38
BsmtFinType1      37
BsmtCond          37
BsmtQual          37
MasVnrArea         8
MasVnrType         8
Electrical         1
dtype: int64
'''

# ** 타겟값 SalePrice의 분포도 확인 **

# In[4]:


plt.title('Original Sale Price Histogram')
sns.distplot(house_df['SalePrice']) # 레이블값 


# ** 로그 변환을 통해 SalePrice 값 분포도 확인 **

# In[5]:


plt.title('Log Transformed Sale Price Histogram')
log_SalePrice = np.log1p(house_df['SalePrice']) # 로그변환으로 조금 더 정규분포와 가깝게 만듦
sns.distplot(log_SalePrice)


# ** 타겟값인 Price를 로그변환하여 정규 분포 형태로 변환하고, 피처들 중 숫자형 컬럼의 Null값 데이터 처리 **

# In[6]:


# SalePrice 로그 변환
original_SalePrice = house_df['SalePrice']
house_df['SalePrice'] = np.log1p(house_df['SalePrice']) # 원본 복사한 house_df의 'SalePrice'컬럼 수정

# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제
house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True)

# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체
house_df.fillna(house_df.mean(),inplace=True)

# Null 값이 있는 피처명과 타입을 추출
null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]
    # [house_df.isnull().sum() > 0] : 불린인덱싱 - null값이 1개 이상 있는 컬럼 인덱스-True, 없으면-False 넣어줌. 
    # house_df.isnull().sum()에서 위의 불린인덱싱으로 인덱스 집어넣으면, True인 row(index에 컬럼네임 들어있음)만 가져옴. 
print('## Null 피처의 Type :\n', house_df.dtypes[null_column_count.index])
    # house_df.dtypes : 전체 컬럼의 데이터 타입을 시리즈로, 인덱스-컬럼네임
    # 거기에서 [null_column_count.index], 즉 null값이 있는 컬럼들(index가 컬럼네임이었음)만 골라서 dtype 알려줌
'''
숫자들은 다 평균값으로 채워줬기 때문에, 이제 남은 건 object타입밖에 안 나옴 

## Null 피처의 Type :
 MasVnrType      object
BsmtQual        object
BsmtCond        object
BsmtExposure    object
BsmtFinType1    object
BsmtFinType2    object
Electrical      object
GarageType      object
GarageFinish    object
GarageQual      object
GarageCond      object
dtype: object
'''

# ** 문자열값은 모두 카테고리값. 판다스의 get_dummies( )를 이용하여 원-핫 인코딩 수행 **

# In[7]:

# get_dummies( ) : 원-핫 인코딩 수행 후 컬럼들이 추가되어 만들어진 새로운 df를 리턴 .
print('get_dummies() 수행 전 데이터 Shape:', house_df.shape)
house_df_ohe = pd.get_dummies(house_df)
print('get_dummies() 수행 후 데이터 Shape:', house_df_ohe.shape)

# 이제 null값이 없음을 확인
null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]
print('## Null 피처의 Type :\n', house_df_ohe.dtypes[null_column_count.index])


# ### 선형 회귀 모델의 학습/예측/평가

# ** RMSE 평가 함수 생성 **

# In[8]:


def get_rmse(model):
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test , pred)
    rmse = np.sqrt(mse)
    print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))
    return rmse

def get_rmses(models):
    rmses = [ ]
    for model in models:
        rmse = get_rmse(model)
        rmses.append(rmse)
    return rmses


# ** LinearRegression, Ridge, Lasso 학습, 예측, 평가 ** 

# In[9]:


from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)

# LinearRegression, Ridge, Lasso 학습, 예측, 평가
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)

ridge_reg = Ridge()
ridge_reg.fit(X_train, y_train)

lasso_reg = Lasso()
lasso_reg.fit(X_train, y_train)

models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
'''
LinearRegression 로그 변환된 RMSE: 0.132
Ridge 로그 변환된 RMSE: 0.128
Lasso 로그 변환된 RMSE: 0.176
'''


# ** 회귀 계수값과 컬럼명 시각화를 위해 상위 10개, 하위 10개(-값으로 가장 큰 10개) 회귀 계수값과 컬럼명을 가지는 Series생성 함수. **  

# In[10]:


def get_top_bottom_coef(model):
    # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. 
    coef = pd.Series(model.coef_, index=X_features.columns) # 회귀계수를 담고(총 271개일 것), 그 인덱스는 컬럼네임으로 하는 Series 만듦
    
    # 이따가 20개만 barplot으로 한눈에 확인해보려고 + 상위 10개 , - 하위 10개 coefficient 추출하여 반환.
    coef_high = coef.sort_values(ascending=False).head(10)
    coef_low = coef.sort_values(ascending=False).tail(10)
    return coef_high, coef_low


# ** 인자로 입력되는 여러개의 회귀 모델들에 대한 회귀계수값과 컬럼명 시각화 **

# In[11]:


def visualize_coefficient(models):
    # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성
    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3)
    fig.tight_layout() 
    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. 
    for i_num, model in enumerate(models):
        # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. 
        coef_high, coef_low = get_top_bottom_coef(model)
        coef_concat = pd.concat( [coef_high , coef_low] ) # concat : concatenate, 물리적으로 합침
        
        # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정. 
        axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size=25)
        axs[i_num].tick_params(axis="y",direction="in", pad=-120)
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num])
        '''
        회귀계수는 LinearRegression이 Ridge, Lasso보다 더 큼. 
        지금 그래프는 스케일링이 달라서 Ridge가 더 커보일 뿐 값은 더 작다. 
        
        당연히 규제는 w가 커지지 않도록 하는 비용함수를 가지므로, w는 LinearRegression이 클 수밖에!
        '''

# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 회귀 계수 시각화.    
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
'''
LinearRegression 로그 변환된 RMSE: 0.132
Ridge 로그 변환된 RMSE: 0.128
Lasso 로그 변환된 RMSE: 0.176

cf. LinearRegression의 과적합 해결하기 위해 Ridge, Lasso 등의 규제 만듦. 
'''


# ** 5 폴드 교차검증으로 모델별로 RMSE와 평균 RMSE출력 **

# In[12]:


from sklearn.model_selection import cross_val_score # 교차점증

def get_avg_rmse_cv(models): # 모델별 교차검증
    for model in models:
        # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력
        rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target,
                                             scoring="neg_mean_squared_error", cv = 5))
        rmse_avg = np.mean(rmse_list)
        print('\n{0} CV RMSE 값 리스트: {1}'.format( model.__class__.__name__, np.round(rmse_list, 3)))
        print('{0} CV 평균 RMSE 값: {1}'.format( model.__class__.__name__, np.round(rmse_avg, 3)))

# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력           
models = [lr_reg, ridge_reg, lasso_reg]
get_avg_rmse_cv(models)
'''
LinearRegression CV RMSE 값 리스트: [0.135 0.165 0.168 0.111 0.198]
LinearRegression CV 평균 RMSE 값: 0.155

Ridge CV RMSE 값 리스트: [0.117 0.154 0.142 0.117 0.189]
Ridge CV 평균 RMSE 값: 0.144

Lasso CV RMSE 값 리스트: [0.161 0.204 0.177 0.181 0.265]
Lasso CV 평균 RMSE 값: 0.198
'''

# ** 각 모델들의 alpha값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재 학습/예측/평가 ** 

# In[13]:



from sklearn.model_selection import GridSearchCV # 최적의 파라미터 찾기

def get_best_params(model, params): # 교차검증 기반으로 퍼포먼스 측정했을 때, 최적의 alpha값 찾기. 
    grid_model = GridSearchCV(model, param_grid=params, 
                              scoring='neg_mean_squared_error', cv=5)
    grid_model.fit(X_features, y_target)
    rmse = np.sqrt(-1* grid_model.best_score_)
    print('{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}'.format(model.__class__.__name__,
                                        np.round(rmse, 4), grid_model.best_params_))
    return grid_model.best_estimator_

ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }
lasso_params = { 'alpha':[0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.005] }
best_rige = get_best_params(ridge_reg, ridge_params)
'''Ridge 5 CV 시 최적 평균 RMSE 값: 0.1418, 최적 alpha:{'alpha': 12}'''
best_lasso = get_best_params(lasso_reg, lasso_params)
'''Lasso 5 CV 시 최적 평균 RMSE 값: 0.14, 최적 alpha:{'alpha': 0.0005}'''


# In[16]:


# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. 
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
ridge_reg = Ridge(alpha=12)
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso(alpha=0.0005)
lasso_reg.fit(X_train, y_train)

# 모든 모델의 RMSE 출력
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
'''
LinearRegression 로그 변환된 RMSE: 0.132
Ridge 로그 변환된 RMSE: 0.124
Lasso 로그 변환된 RMSE: 0.115
'''

# 모든 모델의 회귀 계수 시각화 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
'''
예측 성능 더 좋아짐.
'''


# ** 숫자 피처들에 대한 데이터 분포 왜곡도 확인 후 높은 왜곡도를 가지는 피처 추출 **

# In[14]:


from scipy.stats import skew

# object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출.
features_index = house_df.dtypes[house_df.dtypes != 'object'].index
    # skew()를 적용하는 숫자형 피쳐에서 원-핫 인코딩된 카테고리 숫자형 피처는 제외해야 함
    #   → 숫자값이 무의미함 + 히스토그램 그리면 비대칭성 클 수밖에 없기 때문
    # 그래서 house_df_ohe 말고 house_df 사용

# house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 
skew_features = house_df[features_index].apply(lambda x : skew(x)) # house_df에서 features_index에 들어있는 인덱스의 컬럼들만. 
    # df.apply(lambda x : ~ , axis=0(default))   => df 안의 column 하나하나★
    # df.apply(lambda x : ~ , axis=1)            => df 안의 row 하나하나★ (보통 row방향은 axis=0인데, apply lambda에서는 axis=1)

# skew 정도가 1 이상(왜곡 정도가 높다 == skew() > 1)인 컬럼들만 추출. 
skew_features_top = skew_features[skew_features > 1]
print(skew_features_top.sort_values(ascending=False))
'''
MiscVal          24.451640
PoolArea         14.813135
LotArea          12.195142
3SsnPorch        10.293752
LowQualFinSF      9.002080
KitchenAbvGr      4.483784
BsmtFinSF2        4.250888
ScreenPorch       4.117977
BsmtHalfBath      4.099186
EnclosedPorch     3.086696
MasVnrArea        2.673661
LotFrontage       2.382499
OpenPorchSF       2.361912
BsmtFinSF1        1.683771
WoodDeckSF        1.539792
TotalBsmtSF       1.522688
MSSubClass        1.406210
1stFlrSF          1.375342
GrLivArea         1.365156
dtype: float64
'''


# ** 왜곡도가 1인 피처들은 로그 변환 적용하고 다시 하이퍼 파라미터 튜닝 후 재 학습/예측/평가 **

# In[18]:


house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])


# In[19]:


# Skew가 높은 피처들을 로그 변환 했으므로 다시 원-핫 인코딩 적용 및 피처/타겟 데이터 셋 생성,
house_df_ohe = pd.get_dummies(house_df)
y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)

# 피처들을 로그 변환 후 다시 한번 최적 하이퍼 파라미터와 RMSE 찾아서 출력
ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }
lasso_params = { 'alpha':[0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.005] }
best_ridge = get_best_params(ridge_reg, ridge_params)
'''Ridge 5 CV 시 최적 평균 RMSE 값: 0.1275, 최적 alpha:{'alpha': 10}'''
best_lasso = get_best_params(lasso_reg, lasso_params)
'''Lasso 5 CV 시 최적 평균 RMSE 값: 0.1238, 최적 alpha:{'alpha': 0.0005}'''


# In[20]:


# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. 
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
ridge_reg = Ridge(alpha=10)
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso(alpha=0.0005)
lasso_reg.fit(X_train, y_train)

# 모든 모델의 RMSE 출력
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
'''
LinearRegression 로그 변환된 RMSE: 0.128     → 줄어듦
Ridge 로그 변환된 RMSE: 0.122                → 줄어듦
Lasso 로그 변환된 RMSE: 0.116                → 늘어남
'''

# 모든 모델의 회귀 계수 시각화 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)


# ** 이상치 데이터 검출을 위해 주요 피처인 GrLivArea값에 대한 산포도 확인 **

# In[21]:


plt.scatter(x = house_df_org['GrLivArea'], y = house_df_org['SalePrice'])
plt.ylabel('SalePrice', fontsize=15)
plt.xlabel('GrLivArea', fontsize=15)
plt.show()
'''
GrLivArea : 주거 공간의 크기

[이상한 값 발견 !]
논리적으로는, 주거 공간의 크기가 커질 수록 가격이 늘어나야 하는데, 
몇몇 데이터는 크기가 엄청나게 커도 가격이 낮은 집이 있다. 
=> 실제 값일 수도 있지만, 뭔가 오류가 있을 수 있다! => 뭐든 상관 없다. 제거하자!!

이런 outlier만 제거해줘도 퍼포먼스 굉장히 높아짐 ㅎㅎ
'''


# ** 이상치 데이터 삭제 후 재 학습/예측/평가 **

# In[ ]:


house_df_ohe['GrLivArea']


# In[22]:


# GrLivArea와 SalePrice 모두 로그 변환되었으므로 이를 반영한 조건 생성. 
cond1 = house_df_ohe['GrLivArea'] > np.log1p(4000)   # house_df_ohe['GrLivArea']는 이미 로그캐스팅 돼 있으니까
cond2 = house_df_ohe['SalePrice'] < np.log1p(500000) # house_df_ohe['SalePrice']는 이미 로그캐스팅 돼 있으니까
outlier_index = house_df_ohe[cond1 & cond2].index # 좌표에서 outlier 좌표만 포함된 범위 선택

print('아웃라이어 레코드 index :', outlier_index.values)
print('아웃라이어 삭제 전 house_df_ohe shape:', house_df_ohe.shape)
# DataFrame의 index를 이용하여 아웃라이어 레코드 삭제. 
house_df_ohe.drop(outlier_index, axis=0, inplace=True) # axis=0 : 로우를 제거
print('아웃라이어 삭제 후 house_df_ohe shape:', house_df_ohe.shape)
'''
아웃라이어 레코드 index : [ 523 1298]
아웃라이어 삭제 전 house_df_ohe shape: (1460, 271)
아웃라이어 삭제 후 house_df_ohe shape: (1458, 271) // 잘 삭제됐는지 확인 
'''


# In[23]:

# 로우 바뀌었으니, 이제 새롭게 다시 해보기! train_test_split, get_best_params
y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)

ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }
lasso_params = { 'alpha':[0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.005] }
best_ridge = get_best_params(ridge_reg, ridge_params)
best_lasso = get_best_params(lasso_reg, lasso_params)


# In[24]:


# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. 
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
ridge_reg = Ridge(alpha=8)
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso(alpha=0.0005)
lasso_reg.fit(X_train, y_train)

# 모든 모델의 RMSE 출력
models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
'''
LinearRegression 로그 변환된 RMSE: 0.129
Ridge 로그 변환된 RMSE: 0.103
Lasso 로그 변환된 RMSE: 0.101
'''

# 모든 모델의 회귀 계수 시각화 
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
'''
또 더 좋아짐!
'''


# ### 회귀 트리 학습/예측/평가 
# 
# ** XGBoost와 LightGBM 학습/예측/평가 **

# In[25]:


from xgboost import XGBRegressor

xgb_params = {'n_estimators':[1000]}
xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, 
                       colsample_bytree=0.5, subsample=0.8)
best_xgb = get_best_params(xgb_reg, xgb_params) # 최적 estimator
'''
XGBRegressor 5 CV 시 최적 평균 RMSE 값: 0.1178, 최적 alpha:{'n_estimators': 1000}
'''


# In[26]:


from lightgbm import LGBMRegressor

lgbm_params = {'n_estimators':[1000]}
lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, 
                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)
best_lgbm = get_best_params(lgbm_reg, lgbm_params) # 최적 estimator
'''
LGBMRegressor 5 CV 시 최적 평균 RMSE 값: 0.1163, 최적 alpha:{'n_estimators': 1000}
'''

# ** 트리 회귀 모델의 피처 중요도 시각화 **

# In[27]:


# 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환.
def get_top_features(model):
    ftr_importances_values = model.feature_importances_
    ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns  )
    ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] # 상위 20개만
    return ftr_top20

def visualize_ftr_importances(models):
    # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성
    fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2)
    fig.tight_layout() 
    # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. 
    for i_num, model in enumerate(models):
        # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 
        ftr_top20 = get_top_features(model)
        axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25)
        #font 크기 조정.
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num])

# 앞 예제에서 get_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화    
models = [best_xgb, best_lgbm]
visualize_ftr_importances(models) # models의 피처 중요도 보여주는 함수
'''두 모델의 피처 중요도가 완전히 다르네~!!??? 그런가보다...'''


# ### 회귀 모델들의 예측 결과 혼합을 통한 최종 예측

# In[28]:


def get_rmse_pred(preds):
    for key in preds.keys():
        pred_value = preds[key]
        mse = mean_squared_error(y_test , pred_value)
        rmse = np.sqrt(mse)
        print('{0} 모델의 RMSE: {1}'.format(key, rmse))

# 개별 모델의 학습
ridge_reg = Ridge(alpha=8)
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso(alpha=0.001)
lasso_reg.fit(X_train, y_train)
# 개별 모델 예측
ridge_pred = ridge_reg.predict(X_test)
lasso_pred = lasso_reg.predict(X_test)

# 개별 모델 예측값 "혼합"으로 최종 예측값 도출 > 약~간의 앙상블 느낌을 줬군!
pred = 0.4 * ridge_pred + 0.6 * lasso_pred # 아까 Lasso 로그 변환된 RMSE 성능이 조금 더 좋았기에, 가중치 더 줌. 
preds = {'최종 혼합': pred,
         'Ridge': ridge_pred,
         'Lasso': lasso_pred}
#최종 혼합 모델, 개별모델의 RMSE 값 출력
get_rmse_pred(preds)
'''
최종 혼합 모델의 RMSE: 0.10007930884470514
Ridge 모델의 RMSE: 0.10345177546603257
Lasso 모델의 RMSE: 0.10024170460890039
'''


# In[29]:


xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, 
                       colsample_bytree=0.5, subsample=0.8)
lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, 
                         subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)
xgb_reg.fit(X_train, y_train)
lgbm_reg.fit(X_train, y_train)
xgb_pred = xgb_reg.predict(X_test)
lgbm_pred = lgbm_reg.predict(X_test)

pred = 0.5 * xgb_pred + 0.5 * lgbm_pred
preds = {'최종 혼합': pred,
         'XGBM': xgb_pred,
         'LGBM': lgbm_pred}
        
get_rmse_pred(preds)
'''
최종 혼합 모델의 RMSE: 0.10170077353447762
XGBM 모델의 RMSE: 0.10738295638346222
LGBM 모델의 RMSE: 0.10382510019327311

오히려 더 안 좋음.. 얘네를 사용하기엔 데이터가 너무 적다. 특히 LightGBM은 어느 정도 이상의 데이터를 사용해야 성능 좋음. 
'''

# ### 스태킹 모델을 통한 회귀 예측

# In[30]:


from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. 
def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # n_folds:5
    # 지정된 n_folds값으로 KFold 생성.
    kf = KFold(n_splits=n_folds, shuffle=False, random_state=None) 
        # 일반적으로는 교차검증을 위해 cross_val_score를 해주는데, KFold를 해주면 "어떻게 교차검증 해줄지" 정해줌!
    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 
    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) # 학습 데이터 세트 5영역 구분해서 각각 예측한 결괏값. X_train size:(1166, 270), X_train_n.shape[0]:1166
    test_pred = np.zeros((X_test_n.shape[0],n_folds)) # 원본 테스트 세트의 모든 결괏값 5개를 각각 담음. 나중에 가로로 평균할것. X_test size:(292, 270), X_test_n.shape[0]:292
    print(model.__class__.__name__ , ' model 시작 ')
    
    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): 
            # for문 돌때마다 원래 train데이터세트를 n_fold=5만큼 나눠 하나를 검증데이터, 나머지를 train데이터로 두고, 그 index들을 리턴
            # train_index : 4개의 train데이터, valid_index : 1개의 검증데이터
        
        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 
        print('\t 폴드 세트: ',folder_counter,' 시작 ')
        X_tr = X_train_n[train_index] # 4개의 train데이터 안에서 train_test_split된 X_train
        y_tr = y_train_n[train_index] # 4개의 train데이터 안에서 train_test_split된 y_train
        X_te = X_train_n[valid_index] # 1개의 검증데이터
        
        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.
        model.fit(X_tr , y_tr)    
        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.
        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) # 검증fold 예측한 결괏값 => for문 다 돌면 5영역이 꽉 찬 하나의 컬럼이 나옴. 
        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. 
        test_pred[:, folder_counter] = model.predict(X_test_n) # 원본 테스트 세트로 예측한 결괏값 컬럼 한 줄씩 담음 => for문 다 돌면 컬럼 5개
            
    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 
    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)  # test_pred 이거 column 방향으로 row끼리 평균. (292, 5) → (292, 1)
    
    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터
    return train_fold_pred , test_pred_mean # 한줄씩 2개 리턴


# ** 기반 모델은 리지, 라소, XGBoost, LightGBM 으로 만들고 최종 메타 모델은 라소로 생성하여 학습/예측/평가 **

# In[31]:


# get_stacking_base_datasets( )은 넘파이 ndarray를 인자로 사용하므로 DataFrame을 넘파이로 변환. 
X_train_n = X_train.values
X_test_n = X_test.values
y_train_n = y_train.values

# 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환. 
ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5)
lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5)
xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5)  
lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5)


# In[32]:


# 개별 모델이 반환한 학습 및 테스트용 데이터 세트를 Stacking 형태로 결합.  
Stack_final_X_train = np.concatenate((ridge_train, lasso_train, 
                                      xgb_train, lgbm_train), axis=1) # 4개의 train데이터끼리 column방향으로(옆으로) 땅! 붙임
Stack_final_X_test = np.concatenate((ridge_test, lasso_test, 
                                     xgb_test, lgbm_test), axis=1) # 4개의 test데이터끼리 column방향으로(옆으로) 땅! 붙임

# 최종 메타 모델은 라쏘 모델을 적용. 
meta_model_lasso = Lasso(alpha=0.0005)

#기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정.
meta_model_lasso.fit(Stack_final_X_train, y_train)
final = meta_model_lasso.predict(Stack_final_X_test)

mse = mean_squared_error(y_test , final)
rmse = np.sqrt(mse)
print('스태킹 회귀 모델의 최종 RMSE 값은:', rmse)
'''
스태킹 회귀 모델의 최종 RMSE 값은: 0.09799152965189689
'''

■■■■■■■■■■■■■0607 수업中 새로 알게된것■■■■■■■■■■■■■

######### diabetes.csv 연습퀴즈 #########

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier


diabetes_data = pd.read_csv('C:/jeon/diabetes.csv')

print(diabetes_data.info()) # null값 x

'''
Outcome 1-당뇨병o, 0-당뇨병x

1. 로지스틱 회귀, LGB, XGB, GBM, 랜덤포레스트 이용
    => 각 estimator 마다 - 정확도, 정밀도, 재현율, f1, roc_auc(predict_proba() 필요함)값 측정

2. Glucose, BloodPressure, Insulin의 0 -> 평균값으로 바꿈
SkinTickness의 0 ->  임신횟수(Pregnancies) > 0이면 SkinTickness = 25
                                        == 0                 = 15
BMI의 0 ->   혈압(BloodPressure) >= 110 이면 BMI의 quentile 75%값 (describe로 확인)
                            90<= x <110                     50%
                                 < 90                       25%

'''


X = diabetes_data.iloc[:, :-1]
y = diabetes_data.iloc[:, -1]
X_describe = X.describe()

zero_to_mean=['Glucose', 'BloodPressure', 'Insulin']
for each in zero_to_mean:
    mean = int(X[X[each]!=0][each].agg('mean'))
    X[each] = X[each].apply(lambda x:mean if x==0 else x)

test = X[X['SkinThickness']==0]['Pregnancies'].apply(lambda x:x if x!=0 else(25 if x>0 else 15))
indexes = test.index
X['SkinThickness'][indexes] = test[indexes]

test = X[X['BMI']==0]['BloodPressure'].apply(lambda x:X_describe['BMI']['75%'] if x >= 110 \
                                    else(X_describe['BMI']['50%'] if (x>=90 and x<110) else X_describe['BMI']['25%']))
indexes = test.index
X['BMI'][indexes] = test[indexes]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=156, stratify=y)

#아래에서 인수로 추가해준 건 warning 제거해주기 위해 추가함. 안써줘도 상관없음. 
lr_clf = LogisticRegression(max_iter=200)
lgb_clf = LGBMClassifier()
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
gbm_clf = GradientBoostingClassifier()
rf_clf = RandomForestClassifier()

models = [lr_clf, lgb_clf, xgb_clf, gbm_clf, rf_clf]
for idx, model in enumerate(models):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    pred_proba = model.predict_proba(X_test)[:, 1]
    
    print("\n",end='')
    print(idx+1)
    print("정확도 : {0:.4f}".format(accuracy_score(pred, y_test)))
    print("정밀도 : {0:.4f}".format(precision_score(pred, y_test)))
    print("재현율 : {0:.4f}".format(recall_score(pred, y_test)))
    print("f1 : {0:.4f}".format(f1_score(pred, y_test)))
    print("roc_auc : {0:.4f}".format(roc_auc_score(y_test, pred_proba)))

'''
=== 선생님 풀이! ===

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier

diabetes_data = pd.read_csv('E:/no1/cho/archive/diabetes.csv')

test1 = diabetes_data.describe()
diabetes_data.info()

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))


def get_model_predict(model, X_train, X_test, y_train, y_test):
    print('model: ' , type(model).__name__)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    pred_proba = model.predict_proba(X_test)[:, 1]
    get_clf_eval(y_test , pred, pred_proba)


X = diabetes_data.iloc[:, :-1]
y = diabetes_data.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y)

lr_clf = LogisticRegression()
LGB_clf = LGBMClassifier()
XGB_clf = XGBClassifier()
GB_clf = GradientBoostingClassifier()
rf_clf = RandomForestClassifier()

total = [lr_clf ,LGB_clf, XGB_clf, GB_clf, rf_clf]

for row in total:
    get_model_predict(row, X_train, X_test, y_train, y_test )
    
test = X['SkinThickness'].isnull().sum()

bool0 = X['SkinThickness'] == 0
bool1 = X['Pregnancies'] == 0
bool2 = X['Pregnancies'] > 0

#print(X['SkinThickness'].value_counts())

X['SkinThickness'][bool0 & bool1] = 15
X['SkinThickness'][bool0 & bool2] = 25

#print(X['SkinThickness'].value_counts())

test = X.describe()

test1 = X['BMI'].value_counts()

print(test1[36.6])
print(test1[32])
print(test1[27.3])

bool3 = X['BMI'] == 0
print(bool3.sum())

bool4 = bool3 & X['BloodPressure'] > 110
print(bool4.sum())

bool5 = bool3 & (X['BloodPressure'] <= 110) & (X['BloodPressure'] > 90)
print(bool5.sum())

bool6 = bool3 & (X['BloodPressure'] <= 90)
print(bool6.sum())

X['BMI'][bool3] = 36.6
X['BMI'][bool4] = 32
X['BMI'][bool5] = 27.3

print('-------------------------------------')

test2 = X['BMI'].value_counts()
print(test2[36.6])
print(test2[32])
print(test2[27.3])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y)

# 로지스틱 회귀로 학습,예측 및 평가 수행. 
lr_clf = LogisticRegression()
LGB_clf = LGBMClassifier()
XGB_clf = XGBClassifier()
GB_clf = GradientBoostingClassifier()
rf_clf = RandomForestClassifier()

total = [lr_clf ,LGB_clf, XGB_clf, GB_clf, rf_clf]

for row in total:
    get_model_predict(row, X_train, X_test, y_train, y_test )

from sklearn.preprocessing import Binarizer

def get_eval_by_threshold(y_test , pred_proba_c1, thresholds):
    # thresholds 리스트 객체내의 값을 차례로 iteration하면서 Evaluation 수행.
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) 
        custom_predict = binarizer.transform(pred_proba_c1)
        print('임곗값:',custom_threshold)
        get_clf_eval(y_test , custom_predict, pred_proba_c1)

print('-------------------------------------------------------------')
thresholds = [0.3 , 0.33 ,0.36,0.39, 0.42 , 0.45 ,0.48, 0.50]
pred_proba = lr_clf.predict_proba(X_test)
get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)

'''

■■■■■■■■■■■■■0608 수업中 새로 알게된것■■■■■■■■■■■■■

######### creditcard.csv 연습퀴즈 #########

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier


creditcard_data = pd.read_csv('C:/jeon/creditcard.csv')

print(creditcard_data.info()) # null값 x

'''
Outcome : Class 0-사기X 1-사기O (신용카드)

1. 원본 데이터 프레임 복사 후, 로지스틱회귀와 LGB 이용해서 정확도, 정밀도, 재현율, F1, roc_auc 구하기
2. corr()함수로 데이터 간의 상관관계 계수 구한 후, heatmap 그리기
   그리고 레이블 데이터와 상관계수의 '절댓값'이 가장 높은 3개의 feature데이터가 뭔지 구하기
3. 3개의 feature데이터 오름차순 정렬해서 scatter 찍어보면 outlier 보임. sorting 해야 더 명확히 보임. 
    outlier같은 index는 제거해버리기. 
'''

def get_performance(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=156, stratify=y)

    lr_clf = LogisticRegression(max_iter=500)
    lgb_clf = LGBMClassifier()

    models = [lr_clf, lgb_clf]
    for idx, model in enumerate(models):
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        pred_proba = model.predict_proba(X_test)[:, 1]
        
        print("\n정확도 : {0:.4f}".format(accuracy_score(pred, y_test)))
        print("정밀도 : {0:.4f}".format(precision_score(pred, y_test)))
        print("재현율 : {0:.4f}".format(recall_score(pred, y_test)))
        print("f1 : {0:.4f}".format(f1_score(pred, y_test)))
        print("roc_auc : {0:.4f}".format(roc_auc_score(y_test, pred_proba)))


X = creditcard_data.iloc[:, :-1]
y = creditcard_data.iloc[:, -1]

get_performance(X, y)
'''
정확도 : 0.9990
정밀도 : 0.4898
재현율 : 0.8421
f1 : 0.6194
roc_auc : 0.9126

정확도 : 0.9970
정밀도 : 0.5102
재현율 : 0.2924
f1 : 0.3717
roc_auc : 0.6391
'''
    
corr = creditcard_data.corr()
sns.heatmap(corr, cmap = 'RdBu')

label_corr = corr.iloc[:-1, -1] # 레이블 데이터와의 상관계수
label_corr = abs(label_corr).sort_values(ascending=False)
print(label_corr.head(3))
'''
V17    0.326481
V14    0.302544
V12    0.260593
Name: Class, dtype: float64
'''

head3=[]
for i in range(3):
    head3.append(X[label_corr.head(3).index[i]].sort_values(ascending=False))

min_x_axle = min(min(head3[0]), min(head3[1]), min(head3[2]))
min_y_axle = max(max(head3[0]), max(head3[1]), max(head3[2]))

plt.scatter(np.linspace(min_x_axle, min_y_axle, num=284807), head3[0].values, marker='o', c=head3[0], s=25, cmap='rainbow', edgecolor='k')
plt.scatter(np.linspace(min_x_axle, min_y_axle, num=284807), head3[1].values, marker='o', c=head3[1], s=25, cmap='rainbow', edgecolor='k')
plt.scatter(np.linspace(min_x_axle, min_y_axle, num=284807), head3[2].values, marker='o', c=head3[2], s=25, cmap='rainbow', edgecolor='k')
'''
quentile (75%, 50%, 25%) 확인해서 최댓값+1.5*q, 최솟값-1.5*q로 이상치 구하는 것보다 이렇게 직접 구하는 게 더 정확할 때 多 !
그래프에서 어떤 값까지 이상치로 볼지는 온전히 내 판단. 
나중에 퍼포먼스 보고 다시 여러번 이상치 걸러서 그렇게 이상치 제거된 최적의 피처데이터 만들면 됨. 
'''

cond1 = head3[0] < -25
cond2 = head3[1] > 10
cond3 = head3[2] > 6

outlier_index = X[ cond1 | cond2 | cond3 ].index # 좌표에서 outlier 좌표만 포함된 범위 선택
creditcard_copy = creditcard_data.copy()
creditcard_copy = creditcard_copy.drop(outlier_index, axis=0)

X = creditcard_copy.iloc[:, :-1]
y = creditcard_copy.iloc[:, -1]

get_performance(X, y)
'''
정확도 : 0.9990
정밀도 : 0.5204
재현율 : 0.8095
f1 : 0.6335
roc_auc : 0.9381

    better  => 정확도, 정밀도, f1, roc_auc 
    worse   => 재현율


정확도 : 0.9964
정밀도 : 0.6327
재현율 : 0.2672
f1 : 0.3758
roc_auc : 0.8082

    better  => 정밀도, f1, roc_auc 
    worse   => 정확도, 재현율
'''

■■■■■■■■■■■■■0609 수업中 새로 알게된것■■■■■■■■■■■■■

선형 변환
- 행렬x벡터 곱
	┌ 1 0 ┐: 단위행렬I(역행렬을 곱해주면 A-¹A == I)이자 정방행렬(nxn형태)
	│      │
	└ 0 1 ┘
	┌ 1 0 ┐┌ 3 ┐ ┌ 3 ┐ : 벡터 [3 4]에 1( : 단위행렬I )을 곱해준 것과 같다. 
	│      ││   │=│   │
	└ 0 1 ┘└ 4 ┘ └ 4 ┘
	┌ 2 1 ┐┌ 3 ┐ ┌ 10 ┐ : 벡터 [3 4]에 행렬을 곱해주어서 벡터 [10 17]이 되었다. 
	│      ││   │=│     │   시점은 같고 종점만 바뀜. 벡터에 선형 변환을 일으켰다. 
	└ 3 2 ┘└ 4 ┘ └ 17 ┘
- 요즘 Airbnb도 집 구조 3D로 보여줌. 마우스 휠로 드르륵~ 하면 그래픽 바뀌는걸 보여주는데, 그 행위로 내가 벡터에 계속 어떤 값을 곱해 주면서 선형 변환을 일으킨다. 

차원 축소 (Dimension Reduction)
- 피처끼리 종속적인 데이터에 사용. 상관관계가 높은지 확인 : corr() 구해서 heatmap(corr)
- 종류
	PCA多 (고유 값, 고유 벡터♣) : 피처를 삭제하는 게 아니라, 피처들을 잘 설명하는 (고유값이 가장 큰) 고유벡터를 내가 원하는 k개만큼(걔네는 모두 서로 직각) 찾는 것. 걔네들이 새로운 축이 되는 것. 
	LDA : PCA에서 더 나아가, 클래스 간 분산은 최대화하고 클래스 내부 분산은 최소화하는 방향으로 차원 축소. 
	SVD多
	NMF
	원-핫 인코딩에서 레이블이 많을 수록 새로생긴 column 값이 대부분 0인 Sparse Matrix(↔Dense Matrix 꽉차있는 행렬)가 만들어질 수밖에 없다.  => PCA가 어려움. 대신 SVD 많이 씀. 

PCA ♣
- Ax = λx
	A : 공분산행렬
	x : PCA의 주성분벡터, 입력 데이터의 분산이 큰 '방향'
	λ : PCA의 주성분벡터의 크기, 입력 데이터의 분산(=변동성). "분류"의 관점에서, 클수록 label값 여러개인 데이터 설명 GOOD :)
- λ가 제일 큰(가장 높은 분산을 가지는) 데이터의 축 x(= 주성분벡터 = 고유벡터)를 찾아서 그 축으로 투영해서 차원 축소
- 고유 벡터로 새로운 축 만들지 않으면, 잘못 만들어져서 투영된 label값 다 섞임.
- ex)	개별 데이터를 평가할 'c언어', '파이썬' 2개의 피처에 가중치(고유벡터)를 주어 '종합 점수' 1개로만 평가할 수 있게 되는 느낌. 
	새로 만들어진 '종합점수' 축으로 원래의 2차원 좌표에 찍혀 있던 개별 데이터들의 점수를 투영해서 1차원으로 평가한다. 

LDA
- PCA(공분산행렬로 분산이 가장 큰 축 찾음)에서 더 나아가, LDA는 label을 최대한 분리할 수 있는 축 찾음. 
- 클래스 간 분산 Sb (between-class scatter) 최대↑
  클래스 내부 분산 Sw (within-class scatter) 최소↓
- pca에서 사용했던 Ax = λx 에서 고유값, 고유벡터는 똑같이 구함 - A 어떻게구하는지는 자세히 몰라도 됨^-^

SVD ♣
- U, V^T : 직교행렬,  ∑ : 대각행렬
         A 	= Uㆍ∑ㆍ(V^T)   	// A라는 행렬은 이런 식으로 3개로 분리 가능하다. 
    AㆍV 	= Uㆍ∑	    	// 행렬 A가 V(직교)의 벡터들을 선형 변환. V→Uㆍ∑ 여전히 직교, 크기만 변함! 
		    	// U:직교 :대각 Uㆍ∑:직교x대각=직교. U→Uㆍ∑ 여전히 직교, 크기만 변함!
				// 행렬 A, B의 내적 : A의 행, B의 열 곱함
				// 직교ㆍ대각 - 직교행렬의 각 벡터의 크기만 바뀜 → 여전히 직교
				// 대각ㆍ대각 - 대각행렬의 각 벡터의 크기만 바뀜 → 여전히 대각
         A	= Uㆍ∑ㆍ(V^T)
- 선형 독립적인 row벡터 수(=0이 아닌 ∑값 개수)만큼 차원 축소
	ex_	∑=[2.663 0.807 0.    0.   ] : 0이 아닌 값 2개
		Uㆍ∑ㆍ(V^T) 크기 mxm, mxn, nxn → mx2, 2x2, 2xn : 결과는 둘다 똑같이 mxn

NMF
- 원본 행렬 내의 모든 원소 값이 모두 양수(0 이상)라는 게 보장될 때 사용. 별로 쓸일 없음. 나중에 필요하면 찾아 쓰기. 

고유 값, 고유 벡터
- Ax = λx (A:정방행렬(nxn)인 "공분산행렬", x:벡터, λ:상수)
	수많은 벡터가 있을 텐데, 어떤 x라는 벡터는 x벡터에 A라는 공분산행렬을 곱해서 x에 선형 변환을 해줬을 때, 이 변환이 방향은 그대로이고 크기만 바뀌는 그런 벡터 x가 있다. 
	그 때의 벡터 x : 고유 벡터  (크기가 바뀌기 때문에 0이 아님!)
	그 때의 크기 λ : 고유 값
-         Ax = λx
              = λIx (I : 단위벡터)
    Ax - λIx = 0
    (A - λI)x = 0
    --------------------  // 여기서 만약 행렬 A - λI 의 역행렬이 존재 한다면!!! 양변에 역행렬 곱해주면
             x = 0   	     //  말이 안 됨 =ㅅ=!!

  ∴ 행렬 A - λI 의 역행렬은 존재하지 않는다. 
  ∴ det( A - λI ) = 0    (행렬 A - λI에서 ad-bc=0)

- 행렬 A가 주어졌을 때
	1. 고유 값(λ) 을 구해보자!
		: det( A - λI ) = 0 식에 A=주어진 공분산행렬 A 넣고, 고유 값 λ 구하기!
		> A가 2x2면 λ가 될 수 있는 2차방정식 det( A - λI )=ad-bc=0의 해 최대 2개
		> A가 3x3면 λ, 즉 3차방정식 det( A - λI )=0의 해 최대 3개
	2. 고유 벡터(x) 를 구해보자!
		: Ax = λx 식에 A=주어진A, λ=아까 구한 고유값(여러개 각각 큰 순서대로) 넣고, 각 고유값에 해당하는 고유 벡터 x 구하기!

역행렬
- A-¹A == I
- 역행렬 구하는 공식
	┌ a b ┐		      1	┌ d -b ┐
	│      │의 역행렬 :  ───	│        │
	└ c d ┘		   ad-bc	└ -c  a ┘
- 역행렬이 존재하지 않는다 == "ad-bc = 0"인 경우
- det( A - λI ) = ad-bc

공분산
- Cov(X, Y) = E((X-E(X))(Y-E(Y)))	: x의 편차와 y의 편차를 곱한 것의 평균
	- X, Y의 개별 요소에 각각 평균을 뺀다 == 원점으로 shift된다
	- 상관관계의 지표
	  1. 상관관계가 크다  vs. 작다
		: Cov(X, Y) 값이 크다  vs. 작다
		: (xi-E(X) 와 (yi-E(Y) 의 부호가 계속 같거나 계속 다르면 평균이 계속 커지거나 계속 작아짐
	  2. 양의 상관관계  vs. 음의 상관관계
		: Cov(X, Y)>0  vs. Cov(X, Y)<0
- 분산 : 사실은 동일한 피처 간의 공분산이다. E((X-E(X))(X-E(X))) = E((X-E(X)²)

공분산 행렬
- n개의 모든 피처 간의 공분산을 나타낸 nxn 행렬 (정방행렬(nxn)이자 대칭행렬(A^T=A))
- 선형 변환이 어떻게 되는지 나타내는 표기법. 현재 우리가 가진 데이터 분포를 설명하는 행렬. 
- 대각선 원소 : 각 변수의 분산

직교행렬 Orthogonal Matrix
- 행렬을 이루는 각 벡터(세로)끼리 모두 직각
- (직교행렬) x (직교행렬^T) = I	// 단위행렬I : 행렬의 관점에서 1. 역행렬처럼 동작
				// 서로 직각인 벡터의 내적 = 0
┌ 2 -1 ┐┌  2 1 ┐  ┌ 4+1  2-2┐ ┌ 5 0 ┐
│       ││       │=│            │=│     │ 스케일링 제대로 하면 단위행렬I
└ 1  2 ┘└ -1 2 ┘  └ 2-2  1+4┘ └ 0 1 ┘

대각행렬 Diagonal Matrix
- 주대각선 위의 x1, x2, x3, ..., xn을 제외한 나머지 요소가 모두 0. 
- x1, x2, x3, ..., xn도 0이 될 수 있음. 

■■■■■■■■■■■■■0610 수업中 새로 알게된것■■■■■■■■■■■■■

# ### PCA 개요 

# In[1]:


from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

# 사이킷런 내장 데이터 셋 API 호출
iris = load_iris()

# 넘파이 데이터 셋을 Pandas DataFrame으로 변환
columns = ['sepal_length','sepal_width','petal_length','petal_width'] # 4개의 feature
irisDF = pd.DataFrame(iris.data , columns=columns)
irisDF['target']=iris.target # 3개의 label
irisDF.head(3)


# ** sepal_length, sepal_width 두개의 속성으로 데이터 산포 시각화 **

# In[2]:


#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현
markers=['^', 's', 'o']

#setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot 
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF['target']==i]['sepal_length'] # feature 2개만 뽑아서 축 2개로 scatter해봄
    y_axis_data = irisDF[irisDF['target']==i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) # 3개의 label이 scatter로 그려지는 마커가 서로 다르다. 
    '''
    setosa는 scatter 그래프에서 구분이 명확. 
    versicolor, virginica는 섞여 있는 데이터가 많아서 나중에 모델이 구분 못함. 
    '''

plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()


# ** 평균이 0, 분산이 1인 정규 분포로 원본 데이터를 변환 **

# In[3]:


from sklearn.preprocessing import StandardScaler

iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1]) # StandardScaler로 정규분포로 표준화해서 label값 제외한 피처데이터만 리턴


# In[4]:


iris_scaled.shape


# ** PCA 변환 수행 **

# In[5]:


from sklearn.decomposition import PCA

pca = PCA(n_components=2) # 2개의 축 만들어서 2차원으로 차원 축소하겠다. pca를 수행하는 객체. 

#fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled) # pca가 iris_scaled를 가장 잘 나타내는 축 2개에 데이터 투영해서 리턴
    # 순서1. iris_scaled의 공분산 행렬 만듦
    # 순서2. 가장 높은 고유값 2개 구함
    # 순서3. 그 고유값에 해당하는 2개의 고유벡터(주성분벡터) 구함
    # 순서4. 2개의 주성분벡터에 데이터 투영
print(iris_pca.shape)


# In[6]:


# PCA 변환된 데이터의 컬럼명을 각각 pca_component_1, pca_component_2로 명명
pca_columns=['pca_component_1','pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca,columns=pca_columns)
irisDF_pca['target']=iris.target
irisDF_pca.head(3)


# ** PCA로 차원 축소된 피처들로 데이터 산포도 시각화 **

# In[7]:


#setosa를 세모, versicolor를 네모, virginica를 동그라미로 표시
markers=['^', 's', 'o']

#pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. 
for i, marker in enumerate(markers):
    x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1'] # 이제는 pca 끝난 2개의 축으로 scatter. 
    y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2']
    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])
    '''
    setosa는 scatter 그래프에서 구분이 여전히 명확. 
    versicolor, virginica는 원본 데이터보다 훨씬 구분 명확해짐. 
     => 아까보다 모델링 정확도 높일 수 있음. 
     
    즉, 차원 축소한답시고 column 2개만 남기고 다 제거해서 보는 것보다, 
    이렇게 PCA로 제대로 2개의 축 만들어서 모델링하는 게 훨씬 좋다!!!!
    '''

plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
plt.show()


# ** 각 PCA Component별 변동성 비율 **

# In[8]:


print(pca.explained_variance_ratio_)
'''
[0.72962445 0.22850762]
둘을 더하면 0.95~~ => pca로 4개 → 2개의 축으로 차원 축소했지만, irisDF의 전체 데이터를 95%정도는 설명할 수 있다. 
'''


# ** 원본 데이터와 PCA 변환된 데이터 기반에서 예측 성능 비교 **

# In[9]:


from sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 분류로 퍼포먼스 비교
from sklearn.model_selection import cross_val_score # 3번 교차검증
import numpy as np

rcf = RandomForestClassifier(random_state=156)
scores = cross_val_score(rcf, iris.data, iris.target,scoring='accuracy',cv=3)
print(scores)
print(np.mean(scores))
'''
=== 4개의 축 ===
[0.98 0.94 0.96]    //개별정확도
0.96                //평균정확도
'''

# In[10]:


pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']]
scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3 )
print(scores_pca)
print(np.mean(scores_pca))
'''
=== 2개의 축 ===
[0.88 0.88 0.88]    //개별정확도
0.88                //평균정확도

지금은 정확도 10% 낮아짐. 원래 피처가 4개밖에 없어서 여기선 딱히 별로지만..!
1. 나중에 피처 개수 엄청 늘어나거나
2. 각각의 피처가 상관계수가 높은 피처가 많거나 하면
아주 좋다~~!!!

또한, 시간 압축은 당연히 되지만 '회귀' 관련 문제에서는 '다중 공선성' 기준에서 압축을 통해 퍼포먼스가 좋아질 수 있음!
'''

# ### 신용카드 데이터 세트 PCA 변환      // 이제 PCA변환이 유용할 것으로 예상되는 데이터에 해보자!
# 
# ** 데이터 로드 및 컬럼명 변환 **

# In[11]:


import pandas as pd

df = pd.read_excel('C:/jeon/pca_credit_card.xls', sheet_name='Data', header=1)
print(df.shape) '''(30000, 25) >> 이번엔 컬럼이 25개나!'''
df.head(3)


# In[12]:


df.rename(columns={'PAY_0':'PAY_1','default payment next month':'default'}, inplace=True)
    # PAY_0 다음이 PAY_2니까, 타겟 default payment next month 이름이 너무 기니까 컬럼이름 바꿔줌
y_target = df['default'] # 타겟 'default' 다음달 연체 여부 => 0-연체no, 1-연체yes
# ID, default 컬럼 Drop
X_features = df.drop(['ID','default'], axis=1)


# In[13]:


y_target.value_counts()


# In[14]:


X_features.info() # null값 없음


# ** 피처간 상관도 시각화 **

# In[15]:


import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

corr = X_features.corr() # 상관계수
plt.figure(figsize=(14,14))
sns.heatmap(corr, annot=True, fmt='.1g')
'''
heapmap 그래프 보면
BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6
이 6개 피처들은 상관계수가 엄청 높은 게 보임. 

=> 얘네를 PCA변환으로 차원축소 하면 되겠다 !!
'''


# **상관도가 높은 피처들의 PCA 변환 후 변동성 확인**

# In[16]:


from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#BILL_AMT1 ~ BILL_AMT6까지 6개의 속성명 생성
cols_bill = ['BILL_AMT'+str(i) for i in range(1, 7)] # list comprehension
print('대상 속성명:', cols_bill)

# 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산을 위해 fit( ) 호출
scaler = StandardScaler()
df_cols_scaled = scaler.fit_transform(X_features[cols_bill]) # ID, label 데이터 제외한 피처데이터에서 cols_bill 6개의 컬럼만 정규화
pca = PCA(n_components=2) # 고유값 가장 높은 축 2개로 차원축소하겠다. 
pca.fit(df_cols_scaled) # 6줄의 데이터를 축 2개로 축소시켰다. 

print('PCA Component별 변동성:', pca.explained_variance_ratio_) # 각 축의 분산값
'''
PCA Component별 변동성: [0.90555253 0.0509867 ]  => 각 축의 분산값 == 각 축마다 전체 데이터를 얼만큼 설명하는지. 
총 2개의 축으로 df_cols_scaled의 전체 데이터 6줄을 95%만큼 설명한다.  => 6줄 상관계수가 정말 높았구나~
'''

# ** 원본 데이터 세트와 6개 컴포넌트로 PCA 변환된 데이터 세트로 분류 예측 성능 비교 **

# In[17]:


import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf = RandomForestClassifier(n_estimators=300, random_state=156)
scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3 ) # PCA변환으로 차원 축소 하기 전
                            # X_features : (30000, 23)

print('CV=3 인 경우의 개별 Fold세트별 정확도:',scores)
print('평균 정확도:{0:.4f}'.format(np.mean(scores)))
'''
=== 25개의 축 ===
[0.8083 0.8196 0.8232]  //개별정확도
0.8170                  //평균정확도
'''

# In[18]:


from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 원본 데이터셋에 먼저 StandardScaler적용
scaler = StandardScaler()
df_scaled = scaler.fit_transform(X_features) # 일단 전체 X_features(ID, default(label) 제거된) 데이터를 모두 정규화 

# 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. 
pca = PCA(n_components=6) # 전체 25개의 축을 6개로 축약시키겠다~!
df_pca = pca.fit_transform(df_scaled)
scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3) # PCA 이후
                                # df_pca : (30000, 6)


print('CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:',scores_pca)
print('PCA 변환 데이터 셋 평균 정확도:{0:.4f}'.format(np.mean(scores_pca)))
'''
=== 6개의 축 ===
[0.792  0.7961 0.8053]  //개별정확도
0.7978                  //평균정확도

평균 정확도는 2% 줄었지만, 축이 25 → 6으로 19개나 줄어들었다. 
모든 25개의 축이 전부 다 독립적이었다면 퍼포먼스가 나빠졌겠지만, 상관관계가 높은 축들이 있었기에 크게 나빠지진 않은 것임!!
'''

=============================================================

# ### LDA 개요 
# ### 붓꽃 데이터 셋에 LDA 적용하기 

# In[2]:


from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

iris = load_iris()
iris_scaled = StandardScaler().fit_transform(iris.data) # label빼고 피처값만 모두 정규화


# In[3]:

'''
무조건 갖다 쓰는 건 bad. 
항상 뭘 하든 내부에 어떤 일이 있는지, 어떻게 해야 하는지, 내 뜻대로 안 되면 뭐가 문제인지 
개인 시간 투자해서 10시간 걸려도 파고들어야 한다. 
그래야 감이 생김. => 1년, 2년, 연차 쌓일 수록 속도 확연히 빨라짐. 
'''
lda = LinearDiscriminantAnalysis(n_components=2)
# fit()호출 시 target값 입력 
lda.fit(iris_scaled, iris.target)   # 보통은 fit 시킬 때 feature, target 같이 넣어줘야 함. 
                                    # PCA는 각각의 sample들이 어떤 target값을 가지는지 알 필요 x. feature만 넣어도 됐음.
                                    # LDA는 알고리즘 상 target값이 필요함. label데이터 군집 끼리의 분산, 각 label군집 안에서의 분산이 필요. 그래서 target값도 넣어줌. 
iris_lda = lda.transform(iris_scaled)
print(iris_lda.shape)


# In[4]:


import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

lda_columns=['lda_component_1','lda_component_2']
irisDF_lda = pd.DataFrame(iris_lda,columns=lda_columns)
irisDF_lda['target']=iris.target # irisDF_lda size:(150, 3)

#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현
markers=['^', 's', 'o']

#setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot
for i, marker in enumerate(markers):
    x_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_1']
    y_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_2']

    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])

plt.legend(loc='upper right')
plt.xlabel('lda_component_1')
plt.ylabel('lda_component_2')
plt.show()


# In[ ]:

# PCA 변환한 데이터로 그린 scatter 와 비교. 아래 코드는 PCA. 위에서 그린 LDA가 조금 더 명확하게 분류됨. 

from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

# 사이킷런 내장 데이터 셋 API 호출
iris = load_iris()

# 넘파이 데이터 셋을 Pandas DataFrame으로 변환
columns = ['sepal_length','sepal_width','petal_length','petal_width']
irisDF = pd.DataFrame(iris.data , columns=columns)
irisDF['target']=iris.target
irisDF.head(3)
from sklearn.preprocessing import StandardScaler

iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1])
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

#fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)
print(iris_pca.shape)
pca_columns=['pca_component_1','pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca,columns=pca_columns)
irisDF_pca['target']=iris.target
#setosa를 세모, versicolor를 네모, virginica를 동그라미로 표시
markers=['^', 's', 'o']

#pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. 
for i, marker in enumerate(markers):
    x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1']
    y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2']
    plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])

plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
plt.show()

■■■■■■■■■■■■■0611 수업中 새로 알게된것■■■■■■■■■■■■■

# ### SVD 개요

# In[3]:


# numpy의 svd 모듈 import
import numpy as np
from numpy.linalg import svd

# 4X4 Random 행렬 a 생성 - 행렬 개별 row끼리의 의존성 없애려고. 
np.random.seed(121)
a = np.random.randn(4,4) # 정규분포 상에서 4x4 크기에 해당하는 데이터 개수만큼 랜덤값 뽑기
print(np.round(a, 3))
'''
[[-0.212 -0.285 -0.574 -0.44 ]
 [-0.33   1.184  1.615  0.367]
 [-0.014  0.63   1.71  -1.327]
 [ 0.402 -0.191  1.404 -1.969]]
'''

# **SVD 행렬 분해**

# In[4]:


U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape)
'''
(4, 4) (4,) (4, 4) → ∑는 1차원으로 리턴되었다. 
어차피 대각선에 있는 값들만 필요하기 때문에 걔네만 이렇게 받아도 ㄱㅊ
'''
print('U matrix:\n',np.round(U, 3))
print('Sigma Value:\n',np.round(Sigma, 3))
print('V transpose matrix:\n',np.round(Vt, 3))
'''
U matrix:
 [[-0.079 -0.318  0.867  0.376]
 [ 0.383  0.787  0.12   0.469]
 [ 0.656  0.022  0.357 -0.664]
 [ 0.645 -0.529 -0.328  0.444]]
Sigma Value:
 [3.423 2.023 0.463 0.079]      → 우리가 나중에 대각행렬 꼴로 바꿔주면 됨. 
V transpose matrix:
 [[ 0.041  0.224  0.786 -0.574]
 [-0.2    0.562  0.37   0.712]
 [-0.778  0.395 -0.333 -0.357]
 [-0.593 -0.692  0.366  0.189]]
'''

# **분해가 잘 되었는지 '검증' => 분해된 행렬들을 이용하여 다시 원행렬로 원복**

# In[5]:


# Sima를 다시 0 을 포함한 대칭행렬로 변환
Sigma_mat = np.diag(Sigma)
print(Sigma_mat) # 원래대로 대각행렬 꼴로 바꿔줌. 
'''
[[3.4229581  0.         0.         0.        ]
 [0.         2.02287339 0.         0.        ]
 [0.         0.         0.46263157 0.        ]
 [0.         0.         0.         0.07935069]]
'''
a_ = np.dot(np.dot(U, Sigma_mat), Vt) # 행렬 3개 차례대로 내적(점곱) 해줌. 
print(np.round(a_, 3))
'''
[[-0.212 -0.285 -0.574 -0.44 ]
 [-0.33   1.184  1.615  0.367]
 [-0.014  0.63   1.71  -1.327]
 [ 0.402 -0.191  1.404 -1.969]]

=> 음~ 처음의 a행렬과 같군! 진짜로 SVD 기법으로 A = U∑(V^T) 세 개로 쪼개지는구나!
'''


# **데이터 의존도가 높은 원본 데이터 행렬 생성**

# In[6]:


a[2] = a[0] + a[1]
a[3] = a[0]
print(np.round(a,3)) # 이번엔 무작위가 아니라, 개별 row끼리의 의존성 줌. 어떻게 분해될지 아까와 비교해보자!
'''
[[-0.212 -0.285 -0.574 -0.44 ]
 [-0.33   1.184  1.615  0.367]
 [-0.542  0.899  1.041 -0.073]
 [-0.212 -0.285 -0.574 -0.44 ]]
'''


# In[7]:


# 다시 SVD를 수행하여 Sigma 값 확인 => 특히 ∑의 변화에 집중해보자!
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape)
'''
(4, 4) (4,) (4, 4) → 여전히 ∑는 1차원으로 리턴되었다. 
어차피 대각선에 있는 값들만 필요하기 때문에 걔네만 이렇게 받아도 ㄱㅊ
'''
print('Sigma Value:\n',Sigma)
print('Sigma Value_round3:\n',np.round(Sigma,3))
'''
Sigma Value:
 [2.66335286e+00 8.07035060e-01 1.30310447e-16 3.87711837e-17]   → 1.30310447e-16, 3.87711837e-17 ≒ 0
Sigma Value_round3:
 [2.663 0.807 0.    0.   ]       → 이번엔 ∑에 0값이 포함되어 있다!
                                 → row끼리 의존성이 있는 A를 쪼갰을 때 나오는 ∑는 뒤로 갈수록 0에 수렴하는 값을 보인다!
                                 → 그래서 이게 차원 축소와 무슨 상관?
                                         => 뒤의 0은 제거해도 됨!! 축을 줄일 수 있다!!
'''

# In[8]:


"""
    <차원축소>
    0이 아닌 Sigma값 2개니까 Uㆍ∑ㆍ(V^T) 크기는 각각 4x2, 2x2, 2x4 로 차원 축소할 수 있음!!
"""
# U 행렬의 경우는 Sigma와 내적을 수행하므로 Sigma의 앞 2행에 대응되는 앞 2열만 추출
U_ = U[:, :2] 
Sigma_ = np.diag(Sigma[:2])
# V 전치 행렬의 경우는 앞 2행만 추출
Vt_ = Vt[:2]
print(U_.shape, Sigma_.shape, Vt_.shape)
'''
(4, 2) (2, 2) (2, 4)
'''
# U, Sigma, Vt의 내적을 수행하며, 다시 원본 행렬 복원
a_ = np.dot(np.dot(U_,Sigma_), Vt_)
print(np.round(a_, 3))
'''
[[-0.212 -0.285 -0.574 -0.44 ]    → 아까 의존성 있게 만들어준 a가 그대로 나왔군! 
 [-0.33   1.184  1.615  0.367]      0에 거의 수렴한 값들은 잘라내버려도 원본에 아주 가깝게 구할 수 있구나~!
 [-0.542  0.899  1.041 -0.073]
 [-0.212 -0.285 -0.574 -0.44 ]]

의존성이 낮다면 좀 더 뒤로 가야만 0에 수렴하기 시작함. 그럴 때 차원 축소한다면, 
어느 정도 설명력이 낮아질 것을 감안하고 차원 축소하는 것! (PCA로 2개 피처로만 보면 데이터의 95%만 설명하던 붓꽃 예제처럼)
'''

# * Truncated SVD 를 이용한 행렬 분해

# In[10]:


import numpy as np
from scipy.sparse.linalg import svds
from scipy.linalg import svd

# 원본 행렬을 출력하고, SVD를 적용할 경우 U, Sigma, Vt 의 차원 확인 
np.random.seed(121)
matrix = np.random.random((6, 6))
print('원본 행렬:\n',matrix)
U, Sigma, Vt = svd(matrix, full_matrices=False)
print('\n분해 행렬 차원:',U.shape, Sigma.shape, Vt.shape)
print('\nSigma값 행렬:', Sigma)
'''
원본 행렬:
 [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941]
 [0.5557906  0.74552394 0.24849976 0.9686594  0.95268418 0.48984885]
 [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852]
 [0.4056155  0.56730065 0.24575605 0.22573721 0.03827786 0.58098021]
 [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176]
 [0.51161442 0.46920965 0.6439515  0.82081228 0.14548493 0.01806415]]
분해 행렬 차원: (6, 6) (6,) (6, 6)
Sigma값 행렬: [3.2535007  0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ]   → 각각 랜덤이라 row벡터끼리의 의존도가 높지 않음. 
'''

# Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행. 
num_components = 4
U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components)
print('\nTruncated SVD 분해 행렬 차원:',U_tr.shape, Sigma_tr.shape, Vt_tr.shape)
print('\nTruncated SVD Sigma값 행렬:', Sigma_tr)
'''
num_components = 4
Truncated SVD로 분해 후 복원 행렬:
 [[0.19222941 0.21792946 0.15951023 0.14084013 0.81641405 0.42533093]
 [0.44874275 0.72204422 0.34594106 0.99148577 0.96866325 0.4754868 ]
 [0.12656662 0.88860729 0.30625735 0.59517439 0.28036734 0.93961948]
 [0.23989012 0.51026588 0.39697353 0.27308905 0.05971563 0.57156395]
 [0.83806144 0.78847467 0.93868685 0.72673231 0.6740867  0.73812389]
 [0.59726589 0.47953891 0.56613544 0.80746028 0.13135039 0.03479656]]
Truncated SVD 분해 행렬 차원: (6, 4) (4,) (4, 6)
Truncated SVD Sigma값 행렬: [0.55463089 0.83865238 0.88116505 3.2535007 ]
    # svds() 인수로 num_components 넣어주면 sigma값 거꾸로 정렬되네? 개수 정해주면 맨 뒤부터 거꾸로 오나보넹...

num_components = 5
Truncated SVD로 분해 후 복원 행렬:      → 확실히 num_components = 4 보다 원본을 더 비슷하게 설명함. 
 [[0.11368271 0.19721195 0.23106956 0.15961551 0.82758207 0.41695496]
 [0.55500167 0.75007112 0.24913473 0.96608621 0.95355502 0.48681791]
 [0.01789183 0.85994318 0.40526464 0.62115143 0.29581906 0.92803075]
 [0.40782587 0.55456069 0.24397702 0.23294659 0.035838   0.58947208]
 [0.82711496 0.78558742 0.94865955 0.7293489  0.67564311 0.73695659]
 [0.5136488  0.45748403 0.64231412 0.82744766 0.14323933 0.0258799 ]]
Truncated SVD 분해 행렬 차원: (6, 5) (5,) (5, 6)
Truncated SVD Sigma값 행렬: [0.35834824 0.55463089 0.83865238 0.88116505 3.2535007 ]
'''
matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr)  # output of TruncatedSVD

print('\nTruncated SVD로 분해 후 복원 행렬:\n', matrix_tr)


# ### 사이킷런 TruncatedSVD 클래스를 이용한 변환

# In[11]:


from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()
iris_ftrs = iris.data
# 2개의 주요 component로 TruncatedSVD 변환
tsvd = TruncatedSVD(n_components=2)
tsvd.fit(iris_ftrs)
iris_tsvd = tsvd.transform(iris_ftrs)

# Scatter plot 2차원으로 TruncatedSVD 변환 된 데이터 표현. 품종은 색깔로 구분
plt.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target)
plt.xlabel('TruncatedSVD Component 1')
plt.ylabel('TruncatedSVD Component 2')


# In[12]:


from sklearn.preprocessing import StandardScaler

# iris 데이터를 StandardScaler로 변환
scaler = StandardScaler()
iris_scaled = scaler.fit_transform(iris_ftrs)

# 스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행 
tsvd = TruncatedSVD(n_components=2) # 결국 PCA, LDA처럼 SVD 차원축소도 TruncatedSVD 이 함수 써주기만 하면 됨.  
tsvd.fit(iris_scaled)
iris_tsvd = tsvd.transform(iris_scaled)

# 스케일링된 데이터를 기반으로 PCA 변환 수행 
pca = PCA(n_components=2)
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)

# TruncatedSVD 변환 데이터를 왼쪽에, PCA변환 데이터를 오른쪽에 표현 
fig, (ax1, ax2) = plt.subplots(figsize=(9,4), ncols=2)
ax1.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target)
ax2.scatter(x=iris_pca[:,0], y= iris_pca[:,1], c= iris.target)
ax1.set_title('Truncated SVD Transformed')
ax2.set_title('PCA Transformed')
'''별 차이 없군!'''

============================================================

# ### NMF 개요

# In[13]:


from sklearn.decomposition import NMF
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()
iris_ftrs = iris.data
nmf = NMF(n_components=2)

nmf.fit(iris_ftrs)
iris_nmf = nmf.transform(iris_ftrs)

plt.scatter(x=iris_nmf[:,0], y= iris_nmf[:,1], c= iris.target)
plt.xlabel('NMF Component 1')
plt.ylabel('NMF Component 2')

■■■■■■■■■■■■■0614 수업中 새로 알게된것■■■■■■■■■■■■■

군집화 : 이미지처리 多(선 따서 원하는 물체만 트래킹해서 삭제한다거나..)
- K-평균 알고리즘
- 평균 이동
- GMM(Gaussian Mixture Model)
- DBSCAN(밀도 기반 군집화)♣

군집 평가 → 실루엣 분석
- s(i) = (b(i)-a(i)) / max(a(i),b(i))
	s(i) : i데이터 포인트의 실루엣 계수. -1 <= s(i) <= 1
	a(i) : i데이터가 속한 클러스터 내의 다른 데이터 포인트들의 평균 거리
	b(i) : i데이터와 가장 가까운 타 클러스터 내의 모든 데이터 포인트들의 평균 거리
- 개별 군집의 실루엣 계수의 평균값 = 한 군집에서 전체 i에 대한 s(i)의 평균값
	> 1에 가까울수록 근처 군집과 멀다. Good!
	> 0에 가까울수록 근처 군집과 가깝다. 
	> - 값은 아예 다른 군집에 데이터 포인트 할당됐다. a(i)>b(i)란 뜻이니까. 영역 구분이 애매하구나. 
- 전체 군집의 실루엣 계수의 평균값 : silhouette_score()    (= np.mean(silhouette_samples()))
	> silhouette_score()와 개별 군집의 실루엣 계수의 평균값 사이, 편차가 작아야 한다. 
- 레이블값이 명확히 정의되지 않은 데이터에서 군집 평가에 유용
- 개별 데이터가 가지는 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터들과 얼마나 가깝게 군집화돼 있고, 다른 군집의 데이터들과 얼마나 멀리 분리돼 있는지. 

K-평균 알고리즘
- 순서	1. 군집화 개수 n_clusters 만큼 중심점 설정
        ┌ 2. 각 데이터는 가장 가까운 중심점에 소속
        └ 3. 중심점을 소속된 데이터의 평균으로 이동 
	(2,3 반복) : 두 번째 반복부터, 3 끝난 이후 중심점 소속 변경이 없으면 finish
- 단점	> n_clusters 몇 개로 군집화할 것인가 판단하기 어려움ㅠㅠㅠ
	> 거리 기반 알고리즘이라서 피처 개수 많으면 정확도 떨어짐
		→ PCA로 차원 감소
	> 반복 횟수 많으면 수행 시간 느림
		→ max_iter 파라미터 설정
- 파라미터 > n_clusters : 군집화 개수
	  > init : 초기에 중심정 설정할 방식, 대부분 k-means++
	  > max_iter : 최대 반복 횟수
- 특성 	> make_blobs() 데이터세트 군집 중심점을 기반으로 "둥그렇게" 군집화됨. 

평균 이동 알고리즘
- 순서.  ┌1. 각 데이터에서 K(커널 함수) 이용해서 KDE 그림
          └2. KDE 값 높은, 즉 밀도 높은 곳으로 모든 데이터 이동 (원본x)
	(1,2 반복)
	3. 정해진 횟수만큼 반복 끝나고, 데이터들이 모여 있는 point로 중심점 설정
- 장점	> bandwidth(=h) 정해주면 군집의 개수 알아서 정해줌 (K-평균 알고리즘 단점)
	> 이상치의 영향력 小 (K-평균에서는 중심점이 이상치 영향 더 많이 받음)
- 단점	> 수행 시간 오래 걸림
	> bandwidth에 따른 군집도 영향력 너무 큼
		→ 분석 업무보다는 컴퓨터 비전 영역에서 多
- 파라미터 > bandwidth : 대역폭 h값, K(커널함수)를 얼마나 완만하게 할 것인가
		bandwidth가 너무 작다(h=1.0) - 너무 뾰족 - 군집 중심점 多, 과적합
		bandwidth가 너무 크다(h=10.) - 너무 완만 - 군집 중심점 少, 과소적합
- 특성 	> make_blobs() 데이터세트 군집 중심점을 기반으로 "둥그렇게" 군집화됨. 

GMM(Gaussian Mixture Model) 알고리즘
- 레이블 o - 최대우도법을 통해 각 레이블 별 확률분포 예측. 
  레이블 x - '군집화'에서는 정해진 레이블이 없으므로, 맨 먼저 아무 확률분포나 줘 본다. 
- 순서	1. n_components개의 초기 확률분포 위치를 랜덤하게 설정
        ┌ 2. 데이터 별 더 높은 확률을 가지는 확률분포를 선택
        └ 3. 같은 확률분포를 선택한 데이터끼리 '최대우도법'으로 새로운 확률분포 예측
	(2,3 반복) : 두 번째 반복부터, 3 끝난 이후 확률분포 선택 변경이 없으면 finish
- 파라미터 > n_components : 군집화 개수
- 특성 	> make_blobs()*행렬 => 타원형 데이터세트 군집 중심점을 기반으로 "길게" 군집화 잘 됨. 

최대우도법
- 데이터를 보고 어떤 데이터 셋에서 뽑혔을지, 그 최적의 확률분포를 예측
- 최적의 확률분포 : 데이터 a, b, c, ...가 있을 때 여러 확률분포 그래프 F들에 대해 F(a)xF(b)xF(c)x... 가 가장 큰 확률분포
	> F(a)xF(b)xF(c)x... 곱셈인 이유	=> 각 데이터는 독립적으로 뽑혔으므로.
	> 최댓값 구하는 방법		=> log취하면 곱셈→덧셈. log취한 식의 미분=0인 지점 찾아 최댓값 구함. 

DBSCAN(밀도 기반 군집화) 알고리즘♣
- 핵심포인트(Core Point)	: 입실론 반경(eps) 내에 본인 포함 min_samples 이상의 데이터를 가진 데이터
  경계포인트(Border Point)	: 핵심 포인트의 이웃이지만 min_samples 충족 못 시키는 데이터
  잡음포인트(Noise Point)	: 핵심 포인트의 이웃이 아니고, min_samples 충족 못 시키는 데이터
- 입실론(epsilon) 		: 개별 데이터 중심으로 입실론 반경(eps) 가지는 원형의 영역
- 순서	1. 각 데이터가 핵심포인트인지, 경계포인트인지, 잡음포인트인지 결정
	2. 핵심 포인트와 직접 접근(입실론內)이 가능한 다른 핵심 포인트를 연결하면서 군집 확장
- 장점	> 간단한 알고리즘으로 기하학적으로 복잡한 데이터 세트에도 효과적
- 파라미터 > eps : 입실론 반경
	  > min_samples : 입실론 안에 포함돼야 할 최소 데이터 개수 (본인 포함)
- 특성 	> 내부 원과 외부 원 형태의 make_circles() 데이터세트 군집화하기에 가장 좋음. 

■■■■■■■■■■■■■0615 수업中 새로 알게된것■■■■■■■■■■■■■

##################### K-평균 알고리즘 #####################
# ### K-Means를 이용한 붓꽃(Iris) 데이터 셋 Clustering

# In[11]:


from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()

# 보다 편리한 데이터 Handling을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=['sepal_length','sepal_width','petal_length','petal_width'])
irisDF.head(3)


# **KMeans 객체를 생성하고 군집화 수행**

# In[12]:


kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300,random_state=0) # 3개의 군집으로 묶겠다. 
kmeans.fit(irisDF)


# **labels_ 속성을 통해 각 데이터 포인트별로 할당된 군집 중심점(Centroid)확인하고 irisDF에 'cluster' 컬럼으로 추가**

# In[15]:


print(kmeans.labels_) # 3개의 군집에 레이블 0, 1, 2 부여
print(kmeans.predict(irisDF)) # 위랑 똑같은 code


# In[16]:


irisDF['cluster']=kmeans.labels_


# In[17]:


irisDF['target'] = iris.target # 실제 결정값 => 내가 군집화로 묶은 'cluster'데이터와 정해진 레이블값이 다름. ex_0, 1, 2 vs 1, 0, 2
iris_result = irisDF.groupby(['target','cluster'])['sepal_length'].count()
    # groupby(by='컬럼명') : 엑셀에서 '정렬'같은 느낌. '컬럼명'의 유니크한 값을 기준으로 묶어서 dataframe 만듦. dataframe groupby는 agg()를 써야 처리하기 쉽다.
    # 우선 'target'에 들어 있는 유니크한 레이블값 0, 1, 2로 묶고, 각 묶음 안에서 'cluster'의 유니크한 레이블값 0, 1, 2로 또 묶음. 
    # ['sepal_length'].count()에서 sepal_length는 별 의미 없음. 그냥 몇 개씩 묶였는지 데이터 개수를 확인하고 싶을 뿐. 
print(iris_result)
'''
target  cluster
0       1          50  → target이 0인 setosa는 군집화가 완벽하다. 
1       0          48  → target이 1인 versicolor도 군집화가 꽤 잘 되었다. 
        2           2
2       0          14  → target이 2인 virginica는 망했다...^^ 
        2          36
Name: sepal_length, dtype: int64
'''

# In[19]:


iris.target_names


# **2차원 평면에 데이터 포인트별로 군집화된 결과를 나타내기 위해 2차원 PCA값으로 각 데이터 차원축소**

# In[20]:


from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_transformed = pca.fit_transform(iris.data)

irisDF['pca_x'] = pca_transformed[:,0] # 차원 축소한 2개 피처 중 첫번째 피처
irisDF['pca_y'] = pca_transformed[:,1] # 차원 축소한 2개 피처 중 두번째 피처
irisDF.head(3)


# In[22]:


plt.scatter(x=irisDF.loc[:, 'pca_x'], y=irisDF.loc[:, 'pca_y'], c=irisDF['cluster']) # c : 색깔 구분
# 축소된 피처(차원) 2개로 그린 2차원
# 원래의 target값이 뭐가 됐든 이 이미지만 봤을 땐, 상당히 합리적으로 군집화가 되었다. 

# In[21]:


# cluster 값이 0, 1, 2 인 경우마다 별도의 Index로 추출
marker0_ind = irisDF[irisDF['cluster']==0].index
marker1_ind = irisDF[irisDF['cluster']==1].index
marker2_ind = irisDF[irisDF['cluster']==2].index

# cluster값 0, 1, 2에 해당하는 Index로 각 cluster 레벨의 pca_x, pca_y 값 추출. o, s, ^ 로 marker 표시
plt.scatter(x=irisDF.loc[marker0_ind,'pca_x'], y=irisDF.loc[marker0_ind,'pca_y'], marker='o') 
plt.scatter(x=irisDF.loc[marker1_ind,'pca_x'], y=irisDF.loc[marker1_ind,'pca_y'], marker='s')
plt.scatter(x=irisDF.loc[marker2_ind,'pca_x'], y=irisDF.loc[marker2_ind,'pca_y'], marker='^')

plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('3 Clusters Visualization by 2 PCA Components')
plt.show()


# ### Clustering(군집화) 알고리즘 테스트를 위한 데이터 생성 

# In[23]:


import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
get_ipython().run_line_magic('matplotlib', 'inline')

X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=0)
print(X.shape, y.shape)
'''(200, 2) (200,)'''

# y target 값의 분포를 확인
unique, counts = np.unique(y, return_counts=True) # y 안에 unique한 데이터는 무엇이 있고, 각각 몇 개 있는지 리턴
print(unique,counts)
'''[0 1 2] [67 67 66]  → 200개가 골고루 3개로 잘 나눠졌구나~ '''

# * n_samples: 생성할 총 데이터의 개수입니다. 디폴트는 100개입니다.
#  
#  
# * n_features: 데이터의 피처 개수입니다. 시각화를 목표로 할 경우 2개로 설정해 보통 첫 번째 피처는 x 좌표, 두 번째 피처 
# 는 y 좌표상에 표현합니다.   
#  
#  
# * centers: int 값, 예를 들어 3으로 설정하면 군집의 개수를 나타냅니다. 그렇지 않고 ndarray 형태로 표현할 경우 개별 군 
# 집 중심점의 좌표를 의미합니다.   
#  
#  
# * cluster_std: 생성될 군집 데이터의 표준 편차를 의미합니다. 만일 float 값 0.8과 같은 형태로 지정하면 군집 내에서 데이 
# 터가 표준편차 0.8을 가진 값으로 만들어집니다.    
# [0.8, 1,2, 0.6]과 같은 형태로 표현되면 3개의 군집에서 첫 번째 군집 내 
# 데이터의 표준편차는 0.8, 두 번째 군집 내 데이터의 표준 편차는 1.2, 세 번째 군집 내 데이터의 표준편차는 0.6으로 만듭 
# 니다.    
# 군집별로 서로 다른 표준 편차를 가진 데이터 세트를 만들 때 사용합니다   

# In[24]:


import pandas as pd

clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y
clusterDF.head(3)


# **make_blob()으로 만들어진 데이터 포인트들을 시각화**

# In[25]:


target_list = np.unique(y) # target_list = [0 1 2] 들어감. 
# 각 target별 scatter plot 의 marker 값들. 
markers=['o', 's', '^', 'P','D','H','x']
# 3개의 cluster 영역으로 구분한 데이터 셋을 생성했으므로 target_list는 [0,1,2]
# target==0, target==1, target==2 로 scatter plot을 marker별로 생성. 
for target in target_list:
    target_cluster = clusterDF[clusterDF['target']==target] # target값 0, 1, 2인 데이터끼리 따로 가져와서 각 target별로
    plt.scatter(x=target_cluster['ftr1'], y=target_cluster['ftr2'], edgecolor='k', marker=markers[target] ) # 'o', 's', '^'라는 marker를 이용해 x축, y축 scatter한다. 
plt.show()


# In[26]:


target_list = np.unique(y) # target_list = [0 1 2] 들어감. 
plt.scatter(x=clusterDF['ftr1'], y=clusterDF['ftr2'], edgecolor='k', c=y ) # 이건 실제 결정값 scatter 찍어봄. 


# **K-Means 클러스터링(군집화)을 수행하고 개별 클러스터의 중심 위치를 시각화**

# In[27]:


# KMeans 객체를 이용하여 X 데이터를 3개의 군집으로 K-Means 클러스터링 수행 
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=200, random_state=0)
cluster_labels = kmeans.fit_predict(X) # 이 안에서 반복 다 끝나고, 최종 중심점+소속된 데이터 구하고 군집화 끝남. 
clusterDF['kmeans_label']  = cluster_labels # 실제 target값과 값은 다를 수 있지만 어쨌든 군집화는 잘 됨. 
                                            # target 0  kmeans_label 0
                                            #        1               2
                                            #        2               1

#cluster_centers_ 는 개별 클러스터의 중심 위치 좌표 시각화를 위해 추출
centers = kmeans.cluster_centers_ # kmeans 초기화된 3개의 중심점 좌표
unique_labels = np.unique(cluster_labels) # 표시는 0 1 2로 할거다. 
markers=['o', 's', '^', 'P','D','H','x']

# 군집된 label 유형별로 iteration 하면서 marker 별로 scatter plot 수행. 
for label in unique_labels: # 군집(cluster)별로 for문 수행
    label_cluster = clusterDF[clusterDF['kmeans_label']==label]
    center_x_y = centers[label] # 군집 별로 중심점 좌표 담김
    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], edgecolor='k', 
                marker=markers[label] ) # 각 데이터를 scatter 찍는다. 
    
    # 군집별 중심 위치 좌표 시각화 
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color='white',
                alpha=0.9, edgecolor='k', marker=markers[label]) # 중심점 0, 1, 2 scatter 'o', 's', '^'로 찍는다. 
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k', edgecolor='k', # s:size. 키우면 scatter 표기 크기 커짐. 
                marker='$%d$' % label) # 중심점마다 scatter 찍은 그 위치에 label값, 즉 0, 1, 2 표기

plt.show()

# In[28]:

kmeans.cluster_centers_
''' 최종 중심점 찍힌 좌표
array([[ 0.990103  ,  4.44666506],
       [-1.70636483,  2.92759224],
       [ 1.95763312,  0.81041752]])
'''

# In[30]:

print(clusterDF.groupby('target')['kmeans_label'].value_counts())
'''
target  kmeans_label
0       0               66
        1                1
1       2               67
2       1               65
        2                1
Name: kmeans_label, dtype: int64





##################### 군집 평가 - 실루엣 분석 #####################

# ### 붓꽃(Iris) 데이터 셋을 이용한 클러스터 평가

# In[ ]:


from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
# 실루엣 분석 metric 값을 구하기 위한 API 추가
from sklearn.metrics import silhouette_samples, silhouette_score # 2가지 함수를 import
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()
feature_names = ['sepal_length','sepal_width','petal_length','petal_width']
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300,random_state=0).fit(irisDF) # 3개로 군집화

irisDF['cluster'] = kmeans.labels_

irisDF.head(3)


# In[ ]:


# iris 의 모든 개별 데이터에 실루엣 계수값을 구함. 
score_samples = silhouette_samples(iris.data, irisDF['cluster']) # 전체 데이터 150개의 모든 s(i) 구함
                                                                 # s(i)구하려면 피처데이터iris.data, 데이터i가 어느 cluster에 속해 있는지irisDF['cluster'] 필요. 
print('silhouette_samples( ) return 값의 shape' , score_samples.shape)

# irisDF에 실루엣 계수 컬럼 추가
irisDF['silhouette_coeff'] = score_samples


# In[ ]:


irisDF.head(20)


# In[ ]:


# 모든 데이터의 평균 실루엣 계수값을 구함. 
average_score = silhouette_score(iris.data, irisDF['cluster'])
print('붓꽃 데이터셋 Silhouette Analysis Score:{0:.3f}'.format(average_score))
'''
붓꽃 데이터셋 Silhouette Analysis Score:0.553     → 전체 실루엣 계수의 평균값
'''

# In[ ]:


irisDF['silhouette_coeff'].hist() # 실루엣계수 값 분포 히스토그램


# In[ ]:


irisDF.groupby('cluster')['silhouette_coeff'].mean() # 'cluster'의 유니크한 값별로 (군집별로) 묶어서 각 실루엣 계수의 평균값
'''
cluster
0    0.417320
1    0.798140
2    0.451105
Name: silhouette_coeff, dtype: float64
'''

# ### 클러스터별 평균 실루엣 계수의 시각화를 통한 클러스터 개수 최적화 방법

# In[14]:


### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성
# 그래프 이쁘게 그리려고 함수 좀 길게 써진 것임. 
def visualize_silhouette(cluster_lists, X_features): 
                        # [2, 3, 4, 5], X
    
    from sklearn.datasets import make_blobs
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_samples, silhouette_score

    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    import math
    
    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함
    n_cols = len(cluster_lists)
    
    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 
    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols) # (1, 4) 크기로 subplots 그리겠다. 
    
    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화
    for ind, n_cluster in enumerate(cluster_lists):
        # ind: 0,1,2,3    n_cluster: 2,3,4,5
        
        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. 
        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0) # k-평균 군집화할 객체
        cluster_labels = clusterer.fit_predict(X_features) # 객체 clusterer로 X_features 군집화 해줌. 각 데이터 별 어떤 군집에 해당되는지 레이블값 담김.
        
        sil_avg = silhouette_score(X_features, cluster_labels) # 전체 실루엣 계수의 평균
        sil_values = silhouette_samples(X_features, cluster_labels) # 전체 실루엣 계수
        
        # plot 설정값들
        y_lower = 10
        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\n'
                           'Silhouette Score :' + str(round(sil_avg,3)) )
        axs[ind].set_xlabel("The silhouette coefficient values")
        axs[ind].set_ylabel("Cluster label")
        axs[ind].set_xlim([-0.1, 1])
        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])
        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks
        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])
        
        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. 
        for i in range(n_cluster): # 2, 3, 4, 5바퀴씩 돌 것 → i = 0,1  ,  0,1,2  ,  0,1,2,3  ,  0,1,2,3,4
            ith_cluster_sil_values = sil_values[cluster_labels==i] # 군집이 i인 애들의 실루엣 계수만 모아서
            ith_cluster_sil_values.sort() # 실루엣 계수 정렬
            
            size_cluster_i = ith_cluster_sil_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            color = cm.nipy_spectral(float(i) / n_cluster)
            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values,
                                   facecolor=color, edgecolor=color, alpha=0.7)
            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10
            
        axs[ind].axvline(x=sil_avg, color="red", linestyle="--")
    '''
    전체 실루엣 계수의 평균값 sil_avg 은 n_cluster = 2 일 때 가장 높다. 
    
    BUT !!!
    전체 실루엣 계수의 평균값 sil_avg 과 개별 군집의 실루엣 계수의 평균값 의 차이가 작아야 한다. 
    n_cluster = 2 그래프를 보면, 
    1번 군집은 대체로 sil_avg 보다 커서 좋지만, 그에 따라 반대로 2번 군집은 일부를 제외하고 대체로 sil_avg 보다 낮음. 
    
    OTHERWISE !!!
    n_cluster = 4 그래프를 보면, 
    개별 군집의 평균 실루엣 계수 값이 비교적 균일하므로, n_cluster = 2 일 때보다 sil_avg 가 작지만 조금 더 이상적이다. 
    
    전체 실루엣 계수의 평균값보다 더 중요한 건, 꽤 높은 값의 s(i)들이 고르게 분포되어야 한다는 것이다.
    '''


# In[15]:


# make_blobs 을 통해 clustering 을 위한 4개의 클러스터 중심의 500개 2차원 데이터 셋 생성  
from sklearn.datasets import make_blobs # 테스트 위해서 임의로 샘플 만들기 위한 함수 (군집화된 데이터 샘플 만드는 데 특화됨. )
X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, # 총 4개의 군집으로 이루어진 (500, 2) 데이터 샘플을 만듦. 
                  center_box=(-10.0, 10.0), shuffle=True, random_state=1)  

# cluster 개수를 2개, 3개, 4개, 5개 일때의 클러스터별 실루엣 계수 평균값을 시각화 
visualize_silhouette([ 2, 3, 4, 5], X) # → 이 데이터를 몇 개의 cluster로 묶는 게 나은지 살펴보기 위함. 


# In[16]:


from sklearn.datasets import load_iris

iris=load_iris()
visualize_silhouette([ 2, 3, 4,5 ], iris.data)
'''
붓꽃데이터에서도 실루엣계수 각각 살펴봤는데, 얘는 2개로 군집화할 때가 제일 좋을 듯 하다!
'''


# In[13]:


from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)

# Generating the sample data from make_blobs
# This particular setting has one distinct cluster and 3 clusters placed close
# together.
X, y = make_blobs(n_samples=500,
                  n_features=2,
                  centers=4,
                  cluster_std=1,
                  center_box=(-10.0, 10.0),
                  shuffle=True,
                  random_state=1)  # For reproducibility

range_n_clusters = [2, 3, 4, 5, 6]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
    '''
    Automatically created module for IPython interactive environment
    For n_clusters = 2 The average silhouette_score is : 0.7049787496083262
    For n_clusters = 3 The average silhouette_score is : 0.5882004012129721
    For n_clusters = 4 The average silhouette_score is : 0.6505186632729437
    For n_clusters = 5 The average silhouette_score is : 0.56376469026194
    For n_clusters = 6 The average silhouette_score is : 0.4504666294372765
    '''

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values =             sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()
'''
실제로 군집화된 데이터를 scatter로 살펴보니, 아까 생각한 것처럼 n_cluster = 4 로 두는 게 맞는 것 같다.  
'''




##################### KDE의 이해 _for 평균 이동 알고리즘 #####################

# ### KDE(Kernel Density Estimation)의 이해

# **seaborn의 distplot()을 이용하여 KDE 시각화**  
# https://seaborn.pydata.org/tutorial/distributions.html

# In[2]:


import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

sns.set(color_codes=True)

np.random.seed(0)
x = np.random.normal(0, 1, size=30) # 평균 0, 표준편차 1인 정규분포를 가지는 랜덤값. 
print(x)
sns.distplot(x);


# In[3]:


sns.distplot(x, rug=True)


# In[4]:


sns.distplot(x, kde=False, rug=True)


# In[5]:


sns.distplot(x, hist=False, rug=True);


# **개별 관측데이터에 대해 가우시안 커널 함수를 적용**

# In[7]:


from scipy import stats

#x = np.random.normal(0, 1, size=30)
bandwidth = 1.06 * x.std() * x.size ** (-1 / 5.) # 이해하려 하지 말고, 어찌 됐건 이런 bandwidth를 가지게 했음. 
support = np.linspace(-4, 4, 200)

kernels = []
for x_i in x:
    kernel = stats.norm(x_i, bandwidth).pdf(support) # 각 데이터 포인트가 가지고 있는 커널 함수
    kernels.append(kernel)
    plt.plot(support, kernel, color="r") # 전체 데이터에 대한 커널 함수 red색깔로 그림. 

sns.rugplot(x, color=".2", linewidth=3);


# In[8]:


from scipy.integrate import trapz
density = np.sum(kernels, axis=0)
density /= trapz(density, support)
plt.plot(support, density);


# **seaborn은 kdeplot()으로 kde곡선을 바로 구할 수 있음**

# In[9]:


sns.kdeplot(x, shade=True);


# **bandwidth에 따른 KDE 변화**

# In[10]:


sns.kdeplot(x)
sns.kdeplot(x, bw=.2, label="bw: 0.2") # bandwidth가 작으면 뾰족, 봉우리 여러개
sns.kdeplot(x, bw=2, label="bw: 2") # bandwidth가 크면 완만,  봉우리 한개
plt.legend();





##################### 평균 이동 알고리즘 #####################

# ### 사이킷런을 이용한 Mean Shift 
# 
# make_blobs()를 이용하여 2개의 feature와 3개의 군집 중심점을 가지는 임의의 데이터 200개를 생성하고 MeanShift를 이용하여 군집화 수행

# In[11]:


import numpy as np
from sklearn.datasets import make_blobs
from sklearn.cluster import MeanShift

X, y = make_blobs(n_samples=200, n_features=2, centers=3, # 군집화를 평가하기 위해 군집 중심점 3개 가지는 샘플 만들기
                  cluster_std=0.8, random_state=0)        # cluster_std : 표준편차
plt.scatter(X[:,0], X[:,1])

meanshift= MeanShift(bandwidth=0.9) # 평균이동(bandwidth=0.9로 하기)을 위한 객체 만들어서
cluster_labels = meanshift.fit_predict(X) # 위에서 만든 피처데이터에 적용
print('cluster labels 유형:', np.unique(cluster_labels)) # cluster_labels의 유니크한 값 출력
'''
cluster labels 유형: [0 1 2 3 4 5 6 7]
=> bandwidth=0.9 로 줬더니, 군집 중심점이 8개나 생겼구나. => 너무많아ㅠ.ㅠ => bandwidth값 키워야겠군!
'''


# **커널함수의 bandwidth크기를 1로 약간 증가 후에 Mean Shift 군집화 재 수행**

# In[12]:


meanshift= MeanShift(bandwidth=1)
cluster_labels = meanshift.fit_predict(X)
print('cluster labels 유형:', np.unique(cluster_labels))
'''
cluster labels 유형: [0 1 2]
=> bandwidth=1 로 주는 게 아까보다 훨씬 better!
'''


# **최적의 bandwidth값을 estimate_bandwidth()로 계산 한 뒤에 다시 군집화 수행**

# In[13]:


from sklearn.cluster import estimate_bandwidth

bandwidth = estimate_bandwidth(X,quantile=0.25) # 최적의 bandwidth 구하는 함수
print('bandwidth 값:', round(bandwidth,3))
'''
bandwidth 값: 1.689

estimate_bandwidth()로 최적의 bandwidth 구할 수 있으면 애초에 bandwidth까지 알아서 구해주면 안 되나?
=> 그렇게 하지 않는 이유     1. quantile 정해줄 때 있음 (전체 데이터 中 이만큼만으로 bandwidth 구하겠다. quantile 클수록 bandwidth 큼)
                            2. 군집화 多 쓰는 영상처리, 이미지처리에서는 bandwidth 여러개 사용할 때 있음. 
'''


# In[14]:


import pandas as pd

clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

# estimate_bandwidth()로 최적의 bandwidth 계산
best_bandwidth = estimate_bandwidth(X, quantile=0.25)

meanshift= MeanShift(best_bandwidth)
cluster_labels = meanshift.fit_predict(X)
print('cluster labels 유형:',np.unique(cluster_labels))    
'''cluster labels 유형: [0 1 2]'''


# In[15]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

clusterDF['meanshift_label']  = cluster_labels # 군집화된 레이블값 [0, 1, 2]
centers = meanshift.cluster_centers_ # 군집 중심점 좌표
unique_labels = np.unique(cluster_labels) # 군집화된 레이블값 0, 1, 2
markers=['o', 's', '^', 'x', '*']

for label in unique_labels:
    label_cluster = clusterDF[clusterDF['meanshift_label']==label] # 군집화한 label이 각각 0, 1, 2인 것만
    center_x_y = centers[label] # 0, 1, 2번째 중심점
    # 군집별로 다른 marker로 scatter plot 적용
    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], edgecolor='k', 
                marker=markers[label] ) # 각 레이블별로 0, 1, 2번째 marker로 좌표 찍음
    
    # 군집별 중심 시각화
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color='white',
                edgecolor='k', alpha=0.9, marker=markers[label])
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k', edgecolor='k', 
                marker='$%d$' % label) # '$%d$' % label : label값이 %d 안에 들어감
    
plt.show()


# In[16]:


print(clusterDF.groupby('target')['meanshift_label'].value_counts())
'''
target  meanshift_label
0       0                  67
1       2                  67
2       1                  65
        2                   1
Name: meanshift_label, dtype: int64

이 정도면 ... 너무 완벽하게 군집화 했다! ^-^
'''




##################### GMM 알고리즘 #####################

# ### GMM 을 이용한 붓꽃 데이터 셋 클러스터링

# In[1]:


from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()
feature_names = ['sepal_length','sepal_width','petal_length','petal_width']

# 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)
irisDF['target'] = iris.target


# **GaussianMixture를 이용하여 붓꽃 데이터 군집화**

# In[2]:


from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3, random_state=0).fit(iris.data) # 3개의 랜덤 확률분포로 fit
gmm_cluster_labels = gmm.predict(iris.data) # 군집화 결과 label값

# 클러스터링 결과를 irisDF 의 'gmm_cluster' 컬럼명으로 저장
irisDF['gmm_cluster'] = gmm_cluster_labels


# target 값에 따라서 gmm_cluster 값이 어떻게 매핑되었는지 확인. 
iris_result = irisDF.groupby(['target'])['gmm_cluster'].value_counts()
print(iris_result)
'''
target  gmm_cluster
0       0              50
1       2              45
        1               5
2       1              50
Name: gmm_cluster, dtype: int64
'''


# **붓꽃 데이터 K-Means 군집화 결과**

# In[4]:

iris.target_names

# In[3]:

kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300,random_state=0).fit(iris.data)
kmeans_cluster_labels = kmeans.predict(iris.data)
irisDF['kmeans_cluster'] = kmeans_cluster_labels
iris_result = irisDF.groupby(['target'])['kmeans_cluster'].value_counts()
print(iris_result)
'''
target  kmeans_cluster
0       1                 50
1       0                 48
        2                  2
2       2                 36
        0                 14
Name: kmeans_cluster, dtype: int64

=> K-Means 알고리즘 특성 상, 중심점에서 원 형태로 군집화할 수밖에 없음. 이 데이터셋에선 나쁜 군집화 방법. 
'''

# **클러스터링 결과를 시각화하는 함수 생성**

# In[6]:


### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  
def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):
                         # None        clusterDF, 'target',   iscenter=False
    
    if iscenter :
        centers = clusterobj.cluster_centers_
        
    unique_labels = np.unique(dataframe[label_name].values) # 군집화된 유니크한 레이블값 0, 1, 2 전부 담음
    markers=['o', 's', '^', 'x', '*']
    isNoise=False

    for label in unique_labels: # 군집화 별로
        label_cluster = dataframe[dataframe[label_name]==label] # 클러스터 결과를 담은 DataFrame에서 label_name이 label인 데이터들만 담음
        if label == -1: # 잘못 나온 label - 일단 이번 데이터셋에는 -1 없음. 
            cluster_legend = 'Noise'
            isNoise=True
        else :
            cluster_legend = 'Cluster '+str(label) # 'Cluster 0', 'Cluster 1', 'Cluster 2' 
        
        # for문 돌면서 각 레이블 별 데이터 scatter 찍어줌
        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,
                    edgecolor='k', marker=markers[label], label=cluster_legend)
        
        # 레이블 이름도 같이 띄워주고 싶을 때
        if iscenter:
            center_x_y = centers[label]
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',
                        alpha=0.9, edgecolor='k', marker=markers[label])
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',
                        edgecolor='k', marker='$%d$' % label)
    if isNoise:
        legend_loc='upper center'
    else: legend_loc='upper right'
    
    plt.legend(loc=legend_loc)
    plt.show()


# **GMM군집화와 K-Means군집화를 비교하기 위해 타원형으로 늘어선 임의의 데이터 세트를 생성**

# In[7]:


from sklearn.datasets import make_blobs

# make_blobs() 로 300개의 데이터 셋, 3개의 cluster 셋, cluster_std=0.5 을 만듬. 
X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=0)
plt.scatter(X[:, 0], X[:, 1]) # 이 데이터가

# 길게 늘어난 타원형의 데이터 셋을 생성하기 위해 변환함. 
transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]
X_aniso = np.dot(X, transformation) # (2x2).즉 차원축소 때의 개념과 이어 보자면 2개의 축(2개의 column)에다가 데이터 300개를 투영시켜서 
plt.scatter(X_aniso[:, 0], X_aniso[:, 1]) # 이렇게 새로운 데이터 (300, 2) 로 변한다. 
    # 여기서 새로 그려진 x축, y축은 각각 투영시킨 그 벡터. 즉, x = (0.60834549, -0.40887718), y = (-0.63667341, 0.85253229)

# feature 데이터 셋과 make_blobs( ) 의 y 결과 값을 DataFrame으로 저장
clusterDF = pd.DataFrame(data=X_aniso, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y
# 생성된 데이터 셋을 target 별로 다른 marker 로 표시하여 시각화 함. 
visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)


# **K-Means 군집화 수행**

# In[8]:


# 3개의 Cluster 기반 Kmeans 를 X_aniso 데이터 셋에 적용 
kmeans = KMeans(3, random_state=0)
kmeans_label = kmeans.fit_predict(X_aniso)
clusterDF['kmeans_label'] = kmeans_label

visualize_cluster_plot(kmeans, clusterDF, 'kmeans_label',iscenter=True) # K-평균으로 군집화한 데이터 찍어봄. 
'''
K-Means 알고리즘 특성 상, 중심점에서 원 형태로 군집화할 수밖에 없음. 이 데이터셋에선 나쁜 군집화 방법. 
'''


# **Mean Shift 군집화 수행**

# In[9]:


from sklearn.cluster import MeanShift
from sklearn.cluster import estimate_bandwidth

best_bandwidth = estimate_bandwidth(X_aniso)
meanshift = MeanShift(best_bandwidth)
cluster_labels = meanshift.fit_predict(X_aniso)
clusterDF['meanshift_label'] = cluster_labels

visualize_cluster_plot(meanshift, clusterDF, 'meanshift_label',iscenter=True)

    
# **GMM 군집화 수행**

# In[10]:

# 3개의 n_components기반 GMM을 X_aniso 데이터 셋에 적용 
gmm = GaussianMixture(n_components=3, random_state=0)
gmm_label = gmm.fit(X_aniso).predict(X_aniso)
clusterDF['gmm_label'] = gmm_label

# GaussianMixture는 cluster_centers_ 속성이 없으므로 iscenter를 False로 설정. 
visualize_cluster_plot(gmm, clusterDF, 'gmm_label',iscenter=False)
'''
GMM 알고리즘으로 군집화해보니, 원하던 대로 잘 됐다! 
'''

# **GMM과 K-Means 군집화 결과 비교**

# In[11]:


print('### KMeans Clustering ###')
print(clusterDF.groupby('target')['kmeans_label'].value_counts())
print('\n### Mean Shift Clustering ###')
print(clusterDF.groupby('target')['meanshift_label'].value_counts())
print('\n### Gaussian Mixture Clustering ###')
print(clusterDF.groupby('target')['gmm_label'].value_counts())
'''
### KMeans Clustering ###               => BAD :(
target  kmeans_label
0       2                73
        0                27
1       1               100
2       0                86
        2                14
Name: kmeans_label, dtype: int64

### Mean Shift Clustering ###           => BAD :(
target  meanshift_label
0       0                  100
1       1                  100
2       0                  100
Name: meanshift_label, dtype: int64

### Gaussian Mixture Clustering ###     => GOOD ~! :)
target  gmm_label
0       2            100
1       1            100
2       0            100
Name: gmm_label, dtype: int64
'''




##################### DBSCAN 알고리즘 #####################

# ### DBSCAN 적용하기 – 붓꽃 데이터 셋

# In[5]:


from sklearn.datasets import load_iris

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
get_ipython().run_line_magic('matplotlib', 'inline')

iris = load_iris()
feature_names = ['sepal_length','sepal_width','petal_length','petal_width']

# 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)
irisDF['target'] = iris.target
irisDF.head()


# **eps 0.6 min_samples=8 로 DBSCAN 군집화 적용**

# In[6]:


from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.6, min_samples=8, metric='euclidean')
dbscan_labels = dbscan.fit_predict(iris.data)

irisDF['dbscan_cluster'] = dbscan_labels

iris_result = irisDF.groupby(['target'])['dbscan_cluster'].value_counts()
print(iris_result)
'''
target  dbscan_cluster
0        0                49
        -1                 1    → dbscan_cluster = -1 : 어떤 cluster에도 속해 있지 않은 Noise Point !
1        1                46
        -1                 4
2        1                42
        -1                 8
Name: dbscan_cluster, dtype: int64

즉, Noise를 제외하고, cluster가 2개로 군집화 된 것임! ★★★
'''

# In[7]:


### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  
def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):
    if iscenter :
        centers = clusterobj.cluster_centers_
        
    unique_labels = np.unique(dataframe[label_name].values)
    markers=['o', 's', '^', 'x', '*']
    isNoise=False

    for label in unique_labels:
        label_cluster = dataframe[dataframe[label_name]==label]
        if label == -1:
            cluster_legend = 'Noise'
            isNoise=True
        else :
            cluster_legend = 'Cluster '+str(label)
        
        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,                    edgecolor='k', marker=markers[label], label=cluster_legend)
        
        if iscenter:
            center_x_y = centers[label]
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',
                        alpha=0.9, edgecolor='k', marker=markers[label])
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',                        edgecolor='k', marker='$%d$' % label)
    if isNoise:
        legend_loc='upper center'
    else: legend_loc='upper right'
    
    plt.legend(loc=legend_loc)
    plt.show()


# **PCA 2개 컴포넌트로 기존 feature들을 차원 축소 후 시각화**

# In[8]:


from sklearn.decomposition import PCA
# 2차원으로 시각화하기 위해 PCA n_componets=2로 피처 데이터 세트 변환 : 차원 축소
pca = PCA(n_components=2, random_state=0)
pca_transformed = pca.fit_transform(iris.data)
# visualize_cluster_2d( ) 함수는 ftr1, ftr2 컬럼을 좌표에 표현하므로 PCA 변환값을 해당 컬럼으로 생성
irisDF['ftr1'] = pca_transformed[:,0] # 새로운 축으로 차원 축소했으니까, 그 축 2가지를 각각 x축, y축으로 좌표평면에 plot 그림
irisDF['ftr2'] = pca_transformed[:,1]

visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)


# **eps의 크기를 증가 한 후 노이즈 확인**

# In[9]:


from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.8, min_samples=8, metric='euclidean') # eps값 커짐 => noise 줄어들음. 
dbscan_labels = dbscan.fit_predict(iris.data)
'''
아까 코드 ↓
dbscan = DBSCAN(eps=0.6, min_samples=8, metric='euclidean')
dbscan_labels = dbscan.fit_predict(iris.data)

min_samples 동일할 때   eps 커짐 => 기존의 noise는 core를 포함하게 되어 noise → border. 즉, noise ▽▽
                           작아짐 => 반대로 noise △△
eps 동일할 때   min_samples 커짐 => 기존의 core는 이제 core가 아니게 되어, core, border → border, noise. 즉 noise △△
                           작아짐 => 반대로 noise ▽▽

'''

irisDF['dbscan_cluster'] = dbscan_labels
irisDF['target'] = iris.target

iris_result = irisDF.groupby(['target'])['dbscan_cluster'].value_counts()
print(iris_result)

visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)


# **min_samples의 크기를 증가 후 노이즈 확인**

# In[10]:


dbscan = DBSCAN(eps=0.6, min_samples=16, metric='euclidean') # min_samples 커짐 => noise 늘어남.
dbscan_labels = dbscan.fit_predict(iris.data)

irisDF['dbscan_cluster'] = dbscan_labels
irisDF['target'] = iris.target

iris_result = irisDF.groupby(['target'])['dbscan_cluster'].value_counts()
print(iris_result)
visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)


# ### DBSCAN 적용하기 – make_circles() 데이터 세트

# In[11]:


from sklearn.datasets import make_circles

X, y = make_circles(n_samples=1000, shuffle=True, noise=0.05, random_state=0, factor=0.5) # make_circles() : 내부 원과 외부 원 형태의 2차원 데이터 세트
                  # 샘플 개수                      noise 데이터세트의 비율      외부 원 scale = 1일 때의 내부 원 scale. 
clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)


# In[12]:


# KMeans로 make_circles( ) 데이터 셋을 클러스터링 수행. 
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, max_iter=1000, random_state=0)
kmeans_labels = kmeans.fit_predict(X)
clusterDF['kmeans_cluster'] = kmeans_labels

visualize_cluster_plot(kmeans, clusterDF, 'kmeans_cluster', iscenter=True)


# In[13]:


# GMM으로 make_circles( ) 데이터 셋을 클러스터링 수행. 
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=2, random_state=0)
gmm_label = gmm.fit(X).predict(X)
clusterDF['gmm_cluster'] = gmm_label

visualize_cluster_plot(gmm, clusterDF, 'gmm_cluster', iscenter=False)


# In[14]:


# DBSCAN으로 make_circles( ) 데이터 셋을 클러스터링 수행. 
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.2, min_samples=10, metric='euclidean')
dbscan_labels = dbscan.fit_predict(X)
clusterDF['dbscan_cluster'] = dbscan_labels

visualize_cluster_plot(dbscan, clusterDF, 'dbscan_cluster', iscenter=False)

■■■■■■■■■■■■■0616 수업中 새로 알게된것■■■■■■■■■■■■■

##################### 군집화 실습 - 고객 세그먼테이션 #####################

# ### 데이터 셋 로딩과 데이터 클린징

# In[1]:


import pandas as pd
import datetime
import math
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

retail_df = pd.read_excel(io='C:/jeon/Online Retail.xlsx')
retail_df.head(3)
'''
고객 세그먼테이션은 '고객의 어떤 요소를 기반으로 군집화할 것인가?' 가 중요. 
여기서는 RFM 기법을 사용해서 고객을 군집화해볼 것이다. 
R RECENCY           : 가장 최근 상품 구입 일에서 오늘까지의 기간
F FREQUENCY         : 상품 구매 횟수
M MONETARY VALUE    : 총 구매 금액
'''

# In[2]:


retail_df.info()
retail_df.describe()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 541909 entries, 0 to 541908
Data columns (total 8 columns):
 #   Column       Non-Null Count   Dtype         
---  ------       --------------   -----         
 0   InvoiceNo    541909 non-null  object          주문번호. 'C'로 시작하면 취소 주문. 
 1   StockCode    541909 non-null  object          제품 코드
 2   Description  540455 non-null  object          제품 설명        => Null값 있음!
 3   Quantity     541909 non-null  int64           주문 제품 수량
 4   InvoiceDate  541909 non-null  datetime64[ns]  주문 일자
 5   UnitPrice    541909 non-null  float64         제품 단가
 6   CustomerID   406829 non-null  float64         고객 번호        => Null값 있음!
 7   Country      541909 non-null  object          국가명 (주문 고객의 국적)
dtypes: datetime64[ns](1), float64(2), int64(1), object(4)
memory usage: 33.1+ MB

            Quantity      UnitPrice     CustomerID      => int, float인 column만 나옴. 
count  541909.000000  541909.000000  406829.000000
mean        9.552250       4.611114   15287.690570
std       218.081158      96.759853    1713.600303
min    -80995.000000  -11062.060000   12346.000000      => Quantity(주문 제품 수량), UnitPrice(제품 단가)가 음수가 나온 게 이상하다!! => 제외하자!!
25%         1.000000       1.250000   13953.000000
50%         3.000000       2.080000   15152.000000
75%        10.000000       4.130000   16791.000000
max     80995.000000   38970.000000   18287.000000
'''


# **반품이나 CustomerID가 Null인 데이터는 제외, 영국 이외 국가의 데이터는 제외**

# In[3]:


retail_df = retail_df[retail_df['Quantity'] > 0]
retail_df = retail_df[retail_df['UnitPrice'] > 0]
retail_df = retail_df[retail_df['CustomerID'].notnull()]
print(retail_df.shape) # (397884, 8)
retail_df.isnull().sum() # column별 null값이 몇 개인지
'''
InvoiceNo      0
StockCode      0
Description    0  => 이상한 거 없애주니까 얘도 (딱히 상관없었지만..) null 값 없어졌넹 ㅎㅎ
Quantity       0
InvoiceDate    0
UnitPrice      0
CustomerID     0
Country        0
dtype: int64
'''

# In[4]:


retail_df['Country'].value_counts()[:20]
'''
United Kingdom     354321 多多多 !
Germany              9040
France               8341
EIRE                 7236
Spain                2484
Netherlands          2359
Belgium              2031
Switzerland          1841
Portugal             1462
Australia            1182
Norway               1071
Italy                 758
Channel Islands       748
Finland               685
Cyprus                614
Sweden                451
Austria               398
Denmark               380
Poland                330
Japan                 321
Name: Country, dtype: int64
'''


# In[5]:


retail_df = retail_df[retail_df['Country']=='United Kingdom'] # 영국 고객이 엄청 많았으니까, 영국 데이터만 갖고 봄. 
print(retail_df.shape)


# ### RFM 기반 데이터 가공

# **구매금액 생성**

# In[6]:


retail_df['sale_amount'] = retail_df['Quantity'] * retail_df['UnitPrice'] # 총 구매 금액 (RFM 中 M)
retail_df['CustomerID'] = retail_df['CustomerID'].astype(int)


# In[7]:


print(retail_df['CustomerID'].value_counts().head(5)) # 가장 많은 구매 행위를 한 사람 순으로
'''
17841    7847
14096    5111
12748    4595
14606    2700
15311    2379
Name: CustomerID, dtype: int64
'''
print(retail_df.groupby('CustomerID')['sale_amount'].sum().sort_values(ascending=False)[:5])
    # 동일한 CustomerID 끼리 묶어서, 한 CustomerID로 구매한 모든 sale_amount(총 구매 금액)끼리 다 더한 뒤, 내림차순으로 정렬. 
'''
CustomerID
18102    259657.30
17450    194550.79
16446    168472.50
17511     91062.38
16029     81024.84
Name: sale_amount, dtype: float64
'''

# In[8]:

retail_df.groupby(['InvoiceNo','StockCode']).count() 
    # count() : null이 아닌 모든 데이터 개수. 이렇게만 하면 모든 컬럼 전부 count()해줘서 같은 값 (주로 1)이 계속 들어갈 테니까, 그냥 컬럼 하나 골라서 걔만 보여주는 것. 
retail_df.groupby(['InvoiceNo','StockCode'])['InvoiceNo'].count() 
    # ['InvoiceNo'].count()에서 'InvoiceNo'를 넣든, 'Quantity'를 넣든... Null값 없는 아무 컬럼 뭘 넣든 상관x
    # quentity가 여러개인 게 아니라, 재주문도 아니라, 장바구니에 굳이 여러 번 담아서 산 횟수는 2 이상으로 세어줌. 
'''
InvoiceNo  StockCode    InvoiceNo
536365     21730        1
           22752        1
           71053        1
           84029E       1
           84029G       1
                       ..
581585     84946        1
581586     20685        1
           21217        1
           22061        1
           23275        1
Name: InvoiceNo, Length: 344435, dtype: int64
'''
retail_df.groupby(['InvoiceNo','StockCode'])['InvoiceNo'].count().mean() # 그럴 일은 거의 없기 때문에 평균이 1에 가까움. 
'''1.028702077315023'''

# **고객 기준으로 RFM - Recency, Frequency, Monetary가공**

# In[9]:


# DataFrame의 groupby() 의 multiple 연산을 위해 agg() 이용
# Recency는 InvoiceDate 컬럼의 max() 에서 데이터 가공
# Frequency는 InvoiceNo 컬럼의 count() , Monetary value는 sale_amount 컬럼의 sum()
aggregations = { # R, F, M
    'InvoiceDate': 'max', # 해당 CustomerID 로 주문한 가장 최근 날짜
    'InvoiceNo': 'count', # 해당 CustomerID 로 몇 번의 주문이 있었는가
    'sale_amount':'sum' # 해당 CustomerID 로 지금까지 결제한 총 구매 금액
}
cust_df = retail_df.groupby('CustomerID').agg(aggregations) # .agg() : DataFrame에서 여러 column에 각각의 기능을 수행해줌. 
                                                            # CustomerID로 묶어서 각 CustomerID별로 aggregations기능 수행한 결과 리턴. 
# groupby된 결과 컬럼값을 Recency, Frequency, Monetary로 변경
cust_df = cust_df.rename(columns = {'InvoiceDate':'Recency',
                                    'InvoiceNo':'Frequency',
                                    'sale_amount':'Monetary'
                                   }
                        )
cust_df = cust_df.reset_index()
cust_df.head(3)


# **Recency를 날짜에서 정수형으로 가공**

# In[10]:


cust_df['Recency'].max()
'''Timestamp('2011-12-09 12:49:00')'''

# In[11]:


import datetime as dt

cust_df['Recency'] = dt.datetime(2011,12,10) - cust_df['Recency'] # 가장 최근 : 가장 값이 작다. 
    # dt.datetime(2011,12,10) - Timestamp('2011-12-09 12:49:00') =>   0 days 12:49:00 나옴. 그치만 하루 더 더해줘야 함. 
    # datatime 끼리의 연산도 가능하다! ★ dt.datetime의 강력함 ★
cust_df['Recency'] = cust_df['Recency'].apply(lambda x: x.days+1) # 최종적으로, 오늘로부터 며칠 전인지. 
print('cust_df 로우와 컬럼 건수는 ',cust_df.shape)
cust_df.head(3)


# ### RFM 기반 고객 세그먼테이션

# **Recency, Frequency, Monetary 값의 분포도 확인**

# In[12]:








■■■■■■■■■■■■■0617 수업中 새로 알게된것■■■■■■■■■■■■■