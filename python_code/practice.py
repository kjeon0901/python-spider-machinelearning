import numpy as np

np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) # argmax: ê°€ì¥ í° í¬ì§€ì…˜ì— ìˆëŠ” ì•„ì´ì˜ idx ë¦¬í„´, bincount: ìœ ë‹ˆí¬í•œ ì›ì†Œ ê°œìˆ˜, weights: ê°€ì¤‘ì¹˜
'''1'''
print(np.bincount([0, 0, 1, 3, 3, 3])) # 0â†’2, 1â†’1, 2â†’0, 3â†’3  -> [2 1 0 3]
print(np.bincount([0, 0, 1])) # 0â†’2, 1â†’1  -> [2 1]
print(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) #ê° í¬ì§€ì…˜ì˜ weightì„ ë¶€ì—¬í•˜ë©´ 0â†’0.4, 1â†’0.6  -> [0.4 0.6]
print(np.bincount([0, 0, 1, 3, 3], weights=[0.2, 0.3, 0.6, 0.1, 0.1])) # -> [0.5 0.6 0.  0.2]


ex = np.array([[0.9, 0.1],
               [0.8, 0.2],
               [0.4, 0.6]])

p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) #axis=0 : ë³´í†µ rowë°©í–¥(ì•„ë˜ë°©í–¥) => ê·¸ëƒ¥ í‰ê·  ë‚´ë©´ [0.7 0.3]
print(p)    #ê°€ì¤‘ì¹˜ë¥¼ ëƒˆìœ¼ë‹ˆê¹Œ ê°€ì¤‘í‰ê·  -> 0.9*0.2, 0.8*0.2, 0.4*0.6 ë”í•´ì£¼ë©´ ë(ê°€ì¤‘ì¹˜ ìì²´ê°€ í™•ë¥ ì´ë‹ˆê¹Œ) => [0.58 0.42]
p = np.average(ex, axis=0)
print(p)


from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
import six
from sklearn.base import clone
from sklearn.pipeline import _name_estimators #__init__ì—ì„œ ì‚¬ìš©. estimator ë„£ì–´ì£¼ë©´ ìƒì„¸ ì •ë³´ list(zip(names, estimators))ë¥¼ ë¦¬í„´
import numpy as np
import operator


class MajorityVoteClassifier(BaseEstimator, ClassifierMixin): #BaseEstimator ìƒì† -> MajorityVoteClassifier í´ë˜ìŠ¤ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•œ estimatorì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥ (fit, predict, predict_proba ë©”ì†Œë“œ ë³´ë©´ ì•Œìˆ˜ìˆë“¯)
    def __init__(self, classifiers, vote='classlabel', weights=None):

        self.classifiers = classifiers
        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)} #ë”•ì…”ë„ˆë¦¬
        self.vote = vote
        self.weights = weights

    def fit(self, X, y):
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError("voteëŠ” 'probability' ë˜ëŠ” 'classlabel'ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                             "; (vote=%r)ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤."
                             % self.vote)

        if self.weights and len(self.weights) != len(self.classifiers):
            raise ValueError('ë¶„ë¥˜ê¸° ê°œìˆ˜ì™€ ê°€ì¤‘ì¹˜ ê°œìˆ˜ëŠ” ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.'
                             '; %dê°œì˜ ê°€ì¤‘ì¹˜ì™€, %dê°œì˜ ë¶„ë¥˜ê¸°ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.'
                             % (len(self.weights), len(self.classifiers)))

        # self.predict ë©”ì„œë“œì—ì„œ np.argmaxë¥¼ í˜¸ì¶œí•  ë•Œ 
        # í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ 0ë¶€í„° ì‹œì‘ë˜ì–´ì•¼ í•˜ë¯€ë¡œ LabelEncoderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ->ë ˆì´ë¸”ì¸ì½”ë”© : LabelEncoderë¥¼ ê°ì²´ë¡œ ìƒì„±í•œ í›„ , fit( ) ê³¼ transform( ) ìœ¼ë¡œ label ì¸ì½”ë”© ìˆ˜í–‰. 
        self.lablenc_ = LabelEncoder() #ë°–ì—ì„œ ìš°ë¦¬ê°€ labelencoding í•´ì¤¬ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ í•„ìš” ì—†ëŠ”ë° ê·¸ëƒ¥ í•´ì¤Œ
        self.lablenc_.fit(y) #y: ì•„ë˜ì—ì„œ ë¶–ê½ƒë°ì´í„° testë°ì´í„° ë ˆì´ë¸”ì¸ì½”ë”© ëë‚´ê³  ë” ì•„ë˜ì—ì„œ train_test_splití•´ì¤˜ì„œ ë‚˜ì˜¨ y_train ë°ì´í„°. 50ê°œì´ê³  ë¹„ìœ¨ì€ 0:1=1:1 
        self.classes_ = self.lablenc_.classes_ #ë ˆì´ë¸” ì¸ì½”ë”© ëœ ì•„ì´ì˜ ìœ ë‹ˆí¬í•œ ê°’ì´ ë“¤ì–´ê°. ì—¬ê¸°ì„  0, 1 ë‘ê°œê°€ ë“¤ì–´ê°
        self.classifiers_ = []
        for clf in self.classifiers: #í´ë˜ìŠ¤ ê°ì²´ ë§Œë“¤ ë•Œ ë°›ì•„ì˜¨ estimator 3ê°œ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸. dtype:list, ê°ê°ì˜ ìš”ì†Œ dtype:estimator
            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y)) #clone():ì›ë³¸ë§ê³  ë³µì‚¬, X:featureë°ì´í„°, ì´ì œ fitted_clf : í•™ìŠµëœ estimatorê°€ ë‹´ê²¨ ìˆìŒ!!!
                                    #fit(X_train, y_train)í•´ì¤€ê²ƒ => ê²°ê³¼ë¬¼ : í•™ìŠµëœ estimator
            self.classifiers_.append(fitted_clf) #ê·¸ estimatorê°€ ë“¤ì–´ê°. ì¦‰, forë¬¸ ëë‚˜ë©´ classifiers_ = [pipe1ìœ¼ë¡œ í•™ìŠµëœ estimator, clf2ë¡œ í•™ìŠµëœ estimator, pipe3ë¡œ í•™ìŠµëœ estimator]
        return self

    def predict(self, X):
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else:  # 'classlabel' íˆ¬í‘œ

            #  clf.predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ê²°ê³¼ë¥¼ ëª¨ìë‹ˆë‹¤.
            predictions = np.asarray([clf.predict(X)
                                      for clf in self.classifiers_]).T

            maj_vote = np.apply_along_axis(
                                      lambda x:
                                      np.argmax(np.bincount(x,
                                                weights=self.weights)),
                                      axis=1,
                                      arr=predictions)
        maj_vote = self.lablenc_.inverse_transform(maj_vote)
        return maj_vote

    def predict_proba(self, X):
        probas = np.asarray([clf.predict_proba(X)
                             for clf in self.classifiers_])
        avg_proba = np.average(probas, axis=0, weights=self.weights)
        return avg_proba

    def get_params(self, deep=True):
        """GridSearchë¥¼ ìœ„í•´ì„œ ë¶„ë¥˜ê¸°ì˜ ë§¤ê°œë³€ìˆ˜ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
        if not deep:
            return super(MajorityVoteClassifier, self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(step.get_params(deep=True)):
                    out['%s__%s' % (name, key)] = value
            return out

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
X, y = iris.data[50:, [1, 2]], iris.target[50:] #ì—¬ê¸°ê¹Œì§€ í–ˆì„ ë•Œ yì—ëŠ” 1,1,1,1,1,..2,2,2,2,2...ë§Œ ë“¤ì–´ê°€ìˆê³  0ì€ ì•ˆë“¤ì–´ê°.
le = LabelEncoder()     #y ì•ˆì˜ ê°’ì€ ëª¨ë‘ ìˆ«ìì¸ë° ë ˆì´ë¸”ì¸ì½”ë”© í•´ì£¼ëŠ” ì´ìœ  : ë‚˜ì¤‘ì— bin_countí•  ë•Œ 1ë¶€í„° ì‹œì‘í•˜ë©´ ì• ë§¤í•˜ê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ ë°”ê¿”ì¤Œ
y = le.fit_transform(y) #ë ˆì´ë¸”ì¸ì½”ë”© ëë‚œ í›„ yì—ëŠ” 0,0,0,0,0,.1,1,1,1,1...ë¡œ ë°”ë€Œì–´ ë“¤ì–´ê°€ìˆìŒ

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)
#test_size=0.5, stratify=yë¡œ ë³´ë©´ train, test ë°ì´í„°ëŠ” X rowì˜ ë°˜ì´ë‹ˆê¹Œ 50ì˜ í¬ê¸°ì¼ ê²ƒì´ê³ , ê·¸ ë¹„ìœ¨ì€ ë‘˜ ë‹¤ yì˜ ë¹„ìœ¨ì„ ë”°ë¼ 1:1ì¼ ê²ƒì´ë‹¤. 

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# estimator ê°ì²´ 3ê°œ ë§Œë“¦
clf1 = LogisticRegression(solver='liblinear',
                          penalty='l2', 
                          C=0.001,
                          random_state=1)

clf2 = DecisionTreeClassifier(max_depth=1,  #ì§€ê¸ˆ ì¼ë¶€ëŸ¬ max_depthë¥¼ 1ë¡œ ì¤˜ì„œ ë„ˆë¬´ ì–•ê²Œ ë§Œë“¦ -> ì¼ë¶€ëŸ¬ ì•½í•œ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ì—ˆêµ¬ë‚˜~!
                              criterion='entropy', #get_param()ìœ¼ë¡œ ê¹Œë³´ë©´ criterion defaultê°’ : gini    //gini, entropy, ë¶ˆìˆœë¬¼ì§€ìˆ˜ ì „ë¶€ ë¹„ìŠ·í•œ ëª©ì ì´ë‹¤. ê°™ë‹¤ê³  ìƒê°.
                              random_state=0)

clf3 = KNeighborsClassifier(n_neighbors=1,
                            p=2,
                            metric='minkowski')

pipe1 = Pipeline([['sc', StandardScaler()],
                  ['clf', clf1]])
pipe3 = Pipeline([['sc', StandardScaler()],
                  ['clf', clf3]])
'''
cf. íŒŒì´í”„ë¼ì¸
pipe1, pipe3 ì–˜ë„¤ëŠ” ë‚˜ì¤‘ì— estimatorì²˜ëŸ¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. (fit, predict)
estimatorëŠ” fit(X_train, y_train)í•´ì¤˜ì•¼ í•˜ëŠ”ë°, Pipilineì€ ì¼ë‹¨ ìŠ¤ì¼€ì¼ë§ í•´ì£¼ê³  clf1ìœ¼ë¡œ í•™ìŠµí•´ì¤€ë‹¤. 
'''

clf_labels = ['Logistic regression', 'Decision tree', 'KNN']

print('10-ê²¹ êµì°¨ ê²€ì¦:\n')
for clf, label in zip([pipe1, clf2, pipe3], clf_labels): #1st - pipe1,'Logistic regression', 2nd - clf2,'Decision tree', 
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') # cross_val_score() : ìë™ì ìœ¼ë¡œ Stratified Kí´ë“œ êµì°¨ ê²€ì¦
                    #forë¬¸ ëŒë©´ì„œ clfì— ìˆœì°¨ì ìœ¼ë¡œ 3ê°œ estimator ì „ë¶€ ë“¤ì–´ê°    #scoring : íŒë‹¨ ê¸°ì¤€
                                                                        
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]"
          % (scores.mean(), scores.std(), label))




# ë‹¤ìˆ˜ê²° íˆ¬í‘œ (í´ë˜ìŠ¤ ë ˆì´ë¸” ì¹´ìš´íŠ¸)

mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])

#__init()__ì—ì„œ ë­ ë‹´ê²¨ìˆëŠ”ì§€ ê·¸ëƒ¥ í™•ì¸
print(mv_clf.named_classifiers)
print(mv_clf.vote)
print(mv_clf.weights)


clf_labels += ['Majority voting'] #ë’¤ì— ìƒˆë¡œ ì´ë¦„ í•˜ë‚˜ ì¶”ê°€
all_clf = [pipe1, clf2, pipe3, mv_clf] #ë§ˆì§€ë§‰ ìš”ì†Œ : ë°©ê¸ˆ MajorityVoteClassifierë¡œ ë§Œë“  estimator ê°ì²´

for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')
    #ì§€ê¸ˆ votingë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” estimatorë¥¼ trainë°ì´í„° pipe1, clf2, pipe3, mv_clfë¡œ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ì¤‘!
    
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    '''
    ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
    ROC AUC: 0.87 (+/- 0.18) [Decision tree]
    ROC AUC: 0.85 (+/- 0.13) [KNN]
    ROC AUC: 0.98 (+/- 0.05) [Majority voting]     - ë§ˆì§€ë§‰ mv_clfê°€ ê°€ì¥ ì¢‹ê¸´ í•¨. ê·¼ë° ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê±´ ê·¸ê²Œ ì•„ë‹˜.
    
    1. ìš°ì„  train_test_split
    2. fit()ë©”ì†Œë“œì—ì„œ cv=10ì´ë¯€ë¡œ 10ê°œë¡œ ë‚˜ëˆ ì„œ 1ê°œì”© ë¹¼ë†“ê³  êµì°¨ê²€ì¦
    '''