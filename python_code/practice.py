import numpy as np

np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) # argmax: ê°€ì¥ í° í¬ì§€ì…˜ì— ìˆëŠ” ì•„ì´ì˜ idx ë¦¬í„´, bincount: ìœ ë‹ˆí¬í•œ ì›ì†Œ ê°œìˆ˜, weights: ê°€ì¤‘ì¹˜
'''1'''
print(np.bincount([0, 0, 1, 3, 3, 3])) # 0â†’2, 1â†’1, 2â†’0, 3â†’3  -> [2 1 0 3]
print(np.bincount([0, 0, 1])) # 0â†’2, 1â†’1  -> [2 1]
print(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) #ê° í¬ì§€ì…˜ì˜ weightì„ ë¶€ì—¬í•˜ë©´ 0â†’0.4, 1â†’0.6  -> [0.4 0.6]
print(np.bincount([0, 0, 1, 3, 3], weights=[0.2, 0.3, 0.6, 0.1, 0.1])) # -> [0.5 0.6 0.  0.2]


ex = np.array([[0.9, 0.1],
               [0.8, 0.2],
               [0.4, 0.6]])

p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) #axis=0 : ë³´í†µ rowë°©í–¥(ì•„ë˜ë°©í–¥) => ê·¸ëƒ¥ í‰ê·  ë‚´ë©´ [0.7 0.3]
print(p)    #ê°€ì¤‘ì¹˜ë¥¼ ëƒˆìœ¼ë‹ˆê¹Œ ê°€ì¤‘í‰ê·  -> 0.9*0.2, 0.8*0.2, 0.4*0.6 ë”í•´ì£¼ë©´ ë(ê°€ì¤‘ì¹˜ ìì²´ê°€ í™•ë¥ ì´ë‹ˆê¹Œ) => [0.58 0.42]
p = np.average(ex, axis=0)
print(p)


from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
import six
from sklearn.base import clone
from sklearn.pipeline import _name_estimators #__init__ì—ì„œ ì‚¬ìš©. estimator ë„£ì–´ì£¼ë©´ ìƒì„¸ ì •ë³´ list(zip(names, estimators))ë¥¼ ë¦¬í„´
import numpy as np
import operator

test=[]
test1=[]
test2=[]

class MajorityVoteClassifier(BaseEstimator, ClassifierMixin): #BaseEstimator ìƒì† -> MajorityVoteClassifier í´ë˜ìŠ¤ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•œ estimatorì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥ (fit, predict, predict_proba ë©”ì†Œë“œ ë³´ë©´ ì•Œìˆ˜ìˆë“¯)
    global test
    global test1
    global test2
    
    def __init__(self, classifiers, vote='classlabel', weights=None):

        self.classifiers = classifiers
        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)} #ë”•ì…”ë„ˆë¦¬
        self.vote = vote
        self.weights = weights

    def fit(self, X, y):
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError("voteëŠ” 'probability' ë˜ëŠ” 'classlabel'ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                             "; (vote=%r)ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤."
                             % self.vote)

        if self.weights and len(self.weights) != len(self.classifiers):
            raise ValueError('ë¶„ë¥˜ê¸° ê°œìˆ˜ì™€ ê°€ì¤‘ì¹˜ ê°œìˆ˜ëŠ” ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.'
                             '; %dê°œì˜ ê°€ì¤‘ì¹˜ì™€, %dê°œì˜ ë¶„ë¥˜ê¸°ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.'
                             % (len(self.weights), len(self.classifiers)))

        # self.predict ë©”ì„œë“œì—ì„œ np.argmaxë¥¼ í˜¸ì¶œí•  ë•Œ 
        # í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ 0ë¶€í„° ì‹œì‘ë˜ì–´ì•¼ í•˜ë¯€ë¡œ LabelEncoderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ->ë ˆì´ë¸”ì¸ì½”ë”© : LabelEncoderë¥¼ ê°ì²´ë¡œ ìƒì„±í•œ í›„ , fit( ) ê³¼ transform( ) ìœ¼ë¡œ label ì¸ì½”ë”© ìˆ˜í–‰. 
        # ë ˆì´ë¸” ì¸ì½”ë”©ì—ì„œ fit(A) - A ì•ˆì˜ ìœ ë‹ˆí¬í•œ ë ˆì´ë¸”ê°’ì— ìˆ«ì ë¶€ì—¬í•œ í…Œì´ë¸” í•˜ë‚˜ ë§Œë“¦, B=transform(A) - fitë¡œ ë§Œë“  ë ˆì´ë¸” í…Œì´ë¸”ì„ ê°€ì ¸ì™€ì„œ Aì— ì ìš©ì‹œí‚¨ 0 1 0 1 2 1... ê°’ì„ Bì— ë‹´ìŒ. fit_transform() ë¡œ í•œë²ˆì— í•´ë„ ë¨
        self.lablenc_ = LabelEncoder() #ë°–ì—ì„œ ìš°ë¦¬ê°€ labelencoding í•´ì¤¬ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ í•„ìš” ì—†ëŠ”ë° ê·¸ëƒ¥ í•´ì¤Œ
        self.lablenc_.fit(y) #y: ì•„ë˜ì—ì„œ ë¶–ê½ƒë°ì´í„° testë°ì´í„° ë ˆì´ë¸”ì¸ì½”ë”© ëë‚´ê³  ë” ì•„ë˜ì—ì„œ train_test_splití•´ì¤˜ì„œ ë‚˜ì˜¨ y_train ë°ì´í„°. 50ê°œì´ê³  ë¹„ìœ¨ì€ 0:1=1:1 
        self.classes_ = self.lablenc_.classes_ #ë ˆì´ë¸” ì¸ì½”ë”© ëœ ì•„ì´ì˜ ìœ ë‹ˆí¬í•œ ê°’ì´ ë“¤ì–´ê°. ì—¬ê¸°ì„  0, 1 ë‘ê°œê°€ ë“¤ì–´ê°
        self.classifiers_ = []
        for clf in self.classifiers: #í´ë˜ìŠ¤ ê°ì²´ ë§Œë“¤ ë•Œ ë°›ì•„ì˜¨ estimator 3ê°œ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸. dtype:list, ê°ê°ì˜ ìš”ì†Œ dtype:estimator
            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y)) #clone():ì›ë³¸ë§ê³  ë³µì‚¬, X:featureë°ì´í„°, ì´ì œ fitted_clf : í•™ìŠµëœ estimatorê°€ ë‹´ê²¨ ìˆìŒ!!!
                                    #fit(X_train, y_train)í•´ì¤€ê²ƒ => ê²°ê³¼ë¬¼ : í•™ìŠµëœ estimator
                                    #ê·¼ë° êµì°¨ê²€ì¦ì´ê¸° ë•Œë¬¸ì— 45ê°œì˜ ë°ì´í„°ë§Œ ìš”ê¸°ì„œ í•¨. ì´ì œ ë‚˜ë¨¸ì§€ 5ê°œì˜ testë°ì´í„°ë¡œ predict í•´ë´ì•¼ í•¨.
                                    #ê·¼ë° ì²˜ìŒì— êµì°¨ê²€ì¦ scoring='roc_auc'(xì¶•ì´ FPRë¡œ, thresholdê°’ì˜ ë³€í™”ì— ë”°ë¼ ê·¸ë˜í”„ ê·¸ë ¤ì§)ë¥¼ í•´ì£¼ì—ˆê¸° ë•Œë¬¸ì—, í™•ë¥ ê°’ì´ í•„ìš”í•˜ë‹¤. predictê°€ ì•„ë‹ˆë¼, predict_probaë¥¼ í•´ì¤˜ì•¼ í•¨. ê·¸ë˜ì„œ ìë™ìœ¼ë¡œ ê·¸ë ‡ê²Œ ì‹¤í–‰ë˜ë¯€ë¡œ 22222222222222222222222ê°€ ì¶œë ¥ëœë‹¤. 
                                    #roc_auc - thresholdê°’ ë³€í™” - predict_probaì—ì„œ ë ˆì´ë¸”ê°’ ë‚˜ëˆ„ëŠ” ë¹„ìœ¨ ë³€í™”
            self.classifiers_.append(fitted_clf) #ê·¸ estimatorê°€ ë“¤ì–´ê°. ì¦‰, forë¬¸ ëë‚˜ë©´ classifiers_ = [pipe1ìœ¼ë¡œ í•™ìŠµëœ estimator, clf2ë¡œ í•™ìŠµëœ estimator, pipe3ë¡œ í•™ìŠµëœ estimator]
        return self

    def predict(self, X): #êµì°¨ê²€ì¦ì´ê¸° ë•Œë¬¸ì— XëŠ” 5ê°œì˜ ë°ì´í„°(shape:5x2, cv=10ì´ì—ˆê¸° ë•Œë¬¸ì—)
        print('111111111111111111111111111111')
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else:  # 'classlabel' íˆ¬í‘œ - defaultê°€ ì—¬ê¸°ì„. 

            #  clf.predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ê²°ê³¼ë¥¼ ëª¨ìë‹ˆë‹¤.
            predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T #.T ë¶™ì´ë©´ ì „ì¹˜í–‰ë ¬! í–‰ë ¬ ë’¤ì§‘ê¸°. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê±´ í•˜ë‚˜ì˜ estimatorì˜ˆì¸¡ì´ í•˜ë‚˜ì˜ columnì— ë“¤ì–´ê°€ê¸¸ ì›í•œë‹¤. 
            test1.append(predictions.T) #ì´ë¯¸ í•œë²ˆ ì „ì¹˜í–‰ë ¬ í•´ë‘¬ì„œ AT ëìœ¼ë‹ˆê¹Œ (AT)T í•´ì„œ ë‹¤ì‹œ A ë³¼ ìˆ˜ ìˆìŒ. 
            test2.append(predictions) #3ê°œì˜ estimatorê°€ 5ê°œì˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ ì˜ˆì¸¡ê°’ì„ 5x3í˜•íƒœë¡œ ë‹´ìŒ
            
            maj_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions)
            #ë³´í†µ df.apply(lambda x: ~~) í•˜ë©´ dfì—ì„œ row í•˜ë‚˜ì”© xì— ë“¤ì–´ê°€ëŠ”ë°, ì—¬ê¸°ì„œëŠ” íŠ¹ì •ì§“ì§€ ì•Šê³  npë¼ê³  í•´ì£¼ì—ˆë‹¤. ëŒ€ì‹  ë’¤ì— arr = predictionsë¼ê³  í•´ì¤˜ì„œ predictionsì— ì ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
            '''
            ì´ê²Œ Hard Votingê³¼ ì—°ê²°ë˜ëŠ” ì´ìœ !!! â˜…â˜…â˜…
            3ê°œì˜ estimatorê°€ ê°ê°ì˜ ë°ì´í„°ì— ì˜ˆì¸¡í•œ ê°’ì—ì„œ ë‹¤ìˆ˜ê²°ë¡œ ìµœì¢… ê²°ì •í•œë‹¤. 
            '''
        maj_vote = self.lablenc_.inverse_transform(maj_vote) 
        #ë ˆì´ë¸” ì¸ì½”ë”©í•´ì„œ ì¡°ê¸ˆ ë‹¬ë¼ì§„ ê°’ì„ ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ 0, 1 -> 1, 2. ê·¼ë° ì§€ê¸ˆ ë³´ë©´ ì´ë¯¸ ë°–ì—ì„œ ë ˆì´ë¸” ì¸ì½”ë”© ëë‚œ ë’¤ì— train_test_splití•´ì£¼ì—ˆë‹¤. ê·¸ë¦¬ê³  ì´ ìœ„ì—ì„œ í˜¹ì‹œ ë¹ ì§„ ê²½ìš°ê°€ ìˆì„ê¹Œë´ í•œë²ˆ ë” ë ˆì´ë¸”ì¸ì½”ë”© í•´ì£¼ì—ˆë‹¤. (ì—¬ê¸°ì„  ë³„ë¡œ ì˜ë¯¸ ì—†ì—ˆì§€ë§Œ)
        #ê·¸ë˜ì„œ ì§€ê¸ˆ inverse_transformí•´ë´¤ì, ìœ„ì— ë‘ ë²ˆì§¸ë¡œ ìˆ˜í–‰í•œ ë ˆì´ë¸” ì¸ì½”ë”©ë§Œ ë‹¤ì‹œ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ê²Œ ëœë‹¤. ê·¸ë˜ë´¤ì ëŒë ¤ë†“ì€ ìƒíƒœë„ ê·¸ëŒ€ë¡œ 0, 1 -> 0, 1ì¸ ê±¸!!!
        #ì—¬ê¸°ì„  ë³„ ì˜ë¯¸ ì—†ì§€ë§Œ, ë‹¤ë¥¸ ì¼€ì´ìŠ¤ë“¤ì„ ìƒê°í•´ë³´ë©´ (ë ˆì´ë¸”ì¸ì½”ë”© í•œ ë²ˆë§Œ ìˆ˜í–‰í–ˆê³ , ê·¸ê²Œ split ì´í›„ë¼ë©´) ì´ ì½”ë“œëŠ” ìœ ì˜ë¯¸í•´ì§„ë‹¤!
        return maj_vote

    def predict_proba(self, X): #êµì°¨ê²€ì¦ì´ê¸° ë•Œë¬¸ì— XëŠ” 5ê°œì˜ test ë°ì´í„°(shape:5x2, cv=10ì´ì—ˆê¸° ë•Œë¬¸ì—)
        print('222222222222222222222222222222')
        probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) #probas : 3x5x2 3ì°¨ì› ë°ì´í„°, ê° estimatorë³„ë¡œ predict_probaí•œ ê²°ê³¼ í™•ë¥ ê°’ 5x2ë°ì´í„°ê°€ 3ë²ˆ ë‹´ê¸°ë‹ˆê¹Œ. 
        test=probas #3ì°¨ì› ë°ì´í„°ì¸ ê²ƒ í™•ì¸~
        avg_proba = np.average(probas, axis=0, weights=self.weights) #3ì°¨ì›ì´ê¸° ë•Œë¬¸ì— axis=0,1,2ê¹Œì§€ ê°€ëŠ¥. axis=0ì´ë©´ 3x5x2ì—ì„œ 3ì— í•´ë‹¹. 3ê°œì—ì„œ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” ì• ë“¤ë¼ë¦¬ í‰ê·  ë‚´ê³ , ê²°ê´ê°’ì€ 5x2 shape.
        '''
        ì´ê²Œ Soft Votingê³¼ ì—°ê²°ë˜ëŠ” ì´ìœ !!! â˜…â˜…â˜…
        3ê°œì˜ classifierë“¤ì˜ ê°ê°ì˜ í”¼ì³ ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ì„ ë ˆì´ë¸”ê°’ë³„ë¡œ ë‚¸ í‰ê· ì„ êµ¬í•¨. 
        ì—¬ê¸°ì„œ ìµœê³ ì¸ ë ˆì´ë¸”ê°’ìœ¼ë¡œ ìµœì¢… classê°’ ê²°ì •í•˜ê¸°ë§Œ í•˜ë©´ Soft Voting!
        '''
        return avg_proba

    def get_params(self, deep=True):
        """GridSearchë¥¼ ìœ„í•´ì„œ ë¶„ë¥˜ê¸°ì˜ ë§¤ê°œë³€ìˆ˜ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
        if not deep:
            return super(MajorityVoteClassifier, self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in six.iteritems(self.named_classifiers):
                for key, value in six.iteritems(step.get_params(deep=True)):
                    out['%s__%s' % (name, key)] = value
            return out

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()
X, y = iris.data[50:, [1, 2]], iris.target[50:] #ì—¬ê¸°ê¹Œì§€ í–ˆì„ ë•Œ yì—ëŠ” 1,1,1,1,1,..2,2,2,2,2...ë§Œ ë“¤ì–´ê°€ìˆê³  0ì€ ì•ˆë“¤ì–´ê°.
le = LabelEncoder()     #y ì•ˆì˜ ê°’ì€ ëª¨ë‘ ìˆ«ìì¸ë° ë ˆì´ë¸”ì¸ì½”ë”© í•´ì£¼ëŠ” ì´ìœ  : ë‚˜ì¤‘ì— bincountí•  ë•Œ 1ë¶€í„° ì‹œì‘í•˜ë©´ ì• ë§¤í•˜ê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ ë°”ê¿”ì¤Œ
                        #bincount([0,0,1,3,3,3]):[2 1 0 3], argmax(bincount([0,0,1,3,3,3])):3(ìµœê³³ê°’ ìˆëŠ” ì¸ë±ìŠ¤)
y = le.fit_transform(y) #ë ˆì´ë¸”ì¸ì½”ë”© ëë‚œ í›„ yì—ëŠ” 0,0,0,0,0,.1,1,1,1,1...ë¡œ ë°”ë€Œì–´ ë“¤ì–´ê°€ìˆìŒ

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)
#test_size=0.5, stratify=yë¡œ ë³´ë©´ train, test ë°ì´í„°ëŠ” X rowì˜ ë°˜ì´ë‹ˆê¹Œ 50ì˜ í¬ê¸°ì¼ ê²ƒì´ê³ , ê·¸ ë¹„ìœ¨ì€ ë‘˜ ë‹¤ yì˜ ë¹„ìœ¨ì„ ë”°ë¼ 1:1ì¼ ê²ƒì´ë‹¤. 

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# estimator ê°ì²´ 3ê°œ ë§Œë“¦
clf1 = LogisticRegression(solver='liblinear',
                          penalty='l2', 
                          C=0.001,
                          random_state=1)

clf2 = DecisionTreeClassifier(max_depth=1,  #ì§€ê¸ˆ ì¼ë¶€ëŸ¬ max_depthë¥¼ 1ë¡œ ì¤˜ì„œ ë„ˆë¬´ ì–•ê²Œ ë§Œë“¦ -> ì¼ë¶€ëŸ¬ ì•½í•œ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ì—ˆêµ¬ë‚˜~!
                              criterion='entropy', #get_param()ìœ¼ë¡œ ê¹Œë³´ë©´ criterion defaultê°’ : gini    //gini, entropy, ë¶ˆìˆœë¬¼ì§€ìˆ˜ ì „ë¶€ ë¹„ìŠ·í•œ ëª©ì ì´ë‹¤. ê°™ë‹¤ê³  ìƒê°.
                              random_state=0)

clf3 = KNeighborsClassifier(n_neighbors=1,
                            p=2,
                            metric='minkowski')

pipe1 = Pipeline([['sc', StandardScaler()], #ê·¸ëƒ¥ clf1 ì´ë¦„ì´ pipe1ë¡œ ë°”ê¼ˆêµ¬ë‚˜~ë¼ê³  ìƒê°
                  ['clf', clf1]])
pipe3 = Pipeline([['sc', StandardScaler()], #ê·¸ëƒ¥ clf3 ì´ë¦„ì´ pipe3ìœ¼ë¡œ ë°”ê¼ˆêµ¬ë‚˜~ë¼ê³  ìƒê°
                  ['clf', clf3]])
'''
cf. íŒŒì´í”„ë¼ì¸
pipe1, pipe3 ì–˜ë„¤ëŠ” ë‚˜ì¤‘ì— estimatorì²˜ëŸ¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. (fit, predict)
estimatorëŠ” fit(X_train, y_train)í•´ì¤˜ì•¼ í•˜ëŠ”ë°, Pipilineì€ ì¼ë‹¨ ìŠ¤ì¼€ì¼ë§ í•´ì£¼ê³  clf1ìœ¼ë¡œ í•™ìŠµí•´ì¤€ë‹¤. 
'''

clf_labels = ['Logistic regression', 'Decision tree', 'KNN']

print('10-ê²¹ êµì°¨ ê²€ì¦:\n')
for clf, label in zip([pipe1, clf2, pipe3], clf_labels): #1st - pipe1,'Logistic regression', 2nd - clf2,'Decision tree', 
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') # cross_val_score() : ìë™ì ìœ¼ë¡œ Stratified Kí´ë“œ êµì°¨ ê²€ì¦
                    #forë¬¸ ëŒë©´ì„œ clfì— ìˆœì°¨ì ìœ¼ë¡œ 3ê°œ estimator ì „ë¶€ ë“¤ì–´ê°    #scoring : íŒë‹¨ ê¸°ì¤€
                                                                        
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]"
          % (scores.mean(), scores.std(), label))




# ë‹¤ìˆ˜ê²° íˆ¬í‘œ (í´ë˜ìŠ¤ ë ˆì´ë¸” ì¹´ìš´íŠ¸)

mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3]) #ë¦¬ìŠ¤íŠ¸ ì•ˆì— estimatorë„ ìš”ì†Œë¡œ ë„£ì„ ìˆ˜ ìˆë‹¤. classifiersë¼ëŠ” íŒŒë¼ë¯¸í„° ì„¤ì •ìœ¼ë¡œ ì´ê²ƒ ë„£ì–´ì¤Œ

#__init()__ì—ì„œ ë­ ë‹´ê²¨ìˆëŠ”ì§€ ê·¸ëƒ¥ í™•ì¸
print(mv_clf.named_classifiers)
print(mv_clf.vote)
print(mv_clf.weights)


clf_labels += ['Majority voting'] #ë’¤ì— ìƒˆë¡œ ì´ë¦„ í•˜ë‚˜ ì¶”ê°€
all_clf = [pipe1, clf2, pipe3, mv_clf] #ë§ˆì§€ë§‰ ìš”ì†Œ : ë°©ê¸ˆ MajorityVoteClassifierë¡œ ë§Œë“  estimator ê°ì²´

for clf, label in zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='accuracy') #ì–˜ê°€ accuracyë¡œ ë°”ë€Œë©´ predict_proba ë§ê³  predictê°€ ì‹¤í–‰ë˜ì–´ 11111111111111111111ì´ ì¶œë ¥ëœë‹¤. 
    #ì§€ê¸ˆ votingë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” estimator pipe1, clf2, pipe3, mv_clfë¥¼ trainë°ì´í„°ë¡œ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ì¤‘!
    #50ê°œì˜ ë°ì´í„°ê°€ 10ì¡°ê°ìœ¼ë¡œ foldë˜ì–´ 45ê°œ, 5ê°œë¡œ ë‚˜ëˆ„ì–´ êµì°¨ê²€ì¦
    
    print("ROC AUC: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    '''
    ROC AUC: 0.92 (+/- 0.15) [Logistic regression]
    ROC AUC: 0.87 (+/- 0.18) [Decision tree]
    ROC AUC: 0.85 (+/- 0.13) [KNN]
    ROC AUC: 0.98 (+/- 0.05) [Majority voting]     - ë§ˆì§€ë§‰ mv_clfê°€ ê°€ì¥ ì¢‹ê¸´ í•¨. ê·¼ë° ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê±´ ê·¸ê²Œ ì•„ë‹˜.
    
    1. ìš°ì„  train_test_split
    2. fit()ë©”ì†Œë“œì—ì„œ cv=10ì´ë¯€ë¡œ 10ê°œë¡œ ë‚˜ëˆ ì„œ 1ê°œì”© ë¹¼ë†“ê³  êµì°¨ê²€ì¦
    '''